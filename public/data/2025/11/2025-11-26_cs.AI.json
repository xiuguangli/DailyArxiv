[
    {
        "order": 1,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19577",
        "abs_url": "https://arxiv.org/abs/2511.19577",
        "pdf_url": "https://arxiv.org/pdf/2511.19577",
        "title": "Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder",
        "authors": [
            "Abhay Goyal",
            "Navin Kumar",
            "Kimberly DiMeola",
            "Rafael Trujillo",
            "Soorya Ram Shimgekar",
            "Christian Poellabauer",
            "Pi Zonooz",
            "Ermonda Gjoni-Markaj",
            "Declan Barry",
            "Lynn Madden"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19669",
        "abs_url": "https://arxiv.org/abs/2511.19669",
        "pdf_url": "https://arxiv.org/pdf/2511.19669",
        "title": "HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization",
        "authors": [
            "Souradip Poddar",
            "Chia-Tung Ho",
            "Ziming Wei",
            "Weidong Cao",
            "Haoxing Ren",
            "David Z. Pan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19671",
        "abs_url": "https://arxiv.org/abs/2511.19671",
        "pdf_url": "https://arxiv.org/pdf/2511.19671",
        "title": "FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking",
        "authors": [
            "Rishab Sharma",
            "Iman Saberi",
            "Elham Alipour",
            "Jie JW Wu",
            "Fatemeh Fard"
        ],
        "comments": "3 tables, 11 pages, 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Generative AI in Finance",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19749",
        "abs_url": "https://arxiv.org/abs/2511.19749",
        "pdf_url": "https://arxiv.org/pdf/2511.19749",
        "title": "Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions",
        "authors": [
            "Farzan Karimi-Malekabadi",
            "Pooya Razavi",
            "Sonya Powers"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19780",
        "abs_url": "https://arxiv.org/abs/2511.19780",
        "pdf_url": "https://arxiv.org/pdf/2511.19780",
        "title": "NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents",
        "authors": [
            "Ioannis Tzachristas",
            "Aifen Sui"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models. Our method leverages retrieval-augmented prompting, logit biasing and optional classification heads to inject symbolic intent structure into both input and output representations. We formalize a new evaluation metric-Semantic Intent Similarity (SIS)-based on hierarchical ontology depth, capturing semantic proximity even when predicted intents differ lexically. Experiments on a subset of ambiguous/demanding dialogues of MultiWOZ 2.3 (with oracle labels from GPT-o3) demonstrate that a 3B Llama model with ontology augmentation approaches GPT-4 accuracy (85% vs 90%) at a tiny fraction of the energy and memory footprint. Qualitative comparisons show that ontology-augmented models produce more grounded, disambiguated multi-intent interpretations. Our results validate symbolic alignment as an effective strategy for enabling accurate and efficient on-device NLU.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19798",
        "abs_url": "https://arxiv.org/abs/2511.19798",
        "pdf_url": "https://arxiv.org/pdf/2511.19798",
        "title": "KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)",
        "authors": [
            "Weizhi Liu",
            "Xi Chen",
            "Zekun Jiang",
            "Liang Zhao",
            "Kunyuan Jiang",
            "Ruisi Tang",
            "Li Wang",
            "Mingke You",
            "Hanyu Zhou",
            "Hongyu Chen",
            "Qiankun Xiong",
            "Yong Nie",
            "Kang Li",
            "Jian Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19829",
        "abs_url": "https://arxiv.org/abs/2511.19829",
        "pdf_url": "https://arxiv.org/pdf/2511.19829",
        "title": "A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization",
        "authors": [
            "Ke Chen",
            "Yifeng Wang",
            "Hassan Almosapeeh",
            "Haohan Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19849",
        "abs_url": "https://arxiv.org/abs/2511.19849",
        "pdf_url": "https://arxiv.org/pdf/2511.19849",
        "title": "Reinforcement Learning with $ω$-Regular Objectives and Constraints",
        "authors": [
            "Dominik Wagner",
            "Leon Witzman",
            "Luke Ong"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $\\omega$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk. We address both limitations simultaneously by combining $\\omega$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $\\omega$-regular objective while also adhering to $\\omega$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19864",
        "abs_url": "https://arxiv.org/abs/2511.19864",
        "pdf_url": "https://arxiv.org/pdf/2511.19864",
        "title": "MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support",
        "authors": [
            "Valerie Lockhart",
            "Dan McCreary",
            "Troy A. Peterson"
        ],
        "comments": "42 pages, 4 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19865",
        "abs_url": "https://arxiv.org/abs/2511.19865",
        "pdf_url": "https://arxiv.org/pdf/2511.19865",
        "title": "Agentic AI-Empowered Conversational Embodied Intelligence Networks in 6G",
        "authors": [
            "Mingkai Chen",
            "Zijie Feng",
            "Lei Wang",
            "Yaser Khamayseh"
        ],
        "comments": "7 pages, 8 figures. Preprint submitted to IEEE Vehicle Technology Magazine",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In the 6G era, semantic collaboration among multiple embodied intelligent devices (MEIDs) becomes crucial for complex task execution. However, existing systems face challenges in multimodal information fusion, adaptive communication, and decision interpretability. To address these limitations, we propose a collaborative Conversational Embodied Intelligence Network (CC-EIN) integrating multimodal feature fusion, adaptive semantic communication, task coordination, and interpretability. PerceptiNet performs cross-modal fusion of image and radar data to generate unified semantic representations. An adaptive semantic communication strategy dynamically adjusts coding schemes and transmission power according to task urgency and channel quality. A semantic-driven collaboration mechanism further supports task decomposition and conflict-free coordination among heterogeneous devices. Finally, the InDec module enhances decision transparency through Grad-CAM visualization. Simulation results in post-earthquake rescue scenarios demonstrate that CC-EIN achieves 95.4% task completion rate and 95% transmission efficiency while maintaining strong semantic consistency and energy efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19872",
        "abs_url": "https://arxiv.org/abs/2511.19872",
        "pdf_url": "https://arxiv.org/pdf/2511.19872",
        "title": "Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy",
        "authors": [
            "Daniel I Jackson",
            "Emma L Jensen",
            "Syed-Amad Hussain",
            "Emre Sezgin"
        ],
        "comments": "25 pages,5 tables, 3 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19895",
        "abs_url": "https://arxiv.org/abs/2511.19895",
        "pdf_url": "https://arxiv.org/pdf/2511.19895",
        "title": "RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation",
        "authors": [
            "Yuanyuan Lin",
            "Xiangyu Ouyang",
            "Teng Zhang",
            "Kaixin Sui"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19925",
        "abs_url": "https://arxiv.org/abs/2511.19925",
        "pdf_url": "https://arxiv.org/pdf/2511.19925",
        "title": "Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity",
        "authors": [
            "Qiyao Wei",
            "Edward Morrell",
            "Lea Goetz",
            "Mihaela van der Schaar"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at this https URL and the dataset is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19933",
        "abs_url": "https://arxiv.org/abs/2511.19933",
        "pdf_url": "https://arxiv.org/pdf/2511.19933",
        "title": "A System-Level Taxonomy of Failure Modes in Large Language Model Applications",
        "authors": [
            "Vaishali Vinay"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19969",
        "abs_url": "https://arxiv.org/abs/2511.19969",
        "pdf_url": "https://arxiv.org/pdf/2511.19969",
        "title": "M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation",
        "authors": [
            "Weizi Shao",
            "Taolin Zhang",
            "Zijie Zhou",
            "Chen Chen",
            "Chengyu Wang",
            "Xiaofeng He"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20048",
        "abs_url": "https://arxiv.org/abs/2511.20048",
        "pdf_url": "https://arxiv.org/pdf/2511.20048",
        "title": "Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design",
        "authors": [
            "Zixiao Huang",
            "Wen Zeng",
            "Tianyu Fu",
            "Tengxuan Liu",
            "Yizhou Sun",
            "Ke Hong",
            "Xinhao Yang",
            "Chengchun Liu",
            "Yan Li",
            "Quanlu Zhang",
            "Guohao Dai",
            "Zhenhua Zhu",
            "Yu Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "LLM-based search agents achieve strong performance but suffer from severe latency, as each step requires serialized LLM reasoning followed by action of tool execution. We revisit this bottleneck through the lens of speculation. While traditional predict-verify speculation paradigm can break serial execution, its benefit remains limited, as it retains the full original workload and adds extra inference overhead. We observe that early agent steps often involve simple evidence-gathering, where correct actions can often be predicted without full reasoning. Building on these observations, we present SPAgent, an algorithm-system co-design framework that expands the role of speculation in search agents to reduce latency. Algorithmically, SPAgent introduces a two-phase adaptive speculation mechanism that selectively omits verification when safe. System-wise, a two-level scheduler regulates speculative requests based on engine load to ensure speculation remains beneficial. We implement SPAgent in real-world systems. Across extensive experimental settings, SPAgent achieves up to $1.65\\times$ end-to-end speedup while maintaining same or even achieving higher accuracy, enabling practical deployment of multi-step search agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20067",
        "abs_url": "https://arxiv.org/abs/2511.20067",
        "pdf_url": "https://arxiv.org/pdf/2511.20067",
        "title": "\"Are We Done Yet?\": A Vision-Based Judge for Autonomous Task Completion of Computer Use Agents",
        "authors": [
            "Marta Sumyk",
            "Oleksandr Kosovan"
        ],
        "comments": "This work has been accepted to appear at the AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Computer Use Agents (CUAs) are designed to autonomously operate digital interfaces, yet they often fail to reliably determine whether a given task has been completed. We present an autonomous evaluation and feedback framework that uses vision-language models to assess task completion directly from screenshots and task descriptions. Our dataset covers 42 built-in macOS applications and 1,260 human-labeled tasks across a wide range of scenarios. Our framework achieves up to 73 percent accuracy in task success detection and yields an average relative improvement of 27 percent in overall task success when evaluator feedback is applied. These results show that vision-based evaluation can serve as an effective feedback mechanism that improves the reliability and self-correction of autonomous computer-use agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20085",
        "abs_url": "https://arxiv.org/abs/2511.20085",
        "pdf_url": "https://arxiv.org/pdf/2511.20085",
        "title": "VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis",
        "authors": [
            "Chujie Wang",
            "Zhiyuan Luo",
            "Ruiqi Liu",
            "Can Ran",
            "Shenghua Fan",
            "Xi Chen",
            "Chu He"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and this http URL also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20138",
        "abs_url": "https://arxiv.org/abs/2511.20138",
        "pdf_url": "https://arxiv.org/pdf/2511.20138",
        "title": "From data to concepts via wiring diagrams",
        "authors": [
            "Jason Lo",
            "Mohammadnima Jafari"
        ],
        "comments": "19 pages",
        "subjects": "Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Machine Learning (cs.LG); Combinatorics (math.CO)",
        "abstract": "A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20196",
        "abs_url": "https://arxiv.org/abs/2511.20196",
        "pdf_url": "https://arxiv.org/pdf/2511.20196",
        "title": "Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning",
        "authors": [
            "Zhen Zeng",
            "Leijiang Gu",
            "Zhangling Duan",
            "Feng Li",
            "Zenglin Shi",
            "Cees G. M. Snoek",
            "Meng Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal Large Language Models (MLLMs) achieve remarkable capabilities but can inadvertently memorize privacy-sensitive information. Although existing unlearning methods can remove such knowledge, they fail to achieve benign forgetting because they often degrade the model's general image understanding performance. To address this, we propose the Sculpted Memory Forgetting Adapter (SMFA), which confines forgetting to targeted memory regions while preserving overall capabilities. SMFA first fine-tunes the model to replace sensitive responses with refusals, yielding a memory forgetting adapter, and then applies a retaining anchor-guided masking mechanism to prevent interference with unrelated knowledge and understanding ability. To systematically evaluate selective MLLM unlearning, we introduce S-MLLMUn Bench, the first benchmark designed to jointly assess the removal of sensitive knowledge and retention of general visual understanding. Extensive experiments show that, unlike prior methods, SMFA achieves precise and controllable unlearning while maintaining the model's foundational image understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20200",
        "abs_url": "https://arxiv.org/abs/2511.20200",
        "pdf_url": "https://arxiv.org/pdf/2511.20200",
        "title": "Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025",
        "authors": [
            "Yitian Huang",
            "Yuxuan Lei",
            "Jianxun Lian",
            "Hao Liao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20236",
        "abs_url": "https://arxiv.org/abs/2511.20236",
        "pdf_url": "https://arxiv.org/pdf/2511.20236",
        "title": "Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints",
        "authors": [
            "Szymon Bobek",
            "Łukasz Bałec",
            "Grzegorz J. Nalepa"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Counterfactual explanations enhance the actionable interpretability of machine learning models by identifying the minimal changes required to achieve a desired outcome of the model. However, existing methods often ignore the complex dependencies in real-world datasets, leading to unrealistic or impractical modifications. Motivated by cybersecurity applications in the email marketing domain, we propose a method for generating Diverse, Actionable, and kNowledge-Constrained Explanations (DANCE), which incorporates feature dependencies and causal constraints to ensure plausibility and real-world feasibility of counterfactuals. Our method learns linear and nonlinear constraints from data or integrates expert-provided dependency graphs, ensuring counterfactuals are plausible and actionable. By maintaining consistency with feature relationships, the method produces explanations that align with real-world constraints. Additionally, it balances plausibility, diversity, and sparsity, effectively addressing key limitations in existing algorithms. The work is developed based on a real-life case study with Freshmail, the largest email marketing company in Poland and supported by a joint R&D project Sendguard. Furthermore, we provide an extensive evaluation using 140 public datasets, which highlights its ability to generate meaningful, domain-relevant counterfactuals that outperform other existing approaches based on widely used metrics. The source code for reproduction of the results can be found in a GitHub repository we provide.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20285",
        "abs_url": "https://arxiv.org/abs/2511.20285",
        "pdf_url": "https://arxiv.org/pdf/2511.20285",
        "title": "SMoG: Schema Matching on Graph",
        "authors": [
            "Mingyu Jeon",
            "Jaeyoung Suh",
            "Suwan Cho"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Schema matching is a critical task in data integration, par- ticularly in the medical domain where disparate Electronic Health Record (EHR) systems must be aligned to standard models like OMOP CDM. While Large Language Models (LLMs) have shown promise in schema matching, they suf- fer from hallucination and lack of up-to-date domain knowl- edge. Knowledge Graphs (KGs) offer a solution by pro- viding structured, verifiable knowledge. However, existing KG-augmented LLM approaches often rely on inefficient complex multi-hop queries or storage-intensive vector-based retrieval methods. This paper introduces SMoG (Schema Matching on Graph), a novel framework that leverages iter- ative execution of simple 1-hop SPARQL queries, inspired by successful strategies in Knowledge Graph Question An- swering (KGQA). SMoG enhances explainability and relia- bility by generating human-verifiable query paths while sig- nificantly reducing storage requirements by directly querying SPARQL endpoints. Experimental results on real-world med- ical datasets demonstrate that SMoG achieves performance comparable to state-of-the-art baselines, validating its effec- tiveness and efficiency in KG-augmented schema matching.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20297",
        "abs_url": "https://arxiv.org/abs/2511.20297",
        "pdf_url": "https://arxiv.org/pdf/2511.20297",
        "title": "Improving Language Agents through BREW",
        "authors": [
            "Shashank Kirtania",
            "Param Biyani",
            "Priyanshu Gupta",
            "Yasharth Bajpai",
            "Roshni Iyer",
            "Sumit Gulwani",
            "Gustavo Soares"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $\\tau^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20312",
        "abs_url": "https://arxiv.org/abs/2511.20312",
        "pdf_url": "https://arxiv.org/pdf/2511.20312",
        "title": "Data Augmentation Techniques to Reverse-Engineer Neural Network Weights from Input-Output Queries",
        "authors": [
            "Alexander Beiser",
            "Flavio Martinelli",
            "Wulfram Gerstner",
            "Johanni Brea"
        ],
        "comments": "Proceedings of the III edition of the Workshop on Unifying Representations in Neural Models (UniReps 2025)",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Network weights can be reverse-engineered given enough informative samples of a network's input-output function. In a teacher-student setup, this translates into collecting a dataset of the teacher mapping -- querying the teacher -- and fitting a student to imitate such mapping. A sensible choice of queries is the dataset the teacher is trained on. But current methods fail when the teacher parameters are more numerous than the training data, because the student overfits to the queries instead of aligning its parameters to the teacher. In this work, we explore augmentation techniques to best sample the input-output mapping of a teacher network, with the goal of eliciting a rich set of representations from the teacher hidden layers. We discover that standard augmentations such as rotation, flipping, and adding noise, bring little to no improvement to the identification problem. We design new data augmentation techniques tailored to better sample the representational space of the network's hidden layers. With our augmentations we extend the state-of-the-art range of recoverable network sizes. To test their scalability, we show that we can recover networks of up to 100 times more parameters than training data-points.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20321",
        "abs_url": "https://arxiv.org/abs/2511.20321",
        "pdf_url": "https://arxiv.org/pdf/2511.20321",
        "title": "Active Inference in Discrete State Spaces from First Principles",
        "authors": [
            "Patrick Kenny"
        ],
        "comments": "56 pages",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20333",
        "abs_url": "https://arxiv.org/abs/2511.20333",
        "pdf_url": "https://arxiv.org/pdf/2511.20333",
        "title": "NNGPT: Rethinking AutoML with Large Language Models",
        "authors": [
            "Roman Kochnev",
            "Waleed Khalid",
            "Tolgay Atinc Uzun",
            "Xi Zhang",
            "Yashkumar Sanjaybhai Dhameliya",
            "Furui Qin",
            "Chandini Vysyaraju",
            "Raghuvir Duvvuri",
            "Avi Goyal",
            "Dmitry Ignatov",
            "Radu Timofte"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20468",
        "abs_url": "https://arxiv.org/abs/2511.20468",
        "pdf_url": "https://arxiv.org/pdf/2511.20468",
        "title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs",
        "authors": [
            "Yuanhao Li",
            "Mingshan Liu",
            "Hongbo Wang",
            "Yiding Zhang",
            "Yifei Ma",
            "Wei Tan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and this http URL works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic this http URL-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20471",
        "abs_url": "https://arxiv.org/abs/2511.20471",
        "pdf_url": "https://arxiv.org/pdf/2511.20471",
        "title": "Universe of Thoughts: Enabling Creative Reasoning with Large Language Models",
        "authors": [
            "Yuto Suzuki",
            "Farnoush Banaei-Kashani"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \\textit{combinational}, \\textit{exploratory}, and \\textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \\textit{Universe of Thoughts} (or \\textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20497",
        "abs_url": "https://arxiv.org/abs/2511.20497",
        "pdf_url": "https://arxiv.org/pdf/2511.20497",
        "title": "Quantifying the Privacy Implications of High-Fidelity Synthetic Network Traffic",
        "authors": [
            "Van Tran",
            "Shinan Liu",
            "Tian Li",
            "Nick Feamster"
        ],
        "comments": "14 pages, 13 Figures, 6 Tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "To address the scarcity and privacy concerns of network traffic data, various generative models have been developed to produce synthetic traffic. However, synthetic traffic is not inherently privacy-preserving, and the extent to which it leaks sensitive information, and how to measure such leakage, remain largely unexplored. This challenge is further compounded by the diversity of model architectures, which shape how traffic is represented and synthesized. We introduce a comprehensive set of privacy metrics for synthetic network traffic, combining standard approaches like membership inference attacks (MIA) and data extraction attacks with network-specific identifiers and attributes. Using these metrics, we systematically evaluate the vulnerability of different representative generative models and examine the factors that influence attack success. Our results reveal substantial variability in privacy risks across models and datasets. MIA success ranges from 0% to 88%, and up to 100% of network identifiers can be recovered from generated traffic, highlighting serious privacy vulnerabilities. We further identify key factors that significantly affect attack outcomes, including training data diversity and how well the generative model fits the training data. These findings provide actionable guidance for designing and deploying generative models that minimize privacy leakage, establishing a foundation for safer synthetic network traffic generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20510",
        "abs_url": "https://arxiv.org/abs/2511.20510",
        "pdf_url": "https://arxiv.org/pdf/2511.20510",
        "title": "FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization",
        "authors": [
            "Yuto Suzuki",
            "Paul Awolade",
            "Daniel V. LaBarbera",
            "Farnoush Banaei-Kashani"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a \"vocabulary selection\" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20526",
        "abs_url": "https://arxiv.org/abs/2511.20526",
        "pdf_url": "https://arxiv.org/pdf/2511.20526",
        "title": "Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam",
        "authors": [
            "Xinran Wang",
            "Boran Zhu",
            "Shujuan Zhou",
            "Ziwen Long",
            "Dehua Zhou",
            "Shu Zhang"
        ],
        "comments": "15 pages, 4 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain this http URL China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20586",
        "abs_url": "https://arxiv.org/abs/2511.20586",
        "pdf_url": "https://arxiv.org/pdf/2511.20586",
        "title": "PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic",
        "authors": [
            "Koffi Ismael Ouattara",
            "Ioannis Krontiris",
            "Theo Dimitrakos",
            "Dennis Eisermann",
            "Frank Kargl"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics such as accuracy and precision fail to capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the \\emph{Parallel Trust Assessment System (PaTAS)}, a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through \\emph{Trust Nodes} and \\emph{Trust Functions} that propagate input, parameter, and activation trust across the network. The framework defines a \\emph{Parameter Trust Update} mechanism to refine parameter reliability during training and an \\emph{Inference-Path Trust Assessment (IPTA)} method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a principled foundation for evaluating model reliability across the AI lifecycle.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20610",
        "abs_url": "https://arxiv.org/abs/2511.20610",
        "pdf_url": "https://arxiv.org/pdf/2511.20610",
        "title": "Building a Foundation Model for Trajectory from Scratch",
        "authors": [
            "Gaspard Merten",
            "Mahmoud Sakr",
            "Gilles Dejaegere"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Foundation models are transformative in artificial intelligence, but building them from scratch, especially for mobility trajectories, is not yet clear or documented. This tutorial bridges this gap by demonstrating the steps and code of a minimal implementation of a trajectory-focused foundation model starting from GPT-2. Through a concise, step-by-step, code-driven process, we demonstrate adapting GPT-2 for spatiotemporal data. We then review and compare representative trajectory foundation models, such as TrajFM and TrajGPT, highlighting their architectural innovations and differences. Additionally, we introduce complementary techniques from related domains, like TimesFM's patching approach. Targeted at researchers and practitioners, this tutorial aims to explain the concepts and terminology of foundation models, at the implementation level. We find it timely and indispensable to create this educational material in order to support the SIGSPATIAL community in building and evaluating mobility foundation models, enhancing both research clarity and peer-review effectiveness in mobility AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20623",
        "abs_url": "https://arxiv.org/abs/2511.20623",
        "pdf_url": "https://arxiv.org/pdf/2511.20623",
        "title": "Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development",
        "authors": [
            "David Szczecina",
            "Senan Gaffori",
            "Edmond Li"
        ],
        "comments": "4 pages, 3 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20627",
        "abs_url": "https://arxiv.org/abs/2511.20627",
        "pdf_url": "https://arxiv.org/pdf/2511.20627",
        "title": "Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems",
        "authors": [
            "Anastasia Mavridou",
            "Divya Gopinath",
            "Corina S. Păsăreanu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The integration of AI components, particularly Deep Neural Networks (DNNs), into safety-critical systems such as aerospace and autonomous vehicles presents fundamental challenges for assurance. The opacity of AI systems, combined with the semantic gap between high-level requirements and low-level network representations, creates barriers to traditional verification approaches. These AI-specific challenges are amplified by longstanding issues in Requirements Engineering, including ambiguity in natural language specifications and scalability bottlenecks in formalization. We propose an approach that leverages AI itself to address these challenges through two complementary components. REACT (Requirements Engineering with AI for Consistency and Testing) employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications, enabling early verification and validation. SemaLens (Semantic Analysis of Visual Perception using large Multi-modal models) utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, these components provide a comprehensive pipeline from informal requirements to validated implementations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2507.01196",
        "abs_url": "https://arxiv.org/abs/2507.01196",
        "pdf_url": "https://arxiv.org/pdf/2507.01196",
        "title": "Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning",
        "authors": [
            "Na Lee",
            "Konstantinos Barmpas",
            "Yannis Panagakis",
            "Dimitrios Adamos",
            "Nikolaos Laskaris",
            "Stefanos Zafeiriou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Human-Computer Interaction (cs.HC)",
        "abstract": "Foundation Models have demonstrated significant success across various domains in Artificial Intelligence (AI), yet their capabilities for brainwave modeling remain unclear. In this paper, we comprehensively evaluate current Large Brainwave Foundation Models (LBMs) through systematic fine-tuning experiments across multiple Brain-Computer Interface (BCI) benchmark tasks, including memory tasks and sleep stage classification. Our extensive analysis shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%) over traditional deep architectures while requiring significantly more parameters (millions vs thousands), raising important questions about their efficiency and applicability in BCI contexts. Moreover, through detailed ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce trainable parameters without performance degradation, while demonstrating that architectural and training inefficiencies limit LBMs' current capabilities. Our experiments span both full model fine-tuning and parameter-efficient adaptation techniques, providing insights into optimal training strategies for BCI applications. We pioneer the application of LoRA to LBMs, revealing that performance benefits generally emerge when adapting multiple neural network components simultaneously. These findings highlight the critical need for domain-specific development strategies to advance LBMs, suggesting that current architectures may require redesign to fully leverage the potential of foundation models in brainwave analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.14198",
        "abs_url": "https://arxiv.org/abs/2511.14198",
        "pdf_url": "https://arxiv.org/pdf/2511.14198",
        "title": "DiverseClaire: Simulating Students to Improve Introductory Programming Course Materials for All CS1 Learners",
        "authors": [
            "Wendy Wong",
            "Yuchao Jiang",
            "Yuekang Li"
        ],
        "comments": "2 pages",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Programming Languages (cs.PL)",
        "abstract": "Although CS programs are booming, introductory courses like CS1 still adopt a one-size-fits-all formats that can exacerbate cognitive load and discourage learners with autism, ADHD, dyslexia and other neurological conditions. These call for compassionate pedagogies and Universal Design For Learning (UDL) to create learning environments and materials where cognitive diversity is welcomed. To address this, we introduce DiverseClaire a pilot study, which simulates students including neurodiverse profiles using LLMs and diverse personas. By leveraging Bloom's Taxonomy and UDL, DiverseClaire compared UDL-transformed lecture slides with traditional formats. To evaluate DiverseClaire controlled experiments, we used the evaluation metric the average score. The findings revealed that the simulated neurodiverse students struggled with learning due to lecture slides that were in inaccessible formats. These results highlight the need to provide course materials in multiple formats for diverse learner preferences. Data from our pilot study will be made available to assist future CS1 instructors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.17645",
        "abs_url": "https://arxiv.org/abs/2511.17645",
        "pdf_url": "https://arxiv.org/pdf/2511.17645",
        "title": "BlockCert: Certified Blockwise Extraction of Transformer Mechanisms",
        "authors": [
            "Sandro Andric"
        ],
        "comments": "16 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19457",
        "abs_url": "https://arxiv.org/abs/2511.19457",
        "pdf_url": "https://arxiv.org/pdf/2511.19457",
        "title": "SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference",
        "authors": [
            "Ziyang Zhang",
            "Jie Liu",
            "Luca Mottola"
        ],
        "comments": "14 pages, 12 figures",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid inference methods, however, typically overlook how operator characteristics impact performance. In this work, we present SparOA, a CPU-GPU hybrid inference framework, which leverages both sparsity and computational intensity to optimize operator scheduling. SparOA embraces aforementioned challenges through three key components: (1) a threshold predictor that accurately determines optimal sparsity and computational intensity thresholds; (2) a reinforcement learning-based scheduler that dynamically optimizes resource allocation based on real-time hardware states; and (3) a hybrid inference engine that enhances efficiency through asynchronous execution and batch size this http URL results show that SparOA achieves an average speedup of 1.22-1.31x compared to all baselines, and outperforms the CPU-Only by up to 50.7x. Also, SparOA achieves optimal energy-per-inference, consuming 7\\%-16\\% less energy than the SOTA co-execution baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19460",
        "abs_url": "https://arxiv.org/abs/2511.19460",
        "pdf_url": "https://arxiv.org/pdf/2511.19460",
        "title": "Systemic approach for modeling a generic smart grid",
        "authors": [
            "Sofiane Ben Amor",
            "Guillaume Guerard",
            "Loup-Noé Levy"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19464",
        "abs_url": "https://arxiv.org/abs/2511.19464",
        "pdf_url": "https://arxiv.org/pdf/2511.19464",
        "title": "Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments",
        "authors": [
            "Marcio Pohlmann",
            "Alex Severo",
            "Gefté Almeida",
            "Diego Kreutz",
            "Tiago Heinrich",
            "Lourenço Pereira"
        ],
        "comments": "5 pages, 3 figures, 2 tables, submitted to ERRC/WRSeg 2025",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19465",
        "abs_url": "https://arxiv.org/abs/2511.19465",
        "pdf_url": "https://arxiv.org/pdf/2511.19465",
        "title": "Hidden markov model to predict tourists visited place",
        "authors": [
            "Theo Demessance",
            "Chongke Bi",
            "Sonia Djebali",
            "Guillaume Guerard"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Nowadays, social networks are becoming a popular way of analyzing tourist behavior, thanks to the digital traces left by travelers during their stays on these networks. The massive amount of data generated; by the propensity of tourists to share comments and photos during their trip; makes it possible to model their journeys and analyze their behavior. Predicting the next movement of tourists plays a key role in tourism marketing to understand demand and improve decision support. In this paper, we propose a method to understand and to learn tourists' movements based on social network data analysis to predict future movements. The method relies on a machine learning grammatical inference algorithm. A major contribution in this paper is to adapt the grammatical inference algorithm to the context of big data. Our method produces a hidden Markov model representing the movements of a group of tourists. The hidden Markov model is flexible and editable with new data. The capital city of France, Paris is selected to demonstrate the efficiency of the proposed methodology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19470",
        "abs_url": "https://arxiv.org/abs/2511.19470",
        "pdf_url": "https://arxiv.org/pdf/2511.19470",
        "title": "Quantifying Modality Contributions via Disentangling Multimodal Representations",
        "authors": [
            "Padegal Amit",
            "Omkar Mahesh Kashyap",
            "Namitha Rayasam",
            "Nidhi Shekhar",
            "Surabhi Narayan"
        ],
        "comments": "16 pages, 11 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19472",
        "abs_url": "https://arxiv.org/abs/2511.19472",
        "pdf_url": "https://arxiv.org/pdf/2511.19472",
        "title": "PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer",
        "authors": [
            "Ruogu Ding",
            "Xin Ning",
            "Ulf Schlichtmann",
            "Weikang Qian"
        ],
        "comments": "An extended version that has been accepted by AAAI-2026 conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)",
        "abstract": "Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19473",
        "abs_url": "https://arxiv.org/abs/2511.19473",
        "pdf_url": "https://arxiv.org/pdf/2511.19473",
        "title": "WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning",
        "authors": [
            "Haojin Yang",
            "Rui Hu",
            "Zequn Sun",
            "Rui Zhou",
            "Yujun Cai",
            "Yiwei Wang"
        ],
        "comments": "19 pages. 3 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19475",
        "abs_url": "https://arxiv.org/abs/2511.19475",
        "pdf_url": "https://arxiv.org/pdf/2511.19475",
        "title": "Tracking and Segmenting Anything in Any Modality",
        "authors": [
            "Tianlu Zhang",
            "Qiang Zhang",
            "Guiguang Ding",
            "Jungong Han"
        ],
        "comments": "Accpetd by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Multimedia (cs.MM)",
        "abstract": "Tracking and segmentation play essential roles in video understanding, providing basic positional information and temporal association of objects within video sequences. Despite their shared objective, existing approaches often tackle these tasks using specialized architectures or modality-specific parameters, limiting their generalization and scalability. Recent efforts have attempted to unify multiple tracking and segmentation subtasks from the perspectives of any modality input or multi-task inference. However, these approaches tend to overlook two critical challenges: the distributional gap across different modalities and the feature representation gap across tasks. These issues hinder effective cross-task and cross-modal knowledge sharing, ultimately constraining the development of a true generalist model. To address these limitations, we propose a universal tracking and segmentation framework named SATA, which unifies a broad spectrum of tracking and segmentation subtasks with any modality input. Specifically, a Decoupled Mixture-of-Expert (DeMoE) mechanism is presented to decouple the unified representation learning task into the modeling process of cross-modal shared knowledge and specific information, thus enabling the model to maintain flexibility while enhancing generalization. Additionally, we introduce a Task-aware Multi-object Tracking (TaMOT) pipeline to unify all the task outputs as a unified set of instances with calibrated ID information, thereby alleviating the degradation of task-specific knowledge during multi-task training. SATA demonstrates superior performance on 18 challenging tracking and segmentation benchmarks, offering a novel perspective for more generalizable video understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19476",
        "abs_url": "https://arxiv.org/abs/2511.19476",
        "pdf_url": "https://arxiv.org/pdf/2511.19476",
        "title": "FAST: Topology-Aware Frequency-Domain Distribution Matching for Coreset Selection",
        "authors": [
            "Jin Cui",
            "Boran Zhao",
            "Jiajun Xu",
            "Jiaqi Guo",
            "Shuo Guan",
            "Pengju Ren"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Coreset selection compresses large datasets into compact, representative subsets, reducing the energy and computational burden of training deep neural networks. Existing methods are either: (i) DNN-based, which are tied to model-specific parameters and introduce architectural bias; or (ii) DNN-free, which rely on heuristics lacking theoretical guarantees. Neither approach explicitly constrains distributional equivalence, largely because continuous distribution matching is considered inapplicable to discrete sampling. Moreover, prevalent metrics (e.g., MSE, KL, MMD, CE) cannot accurately capture higher-order moment discrepancies, leading to suboptimal coresets. In this work, we propose FAST, the first DNN-free distribution-matching coreset selection framework that formulates the coreset selection task as a graph-constrained optimization problem grounded in spectral graph theory and employs the Characteristic Function Distance (CFD) to capture full distributional information in the frequency domain. We further discover that naive CFD suffers from a \"vanishing phase gradient\" issue in medium and high-frequency regions; to address this, we introduce an Attenuated Phase-Decoupled CFD. Furthermore, for better convergence, we design a Progressive Discrepancy-Aware Sampling strategy that progressively schedules frequency selection from low to high, preserving global structure before refining local details and enabling accurate matching with fewer frequencies while avoiding overfitting. Extensive experiments demonstrate that FAST significantly outperforms state-of-the-art coreset selection methods across all evaluated benchmarks, achieving an average accuracy gain of 9.12%. Compared to other baseline coreset methods, it reduces power consumption by 96.57% and achieves a 2.2x average speedup, underscoring its high performance and energy efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19480",
        "abs_url": "https://arxiv.org/abs/2511.19480",
        "pdf_url": "https://arxiv.org/pdf/2511.19480",
        "title": "Exploiting the Experts: Unauthorized Compression in MoE-LLMs",
        "authors": [
            "Pinaki Prasad Guha Neogi",
            "Ahmad Mohammadshirazi",
            "Dheeraj Kulshrestha",
            "Rajiv Ramnath"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19482",
        "abs_url": "https://arxiv.org/abs/2511.19482",
        "pdf_url": "https://arxiv.org/pdf/2511.19482",
        "title": "Human Experts' Evaluation of Generative AI for Contextualizing STEAM Education in the Global South",
        "authors": [
            "Matthew Nyaaba",
            "Macharious Nabang",
            "Patrick Kyeremeh",
            "Ibrahim Nantomah",
            "Collins Owusu-Fordjour",
            "Martin Ako",
            "Bismark Nyaaba Akanzire",
            "Kassim Korah Nantom",
            "Cecilia Issaka",
            "Xiaoming Zhai"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "This study investigates how human experts evaluate the capacity of Generative AI (GenAI) to contextualize STEAM education in the Global South, with a focus on Ghana. Using a convergent mixed-methods design, four STEAM specialists assessed GenAI-generated lesson plans created with a customized Culturally Responsive Lesson Planner (CRLP) and compared them to standardized lesson plans from the Ghana National Council for Curriculum and Assessment (NaCCA). Quantitative ratings were based on a validated 25-item Culturally Responsive Pedagogy Rubric measuring bias awareness, cultural representation, contextual relevance, linguistic responsiveness, and teacher agency. Qualitative reflections provided additional insight into how GenAI handles cultural and pedagogical appropriateness. Findings show that GenAI, when paired with the CRLP tool, can support contextualized STEAM instruction by linking abstract curriculum standards to learners' cultural knowledge, community practices, and everyday experiences. Experts rated GenAI-assisted lessons as more culturally grounded and pedagogically responsive than NaCCA plans, integrating Indigenous knowledge, bilingual elements, and locally relevant examples. However, GenAI struggled to represent Ghana's cultural pluralism, often offering surface-level references to language, history, and identity. These weaknesses were most evident in Mathematics and Computing, where cultural nuance was limited. The results highlight the need for continued teacher mediation, community involvement, and culturally attuned refinement of AI outputs. Future work should include classroom trials, expanded expert participation, and model fine-tuning using Indigenous language corpora to strengthen cultural fidelity in Global South contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19483",
        "abs_url": "https://arxiv.org/abs/2511.19483",
        "pdf_url": "https://arxiv.org/pdf/2511.19483",
        "title": "Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation",
        "authors": [
            "Qingsong He",
            "Jing Nan",
            "Jiayu Jiao",
            "Liangjie Tang",
            "Xiaodong Xu",
            "Mengmeng Sun",
            "Qingyao Wang",
            "Minghui Yan"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\\% while achieving a 92\\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19486",
        "abs_url": "https://arxiv.org/abs/2511.19486",
        "pdf_url": "https://arxiv.org/pdf/2511.19486",
        "title": "Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification",
        "authors": [
            "Lei Wang",
            "Zikun Ye",
            "Jinglong Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19488",
        "abs_url": "https://arxiv.org/abs/2511.19488",
        "pdf_url": "https://arxiv.org/pdf/2511.19488",
        "title": "Building Resilient Information Ecosystems: Large LLM-Generated Dataset of Persuasion Attacks",
        "authors": [
            "Hsien-Te Kao",
            "Aleksey Panasyuk",
            "Peter Bautista",
            "William Dupree",
            "Gabriel Ganberg",
            "Jeffrey M. Beaubien",
            "Laura Cassani",
            "Svitlana Volkova"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Organization's communication is essential for public trust, but the rise of generative AI models has introduced significant challenges by generating persuasive content that can form competing narratives with official messages from government and commercial organizations at speed and scale. This has left agencies in a reactive position, often unaware of how these models construct their persuasive strategies, making it more difficult to sustain communication effectiveness. In this paper, we introduce a large LLM-generated persuasion attack dataset, which includes 134,136 attacks generated by GPT-4, Gemma 2, and Llama 3.1 on agency news. These attacks span 23 persuasive techniques from SemEval 2023 Task 3, directed toward 972 press releases from ten agencies. The generated attacks come in two mediums, press release statements and social media posts, covering both long-form and short-form communication strategies. We analyzed the moral resonance of these persuasion attacks to understand their attack vectors. GPT-4's attacks mainly focus on Care, with Authority and Loyalty also playing a role. Gemma 2 emphasizes Care and Authority, while Llama 3.1 centers on Loyalty and Care. Analyzing LLM-generated persuasive attacks across models will enable proactive defense, allow to create the reputation armor for organizations, and propel the development of both effective and resilient communications in the information ecosystem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19489",
        "abs_url": "https://arxiv.org/abs/2511.19489",
        "pdf_url": "https://arxiv.org/pdf/2511.19489",
        "title": "Evolution without an Oracle: Driving Effective Evolution with LLM Judges",
        "authors": [
            "Zhe Zhao",
            "Yuheng Yang",
            "Haibin Wen",
            "Xiaojie Qiu",
            "Zaixi Zhang",
            "Qingfu Zhang"
        ],
        "comments": "14 pages, 5 figures",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through \"Problem Specification.\" By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing \"computable metrics\" to \"describable qualities,\" thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19490",
        "abs_url": "https://arxiv.org/abs/2511.19490",
        "pdf_url": "https://arxiv.org/pdf/2511.19490",
        "title": "Generative Model-Aided Continual Learning for CSI Feedback in FDD mMIMO-OFDM Systems",
        "authors": [
            "Guijun Liu",
            "Yuwen Cao",
            "Tomoaki Ohtsuki",
            "Jiguang He",
            "Shahid Mumtaz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Theory (cs.IT)",
        "abstract": "Deep autoencoder (DAE) frameworks have demonstrated their effectiveness in reducing channel state information (CSI) feedback overhead in massive multiple-input multiple-output (mMIMO) orthogonal frequency division multiplexing (OFDM) systems. However, existing CSI feedback models struggle to adapt to dynamic environments caused by user mobility, requiring retraining when encountering new CSI distributions. Moreover, returning to previously encountered environments often leads to performance degradation due to catastrophic forgetting. Continual learning involves enabling models to incorporate new information while maintaining performance on previously learned tasks. To address these challenges, we propose a generative adversarial network (GAN)-based learning approach for CSI feedback. By using a GAN generator as a memory unit, our method preserves knowledge from past environments and ensures consistently high performance across diverse scenarios without forgetting. Simulation results show that the proposed approach enhances the generalization capability of the DAE framework while maintaining low memory overhead. Furthermore, it can be seamlessly integrated with other advanced CSI feedback models, highlighting its robustness and adaptability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19492",
        "abs_url": "https://arxiv.org/abs/2511.19492",
        "pdf_url": "https://arxiv.org/pdf/2511.19492",
        "title": "Forecasting AI Time Horizon Under Compute Slowdowns",
        "authors": [
            "Parker Whitfill",
            "Ben Snodin",
            "Joel Becker"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "METR's time horizon metric has grown exponentially since 2019, along with compute. However, it is unclear whether compute scaling will persist at current rates through 2030, raising the question of how possible compute slowdowns might impact AI agent capability forecasts. Given a model of time horizon as a function of training compute and algorithms, along with a model of how compute investment spills into algorithmic progress (which, notably, precludes the possibility of a software-only singularity), and the empirical fact that both time horizon and compute have grown at constant rates over 2019--2025, we derive that time horizon growth must be proportional to compute growth. We provide additional, albeit limited, experimental evidence consistent with this theory. We use our model to project time horizon growth under OpenAI's compute projection, finding substantial projected delays in some cases. For example, 1-month time horizons at $80\\%$ reliability occur $7$ years later than simple trend extrapolation suggests.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19495",
        "abs_url": "https://arxiv.org/abs/2511.19495",
        "pdf_url": "https://arxiv.org/pdf/2511.19495",
        "title": "A Systematic Study of Compression Ordering for Large Language Models",
        "authors": [
            "Shivansh Chhawri",
            "Rahul Mahadik",
            "Suparna Rooj"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19496",
        "abs_url": "https://arxiv.org/abs/2511.19496",
        "pdf_url": "https://arxiv.org/pdf/2511.19496",
        "title": "Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM",
        "authors": [
            "Yang Liu",
            "Xiaolong Zhong",
            "Ling Jiang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \\textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \\emph{drop-in agent core}. Training with maximal-update parameterization ($\\mu$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \\emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \\textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\\,\\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\\footnote{this https URL and this https URL (training checkpoints).} Training code and evaluation harness: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19497",
        "abs_url": "https://arxiv.org/abs/2511.19497",
        "pdf_url": "https://arxiv.org/pdf/2511.19497",
        "title": "PeriodNet: Boosting the Potential of Attention Mechanism for Time Series Forecasting",
        "authors": [
            "Bowen Zhao",
            "Huanlai Xing",
            "Zhiwen Xiao",
            "Jincheng Peng",
            "Li Feng",
            "Xinhan Wang",
            "Rong Qu",
            "Hui Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The attention mechanism has demonstrated remarkable potential in sequence modeling, exemplified by its successful application in natural language processing with models such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT). Despite these advancements, its utilization in time series forecasting (TSF) has yet to meet expectations. Exploring a better network structure for attention in TSF holds immense significance across various domains. In this paper, we present PeriodNet with a brand new structure to forecast univariate and multivariate time series. PeriodNet incorporates period attention and sparse period attention mechanism for analyzing adjacent periods. It enhances the mining of local characteristics, periodic patterns, and global dependencies. For efficient cross-variable modeling, we introduce an iterative grouping mechanism which can directly reduce the cross-variable redundancy. To fully leverage the extracted features on the encoder side, we redesign the entire architecture of the vanilla Transformer and propose a period diffuser for precise multi-period prediction. Through comprehensive experiments conducted on eight datasets, we demonstrate that PeriodNet outperforms six state-of-the-art models in both univariate and multivariate TSF scenarios in terms of mean square error and mean absolute error. In particular, PeriodNet achieves a relative improvement of 22% when forecasting time series with a length of 720, in comparison to other models based on the conventional encoder-decoder Transformer architecture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19498",
        "abs_url": "https://arxiv.org/abs/2511.19498",
        "pdf_url": "https://arxiv.org/pdf/2511.19498",
        "title": "Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data",
        "authors": [
            "Yi Zhang",
            "Tianxiang Xu",
            "Zijian Li",
            "Chao Zhang",
            "Kunyu Zhang",
            "Zhan Gao",
            "Meinuo Li",
            "Xiaohan Zhang",
            "Qichao Qi",
            "Bing Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19500",
        "abs_url": "https://arxiv.org/abs/2511.19500",
        "pdf_url": "https://arxiv.org/pdf/2511.19500",
        "title": "CycleChemist: A Dual-Pronged Machine Learning Framework for Organic Photovoltaic Discovery",
        "authors": [
            "Hou Hei Lam",
            "Jiangjie Qiu",
            "Xiuyuan Hu",
            "Wentao Li",
            "Fankun Zeng",
            "Siwei Fu",
            "Hao Zhang",
            "Xiaonan Wang"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Organic photovoltaic (OPV) materials offer a promising path toward sustainable energy generation, but their development is limited by the difficulty of identifying high performance donor and acceptor pairs with strong power conversion efficiencies (PCEs). Existing design strategies typically focus on either the donor or the acceptor alone, rather than using a unified approach capable of modeling both components. In this work, we introduce a dual machine learning framework for OPV discovery that combines predictive modeling with generative molecular design. We present the Organic Photovoltaic Donor Acceptor Dataset (OPV2D), the largest curated dataset of its kind, containing 2000 experimentally characterized donor acceptor pairs. Using this dataset, we develop the Organic Photovoltaic Classifier (OPVC) to predict whether a material exhibits OPV behavior, and a hierarchical graph neural network that incorporates multi task learning and donor acceptor interaction modeling. This framework includes the Molecular Orbital Energy Estimator (MOE2) for predicting HOMO and LUMO energy levels, and the Photovoltaic Performance Predictor (P3) for estimating PCE. In addition, we introduce the Material Generative Pretrained Transformer (MatGPT) to produce synthetically accessible organic semiconductors, guided by a reinforcement learning strategy with three objective policy optimization. By linking molecular representation learning with performance prediction, our framework advances data driven discovery of high performance OPV materials.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19528",
        "abs_url": "https://arxiv.org/abs/2511.19528",
        "pdf_url": "https://arxiv.org/pdf/2511.19528",
        "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories",
        "authors": [
            "Rushuai Yang",
            "Zhiyuan Feng",
            "Tianxiang Zhang",
            "Kaixin Wang",
            "Chuheng Zhang",
            "Li Zhao",
            "Xiu Su",
            "Yi Chen",
            "Jiang Bian"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19536",
        "abs_url": "https://arxiv.org/abs/2511.19536",
        "pdf_url": "https://arxiv.org/pdf/2511.19536",
        "title": "AttackPilot: Autonomous Inference Attacks Against ML Services With LLM-Based Agents",
        "authors": [
            "Yixin Wu",
            "Rui Wen",
            "Chi Cui",
            "Michael Backes",
            "Yang Zhang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Inference attacks have been widely studied and offer a systematic risk assessment of ML services; however, their implementation and the attack parameters for optimal estimation are challenging for non-experts. The emergence of advanced large language models presents a promising yet largely unexplored opportunity to develop autonomous agents as inference attack experts, helping address this challenge. In this paper, we propose AttackPilot, an autonomous agent capable of independently conducting inference attacks without human intervention. We evaluate it on 20 target services. The evaluation shows that our agent, using GPT-4o, achieves a 100.0% task completion rate and near-expert attack performance, with an average token cost of only $0.627 per run. The agent can also be powered by many other representative LLMs and can adaptively optimize its strategy under service constraints. We further perform trace analysis, demonstrating that design choices, such as a multi-agent framework and task-specific action spaces, effectively mitigate errors such as bad plans, inability to follow instructions, task context loss, and hallucinations. We anticipate that such agents could empower non-expert ML service providers, auditors, or regulators to systematically assess the risks of ML services without requiring deep domain expertise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19548",
        "abs_url": "https://arxiv.org/abs/2511.19548",
        "pdf_url": "https://arxiv.org/pdf/2511.19548",
        "title": "When Should Neural Data Inform Welfare? A Critical Framework for Policy Uses of Neuroeconomics",
        "authors": [
            "Yiven"
        ],
        "comments": "Durham Economic Journal 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); General Economics (econ.GN); Neurons and Cognition (q-bio.NC)",
        "abstract": "Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, \"brain-based\" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies \"true\" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19550",
        "abs_url": "https://arxiv.org/abs/2511.19550",
        "pdf_url": "https://arxiv.org/pdf/2511.19550",
        "title": "The Semiotic Channel Principle: Measuring the Capacity for Meaning in LLM Communication",
        "authors": [
            "Davide Picca"
        ],
        "comments": "",
        "subjects": "Information Theory (cs.IT); Artificial Intelligence (cs.AI)",
        "abstract": "This paper proposes a novel semiotic framework for analyzing Large Language Models (LLMs), conceptualizing them as stochastic semiotic engines whose outputs demand active, asymmetric human interpretation. We formalize the trade-off between expressive richness (semiotic breadth) and interpretive stability (decipherability) using information-theoretic tools. Breadth is quantified as source entropy, and decipherability as the mutual information between messages and human interpretations. We introduce a generative complexity parameter (lambda) that governs this trade-off, as both breadth and decipherability are functions of lambda. The core trade-off is modeled as an emergent property of their distinct responses to $\\lambda$. We define a semiotic channel, parameterized by audience and context, and posit a capacity constraint on meaning transmission, operationally defined as the maximum decipherability by optimizing lambda. This reframing shifts analysis from opaque model internals to observable textual artifacts, enabling empirical measurement of breadth and decipherability. We demonstrate the framework's utility across four key applications: (i) model profiling; (ii) optimizing prompt/context design; (iii) risk analysis based on ambiguity; and (iv) adaptive semiotic systems. We conclude that this capacity-based semiotic approach offers a rigorous, actionable toolkit for understanding, evaluating, and designing LLM-mediated communication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19555",
        "abs_url": "https://arxiv.org/abs/2511.19555",
        "pdf_url": "https://arxiv.org/pdf/2511.19555",
        "title": "Online Sparse Feature Selection in Data Streams via Differential Evolution",
        "authors": [
            "Ruiyang Xu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19558",
        "abs_url": "https://arxiv.org/abs/2511.19558",
        "pdf_url": "https://arxiv.org/pdf/2511.19558",
        "title": "SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models",
        "authors": [
            "Mohammed Talha Alam",
            "Nada Saadi",
            "Fahad Shamshad",
            "Nils Lukas",
            "Karthik Nandakumar",
            "Fahkri Karray",
            "Samuele Poppi"
        ],
        "comments": "20 pages, 8 figures, 10 tables",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19562",
        "abs_url": "https://arxiv.org/abs/2511.19562",
        "pdf_url": "https://arxiv.org/pdf/2511.19562",
        "title": "Trust-Based Social Learning for Communication (TSLEC) Protocol Evolution in Multi-Agent Reinforcement Learning",
        "authors": [
            "Abraham Itzhak Weinberg"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "Emergent communication in multi-agent systems typically occurs through independent learning, resulting in slow convergence and potentially suboptimal protocols. We introduce TSLEC (Trust-Based Social Learning with Emergent Communication), a framework where agents explicitly teach successful strategies to peers, with knowledge transfer modulated by learned trust relationships. Through experiments with 100 episodes across 30 random seeds, we demonstrate that trust-based social learning reduces episodes-to-convergence by 23.9% (p < 0.001, Cohen's d = 1.98) compared to independent emergence, while producing compositional protocols (C = 0.38) that remain robust under dynamic objectives (Phi > 0.867 decoding accuracy). Trust scores strongly correlate with teaching quality (r = 0.743, p < 0.001), enabling effective knowledge filtering. Our results establish that explicit social learning fundamentally accelerates emergent communication in multi-agent coordination.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19565",
        "abs_url": "https://arxiv.org/abs/2511.19565",
        "pdf_url": "https://arxiv.org/pdf/2511.19565",
        "title": "Deductive Systems for Logic Programs with Counting",
        "authors": [
            "Jorge Fandinno",
            "Vladimir Lifschitz"
        ],
        "comments": "Under consideration in Theory and Practice of Logic Programming (TPLP)",
        "subjects": "Logic in Computer Science (cs.LO); Artificial Intelligence (cs.AI)",
        "abstract": "In answer set programming, two groups of rules are considered strongly equivalent if they have the same meaning in any context. Strong equivalence of two programs can be sometimes established by deriving rules of each program from rules of the other in an appropriate deductive system. This paper shows how to extend this method of proving strong equivalence to programs containing the counting aggregate.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19580",
        "abs_url": "https://arxiv.org/abs/2511.19580",
        "pdf_url": "https://arxiv.org/pdf/2511.19580",
        "title": "Towards Synergistic Teacher-AI Interactions with Generative Artificial Intelligence",
        "authors": [
            "Mutlu Cukurova",
            "Wannapon Suraworachet",
            "Qi Zhou",
            "Sahan Bulathwela"
        ],
        "comments": "18 pages, 6 pages",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Generative artificial intelligence (GenAI) is increasingly used in education, posing significant challenges for teachers adapting to these changes. GenAI offers unprecedented opportunities for accessibility, scalability and productivity in educational tasks. However, the automation of teaching tasks through GenAI raises concerns about reduced teacher agency, potential cognitive atrophy, and the broader deprofessionalisation of teaching. Drawing findings from prior literature on AI in Education, and refining through a recent systematic literature review, this chapter presents a conceptualisation of five levels of teacher-AI teaming: transactional, situational, operational, praxical and synergistic teaming. The framework aims to capture the nuanced dynamics of teacher-AI interactions, particularly with GenAI, that may lead to the replacement, complementarity, or augmentation of teachers' competences and professional practice. GenAI technological affordances required in supporting teaming, along with empirical studies, are discussed. Drawing on empirical observations, we outline a future vision that moves beyond individual teacher agency toward collaborative decision-making between teachers and AI, in which both agents engage in negotiation, constructive challenge, and co-reasoning that enhance each other's capabilities and enable outcomes neither could realise independently. Further discussion of socio-technical factors beyond teacher-AI teaming is also included to streamline the synergy of teachers and AI in education ethically and practically.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19636",
        "abs_url": "https://arxiv.org/abs/2511.19636",
        "pdf_url": "https://arxiv.org/pdf/2511.19636",
        "title": "Many Ways to be Right: Rashomon Sets for Concept-Based Neural Networks",
        "authors": [
            "Shihan Feng",
            "Cheng Zhang",
            "Michael Xi",
            "Ethan Hsu",
            "Lesia Semenova",
            "Chudi Zhong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Modern neural networks rarely have a single way to be right. For many tasks, multiple models can achieve identical performance while relying on different features or reasoning patterns, a property known as the Rashomon Effect. However, uncovering this diversity in deep architectures is challenging as their continuous parameter spaces contain countless near-optimal solutions that are numerically distinct but often behaviorally similar. We introduce Rashomon Concept Bottleneck Models, a framework that learns multiple neural networks which are all accurate yet reason through distinct human-understandable concepts. By combining lightweight adapter modules with a diversity-regularized training objective, our method constructs a diverse set of deep concept-based models efficiently without retraining from scratch. The resulting networks provide fundamentally different reasoning processes for the same predictions, revealing how concept reliance and decision making vary across equally performing solutions. Our framework enables systematic exploration of data-driven reasoning diversity in deep models, offering a new mechanism for auditing, comparison, and alignment across equally accurate solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19644",
        "abs_url": "https://arxiv.org/abs/2511.19644",
        "pdf_url": "https://arxiv.org/pdf/2511.19644",
        "title": "IRSDA: An Agent-Orchestrated Framework for Enterprise Intrusion Response",
        "authors": [
            "Damodar Panigrahi",
            "Raj Patel",
            "Shaswata Mitra",
            "Sudip Mittal",
            "Shahram Rahimi"
        ],
        "comments": "10 pages, 4 figures",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Modern enterprise systems face escalating cyber threats that are increasingly dynamic, distributed, and multi-stage in nature. Traditional intrusion detection and response systems often rely on static rules and manual workflows, which limit their ability to respond with the speed and precision required in high-stakes environments. To address these challenges, we present the Intrusion Response System Digital Assistant (IRSDA), an agent-based framework designed to deliver autonomous and policy-compliant cyber defense. IRSDA combines Self-Adaptive Autonomic Computing Systems (SA-ACS) with the Knowledge guided Monitor, Analyze, Plan, and Execute (MAPE-K) loop to support real-time, partition-aware decision-making across enterprise infrastructure. IRSDA incorporates a knowledge-driven architecture that integrates contextual information with AI-based reasoning to support system-guided intrusion response. The framework leverages retrieval mechanisms and structured representations to inform decision-making while maintaining alignment with operational policies. We assess the system using a representative real-world microservices application, demonstrating its ability to automate containment, enforce compliance, and provide traceable outputs for security analyst interpretation. This work outlines a modular and agent-driven approach to cyber defense that emphasizes explainability, system-state awareness, and operational control in intrusion response.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19647",
        "abs_url": "https://arxiv.org/abs/2511.19647",
        "pdf_url": "https://arxiv.org/pdf/2511.19647",
        "title": "Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation",
        "authors": [
            "Jennifer Grannen",
            "Michelle Pan",
            "Kenneth Llontop",
            "Cherie Ho",
            "Mark Zolotas",
            "Jeannette Bohg",
            "Dorsa Sadigh"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Foundation models (FM) have unlocked powerful zero-shot capabilities in vision and language, yet their reliance on internet pretraining data leaves them brittle in unstructured, real-world settings. The messy, real-world data encountered during deployment (e.g. occluded or multilingual text) remains massively underrepresented in existing corpora. Robots, as embodied agents, are uniquely positioned to close this gap: they can act in physical environments to collect large-scale, real-world data that enriches FM training with precisely the examples current models lack. We introduce the Robot-Powered Data Flywheel, a framework that transforms robots from FM consumers into data generators. By deploying robots equipped with FMs in the wild, we enable a virtuous cycle: robots perform useful tasks while collecting real-world data that improves both domain-specific adaptation and domain-adjacent generalization. We instantiate this framework with Scanford, a mobile manipulator deployed in the East Asia Library for 2 weeks. Scanford autonomously scans shelves, identifies books using a vision-language model (VLM), and leverages the library catalog to label images without human annotation. This deployment both aids librarians and produces a dataset to finetune the underlying VLM, improving performance on the domain-specific in-the-wild library setting and on domain-adjacent multilingual OCR benchmarks. Using data collected from 2103 shelves, Scanford improves VLM performance on book identification from 32.0% to 71.8% and boosts domain-adjacent multilingual OCR from 24.8% to 46.6% (English) and 30.8% to 38.0% (Chinese), while saving an ~18.7 hrs of human time. These results highlight how robot-powered data flywheels can both reduce human effort in real deployments and unlock new pathways for continually adapting FMs to the messiness of reality. More details are at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19648",
        "abs_url": "https://arxiv.org/abs/2511.19648",
        "pdf_url": "https://arxiv.org/pdf/2511.19648",
        "title": "Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search",
        "authors": [
            "Manil Shrestha",
            "Edward Kim"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19649",
        "abs_url": "https://arxiv.org/abs/2511.19649",
        "pdf_url": "https://arxiv.org/pdf/2511.19649",
        "title": "Synthetic Data: AI's New Weapon Against Android Malware",
        "authors": [
            "Angelo Gaspar Diniz Nogueira",
            "Kayua Oleques Paim",
            "Hendrio Bragança",
            "Rodrigo Brandão Mansilha",
            "Diego Kreutz"
        ],
        "comments": "23 pages, 18 figures, 8 tables. Accepted for publication at the JBCS",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The ever-increasing number of Android devices and the accelerated evolution of malware, reaching over 35 million samples by 2024, highlight the critical importance of effective detection methods. Attackers are now using Artificial Intelligence to create sophisticated malware variations that can easily evade traditional detection techniques. Although machine learning has shown promise in malware classification, its success relies heavily on the availability of up-to-date, high-quality datasets. The scarcity and high cost of obtaining and labeling real malware samples presents significant challenges in developing robust detection models. In this paper, we propose MalSynGen, a Malware Synthetic Data Generation methodology that uses a conditional Generative Adversarial Network (cGAN) to generate synthetic tabular data. This data preserves the statistical properties of real-world data and improves the performance of Android malware classifiers. We evaluated the effectiveness of this approach using various datasets and metrics that assess the fidelity of the generated data, its utility in classification, and the computational efficiency of the process. Our experiments demonstrate that MalSynGen can generalize across different datasets, providing a viable solution to address the issues of obsolescence and low quality data in malware detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19654",
        "abs_url": "https://arxiv.org/abs/2511.19654",
        "pdf_url": "https://arxiv.org/pdf/2511.19654",
        "title": "Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning",
        "authors": [
            "Stephen C. Gravereaux",
            "Sheikh Rabiul Islam"
        ],
        "comments": "Accepted in IEEE Big Data 2025",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19693",
        "abs_url": "https://arxiv.org/abs/2511.19693",
        "pdf_url": "https://arxiv.org/pdf/2511.19693",
        "title": "TREASURE: A Transformer-Based Foundation Model for High-Volume Transaction Understanding",
        "authors": [
            "Chin-Chia Michael Yeh",
            "Uday Singh Saini",
            "Xin Dai",
            "Xiran Fan",
            "Shubham Jain",
            "Yujie Fan",
            "Jiarui Sun",
            "Junpeng Wang",
            "Menghai Pan",
            "Yingtong Dou",
            "Yuzhong Chen",
            "Vineeth Rakesh",
            "Liang Wang",
            "Yan Zheng",
            "Mahashweta Das"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Payment networks form the backbone of modern commerce, generating high volumes of transaction records from daily activities. Properly modeling this data can enable applications such as abnormal behavior detection and consumer-level insights for hyper-personalized experiences, ultimately improving people's lives. In this paper, we present TREASURE, TRansformer Engine As Scalable Universal transaction Representation Encoder, a multipurpose transformer-based foundation model specifically designed for transaction data. The model simultaneously captures both consumer behavior and payment network signals (such as response codes and system flags), providing comprehensive information necessary for applications like accurate recommendation systems and abnormal behavior detection. Verified with industry-grade datasets, TREASURE features three key capabilities: 1) an input module with dedicated sub-modules for static and dynamic attributes, enabling more efficient training and inference; 2) an efficient and effective training paradigm for predicting high-cardinality categorical attributes; and 3) demonstrated effectiveness as both a standalone model that increases abnormal behavior detection performance by 111% over production systems and an embedding provider that enhances recommendation models by 104%. We present key insights from extensive ablation studies, benchmarks against production models, and case studies, highlighting valuable knowledge gained from developing TREASURE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19694",
        "abs_url": "https://arxiv.org/abs/2511.19694",
        "pdf_url": "https://arxiv.org/pdf/2511.19694",
        "title": "TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification",
        "authors": [
            "Chin-Chia Michael Yeh",
            "Uday Singh Saini",
            "Junpeng Wang",
            "Xin Dai",
            "Xiran Fan",
            "Jiarui Sun",
            "Yujie Fan",
            "Yan Zheng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19699",
        "abs_url": "https://arxiv.org/abs/2511.19699",
        "pdf_url": "https://arxiv.org/pdf/2511.19699",
        "title": "A Layered Protocol Architecture for the Internet of Agents",
        "authors": [
            "Charles Fleming",
            "Vijoy Pandey",
            "Ramana Kompella",
            "Luca Muscariello"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance improvements and the ability to learn domain-specific languages (DSLs), including APIs and tool interfaces. This capability has enabled the creation of AI agents that can perform preliminary computations and act through tool calling, now being standardized via protocols like MCP. However, LLMs face fundamental limitations: their context windows cannot grow indefinitely, constraining their memory and computational capacity. Agent collaboration emerges as essential for solving increasingly complex problems, mirroring how computational systems rely on different types of memory to scale. The \"Internet of Agents\" (IoA) represents the communication stack that enables agents to scale by distributing computation across collaborating entities. Current network architectural stacks (OSI and TCP/IP) were designed for data delivery between hosts and processes, not for agent collaboration with semantic understanding. To address this gap, we propose two new layers: an \\textbf{Agent Communication Layer (L8)} and an \\textbf{Agent Semantic Negotiation Layer (L9)}. L8 formalizes the \\textit{structure} of communication, standardizing message envelopes, speech-act performatives (e.g., REQUEST, INFORM), and interaction patterns (e.g., request-reply, publish-subscribe), building on protocols like MCP. L9, which does not exist today, formalizes the \\textit{meaning} of communication, enabling agents to discover, negotiate, and lock a \"Shared Context\" -- a formal schema defining the concepts, tasks, and parameters relevant to their interaction. Together, these layers provide the foundation for scalable, distributed agent collaboration, enabling the next generation of multi-agentic systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19703",
        "abs_url": "https://arxiv.org/abs/2511.19703",
        "pdf_url": "https://arxiv.org/pdf/2511.19703",
        "title": "The Alexander-Hirschowitz theorem for neurovarieties",
        "authors": [
            "A. Massarenti",
            "M. Mella"
        ],
        "comments": "21 pages",
        "subjects": "Algebraic Geometry (math.AG); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Commutative Algebra (math.AC)",
        "abstract": "We study neurovarieties for polynomial neural networks and fully characterize when they attain the expected dimension in the single-output case. As consequences, we establish non-defectiveness and global identifiability for multi-output architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19711",
        "abs_url": "https://arxiv.org/abs/2511.19711",
        "pdf_url": "https://arxiv.org/pdf/2511.19711",
        "title": "CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation",
        "authors": [
            "Jinyu Liu",
            "Gang Tan",
            "Kiwan Maeng"
        ],
        "comments": "28 pages, 17 figures. Submitted to PLDI 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Programming Languages (cs.PL)",
        "abstract": "Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\\times$ end-to-end speedup compared to the popular framework, CrypTen.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19726",
        "abs_url": "https://arxiv.org/abs/2511.19726",
        "pdf_url": "https://arxiv.org/pdf/2511.19726",
        "title": "An Adaptive, Data-Integrated Agent-Based Modeling Framework for Explainable and Contestable Policy Design",
        "authors": [
            "Roberto Garrone"
        ],
        "comments": "27 pages, 2 case studies (emissions and smart grids). Preprint prepared during the author's PhD research at the Open University of Cyprus and the University of Milano-Bicocca. Introduces a unified framework for adaptive multi-agent learning with information-theoretic, causal, and clustering diagnostics",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Multi-agent systems often operate under feedback, adaptation, and non-stationarity, yet many simulation studies retain static decision rules and fixed control parameters. This paper introduces a general adaptive multi-agent learning framework that integrates: (i) four dynamic regimes distinguishing static versus adaptive agents and fixed versus adaptive system parameters; (ii) information-theoretic diagnostics (entropy rate, statistical complexity, and predictive information) to assess predictability and structure; (iii) structural causal models for explicit intervention semantics; (iv) procedures for generating agent-level priors from aggregate or sample data; and (v) unsupervised methods for identifying emergent behavioral regimes. The framework offers a domain-neutral architecture for analyzing how learning agents and adaptive controls jointly shape system trajectories, enabling systematic comparison of stability, performance, and interpretability across non-equilibrium, oscillatory, or drifting dynamics. Mathematical definitions, computational operators, and an experimental design template are provided, yielding a structured methodology for developing explainable and contestable multi-agent decision processes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19727",
        "abs_url": "https://arxiv.org/abs/2511.19727",
        "pdf_url": "https://arxiv.org/pdf/2511.19727",
        "title": "Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts",
        "authors": [
            "Steven Peh"
        ],
        "comments": "44 pages, 1 figure",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19768",
        "abs_url": "https://arxiv.org/abs/2511.19768",
        "pdf_url": "https://arxiv.org/pdf/2511.19768",
        "title": "Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering",
        "authors": [
            "Noah Frahm",
            "Prakrut Patel",
            "Yue Zhang",
            "Shoubin Yu",
            "Mohit Bansal",
            "Roni Sengupta"
        ],
        "comments": "webpage: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19808",
        "abs_url": "https://arxiv.org/abs/2511.19808",
        "pdf_url": "https://arxiv.org/pdf/2511.19808",
        "title": "Learning to Clean: Reinforcement Learning for Noisy Label Correction",
        "authors": [
            "Marzi Heidari",
            "Hanping Zhang",
            "Yuhong Guo"
        ],
        "comments": "NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19818",
        "abs_url": "https://arxiv.org/abs/2511.19818",
        "pdf_url": "https://arxiv.org/pdf/2511.19818",
        "title": "Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana",
        "authors": [
            "Koena Ronny Mabokela",
            "Tim Schlippe",
            "Mpho Raborife",
            "Turgay Celik"
        ],
        "comments": "Published in the The Fourth Workshop on Processing Emotions, Decisions and Opinions (EDO 2023) at 10th Language & Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics (LTC 2023), Poznań, Poland, 21-23 April 2023. ISBN: 978-83-232-4176-8",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19822",
        "abs_url": "https://arxiv.org/abs/2511.19822",
        "pdf_url": "https://arxiv.org/pdf/2511.19822",
        "title": "Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models",
        "authors": [
            "Wentao Hu",
            "Mingkuan Zhao",
            "Shuangyong Song",
            "Xiaoyan Zhu",
            "Xin Lai",
            "Jiayin Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select\" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream this http URL experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\\% gain on general tasks and 8.92\\% on specialized tasks like math reasoning and code generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19830",
        "abs_url": "https://arxiv.org/abs/2511.19830",
        "pdf_url": "https://arxiv.org/pdf/2511.19830",
        "title": "Beyond Relational: Semantic-Aware Multi-Modal Analytics with LLM-Native Query Optimization",
        "authors": [
            "Junhao Zhu",
            "Lu Chen",
            "Xiangyu Ke",
            "Ziquan Fang",
            "Tianyi Li",
            "Yunjun Gao",
            "Christian S. Jensen"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Artificial Intelligence (cs.AI)",
        "abstract": "Multi-modal analytical processing has the potential to transform applications in e-commerce, healthcare, entertainment, and beyond. However, real-world adoption remains elusive due to the limited ability of traditional relational query operators to capture query semantics. The emergence of foundation models, particularly the large language models (LLMs), opens up new opportunities to develop flexible, semantic-aware data analytics systems that transcend the relational paradigm. We present Nirvana, a multi-modal data analytics framework that incorporates programmable semantic operators while leveraging both logical and physical query optimization strategies, tailored for LLM-driven semantic query processing. Nirvana addresses two key challenges. First, it features an agentic logical optimizer that uses natural language-specified transformation rules and random-walk-based search to explore vast spaces of semantically equivalent query plans -- far beyond the capabilities of conventional optimizers. Second, it introduces a cost-aware physical optimizer that selects the most effective LLM backend for each operator using a novel improvement-score metric. To further enhance efficiency, Nirvana incorporates computation reuse and evaluation pushdown techniques guided by model capability hypotheses. Experimental evaluations on three real-world benchmarks demonstrate that Nirvana is able to reduce end-to-end runtime by 10%--85% and reduces system processing costs by 76% on average, outperforming state-of-the-art systems at both efficiency and scalability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19837",
        "abs_url": "https://arxiv.org/abs/2511.19837",
        "pdf_url": "https://arxiv.org/pdf/2511.19837",
        "title": "GED-Consistent Disentanglement of Aligned and Unaligned Substructures for Graph Similarity Learning",
        "authors": [
            "Zhentao Zhan",
            "Xiaoliang Xu",
            "Jingjing Wang",
            "Junmei Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "Graph Similarity Computation (GSC) is a fundamental graph related task where Graph Edit Distance (GED) serves as a prevalent metric. GED is determined by an optimal alignment between a pair of graphs that partitions each into aligned (zero-cost) and unaligned (cost-incurring) substructures. Due to NP-hard nature of exact GED computation, GED approximations based on Graph Neural Network(GNN) have emerged. Existing GNN-based GED approaches typically learn node embeddings for each graph and then aggregate pairwise node similarities to estimate the final similarity. Despite their effectiveness, we identify a mismatch between this prevalent node-centric matching paradigm and the core principles of GED. This discrepancy leads to two critical limitations: (1) a failure to capture the global structural correspondence for optimal alignment, and (2) a misattribution of edit costs driven by spurious node level signals. To address these limitations, we propose GCGSim, a GED-consistent graph similarity learning framework centering on graph-level matching and substructure-level edit costs. Specifically, we make three core technical contributions. Extensive experiments on four benchmark datasets show that GCGSim achieves state-of-the-art performance. Our comprehensive analyses further validate that the framework effectively learns disentangled and semantically meaningful substructure representations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19841",
        "abs_url": "https://arxiv.org/abs/2511.19841",
        "pdf_url": "https://arxiv.org/pdf/2511.19841",
        "title": "Cisco Time Series Model Technical Report",
        "authors": [
            "Liang Gou",
            "Archit Khare",
            "Praneet Pabolu",
            "Prachi Patel",
            "Joseph Ross",
            "Hercy Shen",
            "Yuhan",
            "Song",
            "Jingze Sun",
            "Kristal Curtis",
            "Vedant Dharnidharka",
            "Abhinav Mathur",
            "Hao Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19858",
        "abs_url": "https://arxiv.org/abs/2511.19858",
        "pdf_url": "https://arxiv.org/pdf/2511.19858",
        "title": "A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction",
        "authors": [
            "Farzad Ahmed",
            "Joniel Augustine Jerome",
            "Meliha Yetisgen",
            "Özlem Uzuner"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction. Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning. Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections. Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19874",
        "abs_url": "https://arxiv.org/abs/2511.19874",
        "pdf_url": "https://arxiv.org/pdf/2511.19874",
        "title": "Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains",
        "authors": [
            "Arun Chowdary Sanna"
        ],
        "comments": "10 pages, 2 figures, 8 tables. Evaluation across 6 production LLMs with 1,198 traces",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "As AI agents become integral to enterprise workflows, their reliance on shared tool libraries and pre-trained components creates significant supply chain vulnerabilities. While previous work has demonstrated behavioral backdoor detection within individual LLM architectures, the critical question of cross-LLM generalization remains unexplored, a gap with serious implications for organizations deploying multiple AI systems. We present the first systematic study of cross-LLM behavioral backdoor detection, evaluating generalization across six production LLMs (GPT-5.1, Claude Sonnet 4.5, Grok 4.1, Llama 4 Maverick, GPT-OSS 120B, and DeepSeek Chat V3.1). Through 1,198 execution traces and 36 cross-model experiments, we quantify a critical finding: single-model detectors achieve 92.7% accuracy within their training distribution but only 49.2% across different LLMs, a 43.4 percentage point generalization gap equivalent to random guessing. Our analysis reveals that this gap stems from model-specific behavioral signatures, particularly in temporal features (coefficient of variation > 0.8), while structural features remain stable across architectures. We show that model-aware detection incorporating model identity as an additional feature achieves 90.6% accuracy universally across all evaluated models. We release our multi-LLM trace dataset and detection framework to enable reproducible research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19875",
        "abs_url": "https://arxiv.org/abs/2511.19875",
        "pdf_url": "https://arxiv.org/pdf/2511.19875",
        "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection",
        "authors": [
            "Qingyu Zhang",
            "Puzhuo Liu",
            "Peng Di",
            "Chenxiong Qian"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19887",
        "abs_url": "https://arxiv.org/abs/2511.19887",
        "pdf_url": "https://arxiv.org/pdf/2511.19887",
        "title": "Distilling Cross-Modal Knowledge via Feature Disentanglement",
        "authors": [
            "Junhong Liu",
            "Yuan Zhang",
            "Tao Huang",
            "Wenchao Xu",
            "Renyu Yang"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Knowledge distillation (KD) has proven highly effective for compressing large models and enhancing the performance of smaller ones. However, its effectiveness diminishes in cross-modal scenarios, such as vision-to-language distillation, where inconsistencies in representation across modalities lead to difficult knowledge transfer. To address this challenge, we propose frequency-decoupled cross-modal knowledge distillation, a method designed to decouple and balance knowledge transfer across modalities by leveraging frequency-domain features. We observed that low-frequency features exhibit high consistency across different modalities, whereas high-frequency features demonstrate extremely low cross-modal similarity. Accordingly, we apply distinct losses to these features: enforcing strong alignment in the low-frequency domain and introducing relaxed alignment for high-frequency features. We also propose a scale consistency loss to address distributional shifts between modalities, and employ a shared classifier to unify feature spaces. Extensive experiments across multiple benchmark datasets show our method substantially outperforms traditional KD and state-of-the-art cross-modal KD approaches. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19902",
        "abs_url": "https://arxiv.org/abs/2511.19902",
        "pdf_url": "https://arxiv.org/pdf/2511.19902",
        "title": "Zero-Knowledge Proof Based Verifiable Inference of Models",
        "authors": [
            "Yunxiao Wang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in artificial intelligence (AI), particularly deep learning, have led to widespread adoption across various applications. Yet, a fundamental challenge persists: how can we verify the correctness of AI model inference when model owners cannot (or will not) reveal their parameters? These parameters represent enormous training costs and valuable intellectual property, making transparent verification difficult. In this paper, we introduce a zero-knowledge framework capable of verifying deep learning inference without exposing model internal parameters. Built on recursively composed zero-knowledge proofs and requiring no trusted setup, our framework supports both linear and nonlinear neural network layers, including matrix multiplication, normalization, softmax, and SiLU. Leveraging the Fiat-Shamir heuristic, we obtain a succinct non-interactive argument of knowledge (zkSNARK) with constant-size proofs. To demonstrate the practicality of our approach, we translate the DeepSeek model into a fully SNARK-verifiable version named ZK-DeepSeek and show experimentally that our framework delivers both efficiency and flexibility in real-world AI verification workloads.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19931",
        "abs_url": "https://arxiv.org/abs/2511.19931",
        "pdf_url": "https://arxiv.org/pdf/2511.19931",
        "title": "LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training",
        "authors": [
            "Ziwei Liu",
            "Qidong Liu",
            "Wanyu Wang",
            "Yejing Wang",
            "Tong Xu",
            "Wei Huang",
            "Chong Chen",
            "Peng Chuan",
            "Xiangyu Zhao"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
        "abstract": "Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19941",
        "abs_url": "https://arxiv.org/abs/2511.19941",
        "pdf_url": "https://arxiv.org/pdf/2511.19941",
        "title": "Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning",
        "authors": [
            "Shenjun Zhong",
            "Zhifeng Chen",
            "Zhaolin Chen"
        ],
        "comments": "4 pages, 5 figures, submitted to conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19943",
        "abs_url": "https://arxiv.org/abs/2511.19943",
        "pdf_url": "https://arxiv.org/pdf/2511.19943",
        "title": "AI/ML based Joint Source and Channel Coding for HARQ-ACK Payload",
        "authors": [
            "Akash Doshi",
            "Pinar Sen",
            "Kirill Ivanov",
            "Wei Yang",
            "June Namgoong",
            "Runxin Wang",
            "Rachel Wang",
            "Taesang Yoo",
            "Jing Jiang",
            "Tingfang Ji"
        ],
        "comments": "39 pages, 15 figures. Under consideration for publication in Journal of Sel. Areas in Information Theory. This paper was presented in part at the International Symposium on Topics in Coding, August 2025 in the Session for Coding and AI",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Channel coding from 2G to 5G has assumed the inputs bits at the physical layer to be uniformly distributed. However, hybrid automatic repeat request acknowledgement (HARQ-ACK) bits transmitted in the uplink are inherently non-uniformly distributed. For such sources, significant performance gains could be obtained by employing joint source channel coding, aided by deep learning-based techniques. In this paper, we learn a transformer-based encoder using a novel \"free-lunch\" training algorithm and propose per-codeword power shaping to exploit the source prior at the encoder whilst being robust to small changes in the HARQ-ACK distribution. Furthermore, any HARQ-ACK decoder has to achieve a low negative acknowledgement (NACK) error rate to avoid radio link failures resulting from multiple NACK errors. We develop an extension of the Neyman-Pearson test to a coded bit system with multiple information bits to achieve Unequal Error Protection of NACK over ACK bits at the decoder. Finally, we apply the proposed encoder and decoder designs to a 5G New Radio (NR) compliant uplink setup under a fading channel, describing the optimal receiver design and a low complexity coherent approximation to it. Our results demonstrate 3-6 dB reduction in the average transmit power required to achieve the target error rates compared to the NR baseline, while also achieving a 2-3 dB reduction in the maximum transmit power, thus providing for significant coverage gains and power savings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19982",
        "abs_url": "https://arxiv.org/abs/2511.19982",
        "pdf_url": "https://arxiv.org/pdf/2511.19982",
        "title": "EmoFeedback2: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback",
        "authors": [
            "Jingyang Jia",
            "Kai Shu",
            "Gang Yang",
            "Long Xing",
            "Xun Chen",
            "Aiping Liu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Continuous emotional image generation (C-EICG) is emerging rapidly due to its ability to produce images aligned with both user descriptions and continuous emotional values. However, existing approaches lack emotional feedback from generated images, limiting the control of emotional continuity. Additionally, their simple alignment between emotions and naively generated texts fails to adaptively adjust emotional prompts according to image content, leading to insufficient emotional fidelity. To address these concerns, we propose a novel generation-understanding-feedback reinforcement paradigm (EmoFeedback2) for C-EICG, which exploits the reasoning capability of the fine-tuned large vision-language model (LVLM) to provide reward and textual feedback for generating high-quality images with continuous emotions. Specifically, we introduce an emotion-aware reward feedback strategy, where the LVLM evaluates the emotional values of generated images and computes the reward against target emotions, guiding the reinforcement fine-tuning of the generative model and enhancing the emotional continuity of images. Furthermore, we design a self-promotion textual feedback framework, in which the LVLM iteratively analyzes the emotional content of generated images and adaptively produces refinement suggestions for the next-round prompt, improving the emotional fidelity with fine-grained content. Extensive experimental results demonstrate that our approach effectively generates high-quality images with the desired emotions, outperforming existing state-of-the-art methods in our custom dataset. The code and dataset will be released soon.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19997",
        "abs_url": "https://arxiv.org/abs/2511.19997",
        "pdf_url": "https://arxiv.org/pdf/2511.19997",
        "title": "Directional Optimization Asymmetry in Transformers: A Synthetic Stress Test",
        "authors": [
            "Mihir Sahasrabudhe"
        ],
        "comments": "19 pages, 4 figures. Code available at this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Transformers are theoretically reversal-invariant: their function class does not prefer left-to-right over right-to-left mappings. Yet empirical studies on natural language repeatedly report a \"reversal curse,\" and recent work on temporal asymmetry in LLMs suggests that real-world corpora carry their own arrow of time. This leaves an unresolved question: do directional failures stem from linguistic statistics, or from the architecture itself? We cut through this ambiguity with a fully synthetic, entropy-controlled benchmark designed as a clean-room stress test for directional learning. Using random string mappings with tunable branching factor K, we construct forward tasks with zero conditional entropy and inverse tasks with analytically determined entropy floors. Excess loss above these floors reveals that even scratch-trained GPT-2 models exhibit a strong, reproducible directional optimization gap (e.g., 1.16 nats at K=5), far larger than that of an MLP trained on the same data. Pre-trained initializations shift optimization behavior but do not eliminate this gap, while LoRA encounters a sharp capacity wall on high-entropy inverse mappings. Together, these results isolate a minimal, semantics-free signature of directional friction intrinsic to causal Transformer training-one that persists even when linguistic priors, token frequencies, and corpus-level temporal asymmetries are removed. Our benchmark provides a controlled instrument for dissecting directional biases in modern sequence models and motivates deeper mechanistic study of why inversion remains fundamentally harder for Transformers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.19999",
        "abs_url": "https://arxiv.org/abs/2511.19999",
        "pdf_url": "https://arxiv.org/pdf/2511.19999",
        "title": "Popularity Bias Alignment Estimates",
        "authors": [
            "Anton Lyubinin"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "We are extending Popularity Bias Memorization theorem from arXiv:archive/2404.12008 in several directions. We extend it to arbitrary degree distributions and also prove both upper and lower estimates for the alignment with top-k singular hyperspace.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20004",
        "abs_url": "https://arxiv.org/abs/2511.20004",
        "pdf_url": "https://arxiv.org/pdf/2511.20004",
        "title": "Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting",
        "authors": [
            "Peining Zhang",
            "Hongchen Qin",
            "Haochen Zhang",
            "Ziqi Guo",
            "Guiling Wang",
            "Jinbo Bi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20006",
        "abs_url": "https://arxiv.org/abs/2511.20006",
        "pdf_url": "https://arxiv.org/pdf/2511.20006",
        "title": "BERT-APC: A Reference-free Framework for Automatic Pitch Correction via Musical Context Inference",
        "authors": [
            "Sungjae Kim",
            "Kihyun Na",
            "Jinyoung Choi",
            "Injung Kim"
        ],
        "comments": "12 pages, 6 figures, 5 tables",
        "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Sound (cs.SD)",
        "abstract": "Automatic Pitch Correction (APC) enhances vocal recordings by aligning pitch deviations with the intended musical notes. However, existing APC systems either rely on reference pitches, which limits their practical applicability, or employ simple pitch estimation algorithms that often fail to preserve expressiveness and naturalness. We propose BERT-APC, a novel reference-free APC framework that corrects pitch errors while maintaining the natural expressiveness of vocal performances. In BERT-APC, a novel stationary pitch predictor first estimates the perceived pitch of each note from the detuned singing voice. A context-aware note pitch predictor estimates the intended pitch sequence by leveraging a music language model repurposed to incorporate musical context. Finally, a note-level correction algorithm fixes pitch errors while preserving intentional pitch deviations for emotional expression. In addition, we introduce a learnable data augmentation strategy that improves the robustness of the music language model by simulating realistic detuning patterns. Compared to two recent singing voice transcription models, BERT-APC demonstrated superior performance in note pitch prediction, outperforming the second-best model, ROSVOT, by 10.49%p on highly detuned samples in terms of the raw pitch accuracy. In the MOS test, BERT-APC achieved the highest score of $4.32 \\pm 0.15$, which is significantly higher than those of the widely-used commercial APC tools, AutoTune ($3.22 \\pm 0.18$) and Melodyne ($3.08 \\pm 0.18$), while maintaining a comparable ability to preserve expressive nuances. To the best of our knowledge, this is the first APC model that leverages a music language model to achieve reference-free pitch correction with symbolic musical context. The corrected audio samples of BERT-APC are available online.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20011",
        "abs_url": "https://arxiv.org/abs/2511.20011",
        "pdf_url": "https://arxiv.org/pdf/2511.20011",
        "title": "Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments",
        "authors": [
            "Yuanzhe Li",
            "Hang Zhong",
            "Steffen Müller"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Pedestrian crossing intention prediction is essential for autonomous vehicles to improve pedestrian safety and reduce traffic accidents. However, accurate pedestrian intention prediction in urban environments remains challenging due to the multitude of factors affecting pedestrian behavior. In this paper, we propose a multi-context fusion Transformer (MFT) that leverages diverse numerical contextual attributes across four key dimensions, encompassing pedestrian behavior context, environmental context, pedestrian localization context and vehicle motion context, to enable accurate pedestrian intention prediction. MFT employs a progressive fusion strategy, where mutual intra-context attention enables reciprocal interactions within each context, thereby facilitating feature sequence fusion and yielding a context token as a context-specific representation. This is followed by mutual cross-context attention, which integrates features across contexts with a global CLS token serving as a compact multi-context representation. Finally, guided intra-context attention refines context tokens within each context through directed interactions, while guided cross-context attention strengthens the global CLS token to promote multi-context fusion via guided information propagation, yielding deeper and more efficient integration. Experimental results validate the superiority of MFT over state-of-the-art methods, achieving accuracy rates of 73%, 93%, and 90% on the JAADbeh, JAADall, and PIE datasets, respectively. Extensive ablation studies are further conducted to investigate the effectiveness of the network architecture and contribution of different input context. Our code is open-source: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20018",
        "abs_url": "https://arxiv.org/abs/2511.20018",
        "pdf_url": "https://arxiv.org/pdf/2511.20018",
        "title": "Energy Costs and Neural Complexity Evolution in Changing Environments",
        "authors": [
            "Sian Heesom-Green",
            "Jonathan Shock",
            "Geoff Nitschke"
        ],
        "comments": "Presented at ALIFE 2025, proceedings forthcoming (MIT Press)",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI)",
        "abstract": "The Cognitive Buffer Hypothesis (CBH) posits that larger brains evolved to enhance survival in changing conditions. However, larger brains also carry higher energy demands, imposing additional metabolic burdens. Alongside brain size, brain organization plays a key role in cognitive ability and, with suitable architectures, may help mitigate energy challenges. This study evolves Artificial Neural Networks (ANNs) used by Reinforcement Learning (RL) agents to investigate how environmental variability and energy costs influence the evolution of neural complexity, defined in terms of ANN size and structure. Results indicate that under energy constraints, increasing seasonality led to smaller ANNs. This challenges CBH and supports the Expensive Brain Hypothesis (EBH), as highly seasonal environments reduced net energy intake and thereby constrained brain size. ANN structural complexity primarily emerged as a byproduct of size, where energy costs promoted the evolution of more efficient networks. These results highlight the role of energy constraints in shaping neural complexity, offering in silico support for biological theory and energy-efficient robotic design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20041",
        "abs_url": "https://arxiv.org/abs/2511.20041",
        "pdf_url": "https://arxiv.org/pdf/2511.20041",
        "title": "MFM-point: Multi-scale Flow Matching for Point Cloud Generation",
        "authors": [
            "Petr Molodyk",
            "Jaemoo Choi",
            "David W. Romero",
            "Ming-Yu Liu",
            "Yongxin Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In recent years, point cloud generation has gained significant attention in 3D generative modeling. Among existing approaches, point-based methods directly generate point clouds without relying on other representations such as latent features, meshes, or voxels. These methods offer low training cost and algorithmic simplicity, but often underperform compared to representation-based approaches. In this paper, we propose MFM-Point, a multi-scale Flow Matching framework for point cloud generation that substantially improves the scalability and performance of point-based methods while preserving their simplicity and efficiency. Our multi-scale generation algorithm adopts a coarse-to-fine generation paradigm, enhancing generation quality and scalability without incurring additional training or inference overhead. A key challenge in developing such a multi-scale framework lies in preserving the geometric structure of unordered point clouds while ensuring smooth and consistent distributional transitions across resolutions. To address this, we introduce a structured downsampling and upsampling strategy that preserves geometry and maintains alignment between coarse and fine resolutions. Our experimental results demonstrate that MFM-Point achieves best-in-class performance among point-based methods and challenges the best representation-based methods. In particular, MFM-point demonstrates strong results in multi-category and high-resolution generation tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20090",
        "abs_url": "https://arxiv.org/abs/2511.20090",
        "pdf_url": "https://arxiv.org/pdf/2511.20090",
        "title": "R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation",
        "authors": [
            "Zizhang Luo",
            "Fan Cui",
            "Kexing Zhou",
            "Runlin Guo",
            "Mile Xia",
            "Hongyuan Hou",
            "Yun Lian"
        ],
        "comments": "",
        "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI)",
        "abstract": "Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20094",
        "abs_url": "https://arxiv.org/abs/2511.20094",
        "pdf_url": "https://arxiv.org/pdf/2511.20094",
        "title": "The Making of Digital Ghosts: Designing Ethical AI Afterlives",
        "authors": [
            "Giovanni Spitale",
            "Federico Germani"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Advances in artificial intelligence now make it possible to simulate the dead through chatbots, voice clones, and video avatars trained on a person's digital traces. These \"digital ghosts\" are moving from fiction to commercial reality, reshaping how people mourn and remember. This paper offers a conceptual and ethical analysis of AI-mediated digital afterlives. We define what counts as a digital ghost, trace their rise across personal, commercial, and institutional contexts, and identify core ethical tensions around grief and well-being, truthfulness and deception, consent and posthumous privacy, dignity and misrepresentation, and the commercialization of mourning. To analyze these challenges, we propose a nine-dimensional taxonomy of digital afterlife technologies and, building on it, outline the features of an ethically acceptable digital ghost: premortem intent, mutual consent, transparent and limited data use, clear disclosure, restricted purposes and access, family or estate stewardship, and minimal behavioral agency. We argue for targeted regulation and professional guidelines to ensure that digital ghosts can aid remembrance without slipping into forms of deception.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20104",
        "abs_url": "https://arxiv.org/abs/2511.20104",
        "pdf_url": "https://arxiv.org/pdf/2511.20104",
        "title": "The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs",
        "authors": [
            "Craig Dickson"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed \"emergent misalignment\" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales. We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%. We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20120",
        "abs_url": "https://arxiv.org/abs/2511.20120",
        "pdf_url": "https://arxiv.org/pdf/2511.20120",
        "title": "\"When Data is Scarce, Prompt Smarter\"... Approaches to Grammatical Error Correction in Low-Resource Settings",
        "authors": [
            "Somsubhra De",
            "Harsh Kumar",
            "Arun Prakash A"
        ],
        "comments": "10 pages, 5 figures, 5 tables; Accept-demonstration at BHASHA Workshop, IJCNLP-AACL 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20141",
        "abs_url": "https://arxiv.org/abs/2511.20141",
        "pdf_url": "https://arxiv.org/pdf/2511.20141",
        "title": "IDAP++: Advancing Divergence-Based Pruning via Filter-Level and Layer-Level Optimization",
        "authors": [
            "Aleksei Samarin",
            "Artem Nazarenko",
            "Egor Kotenko",
            "Valentin Malykh",
            "Alexander Savelev",
            "Aleksei Toropov"
        ],
        "comments": "65 pages, 4 figures, 38 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper presents a novel approach to neural network compression that addresses redundancy at both the filter and architectural levels through a unified framework grounded in information flow analysis. Building on the concept of tensor flow divergence, which quantifies how information is transformed across network layers, we develop a two-stage optimization process. The first stage employs iterative divergence-aware pruning to identify and remove redundant filters while preserving critical information pathways. The second stage extends this principle to higher-level architecture optimization by analyzing layer-wise contributions to information propagation and selectively eliminating entire layers that demonstrate minimal impact on network performance. The proposed method naturally adapts to diverse architectures, including convolutional networks, transformers, and hybrid designs, providing a consistent metric for comparing the structural importance across different layer types. Experimental validation across multiple modern architectures and datasets reveals that this combined approach achieves substantial model compression while maintaining competitive accuracy. The presented approach achieves parameter reduction results that are globally comparable to those of state-of-the-art solutions and outperforms them across a wide range of modern neural network architectures, from convolutional models to transformers. The results demonstrate how flow divergence serves as an effective guiding principle for both filter-level and layer-level optimization, offering practical benefits for deployment in resource-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20143",
        "abs_url": "https://arxiv.org/abs/2511.20143",
        "pdf_url": "https://arxiv.org/pdf/2511.20143",
        "title": "SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models",
        "authors": [
            "Wen-Fang Su",
            "Hsiao-Wei Chou",
            "Wen-Yang Lin"
        ],
        "comments": "9 pages, 5 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20168",
        "abs_url": "https://arxiv.org/abs/2511.20168",
        "pdf_url": "https://arxiv.org/pdf/2511.20168",
        "title": "On the Limits of Momentum in Decentralized and Federated Optimization",
        "authors": [
            "Riccardo Zaccone",
            "Sai Praneeth Karimireddy",
            "Carlo Masone"
        ],
        "comments": "Accepted at the 17th Workshop on Optimization for Machine Learning (OPT@NeurIPS2025)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $\\Theta\\left(1/t\\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20172",
        "abs_url": "https://arxiv.org/abs/2511.20172",
        "pdf_url": "https://arxiv.org/pdf/2511.20172",
        "title": "Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management",
        "authors": [
            "Xinjun Yang",
            "Qingda Hu",
            "Junru Li",
            "Feifei Li",
            "Yuqi Zhou",
            "Yicong Zhu",
            "Qiuru Lin",
            "Jian Dai",
            "Yang Kong",
            "Jiayu Zhang",
            "Guoqiang Xu",
            "Qiang Liu"
        ],
        "comments": "13 pages, accepted by SIGMOD'26",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20179",
        "abs_url": "https://arxiv.org/abs/2511.20179",
        "pdf_url": "https://arxiv.org/pdf/2511.20179",
        "title": "Human-computer interactions predict mental health",
        "authors": [
            "Veith Weilnhammer",
            "Jefferson Ortega",
            "David Whitney"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Scalable assessments of mental illness, the leading driver of disability worldwide, remain a critical roadblock toward accessible and equitable care. Here, we show that human-computer interactions encode multiple dimensions of self-reported mental health and their changes over time. We introduce MAILA, a MAchine-learning framework for Inferring Latent mental states from digital Activity. We trained MAILA to predict 1.3 million mental-health self-reports from 20,000 cursor and touchscreen recordings recorded in 9,000 online participants. The dataset includes 2,000 individuals assessed longitudinally, 1,500 diagnosed with depression, and 500 with obsessive-compulsive disorder. MAILA tracks dynamic mental states along three orthogonal dimensions, generalizes across contexts, and achieves near-ceiling accuracy when predicting group-level mental health. The model translates from general to clinical populations, identifies individuals living with mental illness, and captures signatures of psychological function that are not conveyed by language. Our results demonstrate how everyday human-computer interactions can power passive, reliable, dynamic, and maximally scalable mental health assessments. The ability to decode mental states at zero marginal cost sets new benchmarks for precision medicine and public health, while raising important questions about privacy, agency, and autonomy online.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20224",
        "abs_url": "https://arxiv.org/abs/2511.20224",
        "pdf_url": "https://arxiv.org/pdf/2511.20224",
        "title": "DUO-TOK: Dual-Track Semantic Music Tokenizer for Vocal-Accompaniment Generation",
        "authors": [
            "Rui Lin",
            "Zhiyue Wu",
            "Jiahe Le",
            "Kangdi Wang",
            "Weixiong Chen",
            "Junyu Dai",
            "Tao Jiang"
        ],
        "comments": "17 pages, 5 figures, 8 tables. Project page: this https URL",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Duo-Tok is a source-aware dual-codebook tokenizer for vocal-accompaniment music that targets the growing tension between reconstruction quality and language-model (LM) learnability in modern lyrics-to-song systems. Existing codecs either prioritize high-fidelity reconstruction with difficult-to-model acoustic tokens or compress aggressively into semantic tokens that are LM-friendly but lossy, and they rarely make the tokenizer itself aware of dual-track structure. Duo-Tok follows a four-stage, SSL-centered pipeline: we first pretrain a BEST-RQ-style encoder on large-scale audio, then stabilize and factorize the representation with Gaussian replacement noise and multi-task supervision, before freezing the encoder to learn SimVQ-based dual codebooks with hard routing for vocals and accompaniment, and finally training latent diffusion decoders on top of the discrete tokens. Duo-Tok at 0.75 kbps shifts the empirical reconstruction-generation Pareto frontier, achieving the best music-tagging AP and the lowest vocabulary-normalized LM perplexity among compared codecs while maintaining reconstruction quality comparable to state-of-the-art music tokenizers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20234",
        "abs_url": "https://arxiv.org/abs/2511.20234",
        "pdf_url": "https://arxiv.org/pdf/2511.20234",
        "title": "Leveraging weights signals - Predicting and improving generalizability in reinforcement learning",
        "authors": [
            "Olivier Moulin",
            "Vincent Francois-lavet",
            "Paul Elbers",
            "Mark Hoogendoorn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20254",
        "abs_url": "https://arxiv.org/abs/2511.20254",
        "pdf_url": "https://arxiv.org/pdf/2511.20254",
        "title": "XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface",
        "authors": [
            "Alexander C. Jenke",
            "Gregor Just",
            "Claas de Boer",
            "Martin Wagner",
            "Sebastian Bodenstedt",
            "Stefanie Speidel"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Purpose: Robot-assisted minimally invasive surgery relies on endoscopic video as the sole intraoperative visual feedback. The DaVinci Xi system overlays a graphical user interface (UI) that indicates the state of each robotic arm, including the activation of the endoscope arm. Detecting this activation provides valuable metadata such as camera movement information, which can support downstream surgical data science tasks including tool tracking, skill assessment, or camera control automation. Methods: We developed a lightweight pipeline based on a ResNet18 convolutional neural network to automatically identify the position of the camera tile and its activation state within the DaVinci Xi UI. The model was fine-tuned on manually annotated data from the SurgToolLoc dataset and evaluated across three public datasets comprising over 70,000 frames. Results: The model achieved F1-scores between 0.993 and 1.000 for the binary detection of active cameras and correctly localized the camera tile in all cases without false multiple-camera detections. Conclusion: The proposed pipeline enables reliable, real-time extraction of camera activation metadata from surgical videos, facilitating automated preprocessing and analysis for diverse downstream applications. All code, trained models, and annotations are publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20257",
        "abs_url": "https://arxiv.org/abs/2511.20257",
        "pdf_url": "https://arxiv.org/pdf/2511.20257",
        "title": "Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling",
        "authors": [
            "Zhiguo Zhang",
            "Xiaoliang Ma",
            "Daniel Schlesinger"
        ],
        "comments": "Accepted to 2025 IEEE International Conference on Big Data",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20273",
        "abs_url": "https://arxiv.org/abs/2511.20273",
        "pdf_url": "https://arxiv.org/pdf/2511.20273",
        "title": "Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits",
        "authors": [
            "Areeb Ahmad",
            "Abhinav Joshi",
            "Ashutosh Modi"
        ],
        "comments": "Accepted at NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20277",
        "abs_url": "https://arxiv.org/abs/2511.20277",
        "pdf_url": "https://arxiv.org/pdf/2511.20277",
        "title": "HVAdam: A Full-Dimension Adaptive Optimizer",
        "authors": [
            "Yiheng Zhang",
            "Shaowu Wu",
            "Yuanzhuo Xu",
            "Jiajun Wu",
            "Shang Xu",
            "Steve Drew",
            "Xiaoguang Niu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20284",
        "abs_url": "https://arxiv.org/abs/2511.20284",
        "pdf_url": "https://arxiv.org/pdf/2511.20284",
        "title": "Can LLMs Make (Personalized) Access Control Decisions?",
        "authors": [
            "Friederike Groschupp",
            "Daniele Lain",
            "Aritra Dhar",
            "Lara Magdalena Lazier",
            "Srdjan Čapkun"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions. Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20293",
        "abs_url": "https://arxiv.org/abs/2511.20293",
        "pdf_url": "https://arxiv.org/pdf/2511.20293",
        "title": "Forgetting by Pruning: Data Deletion in Join Cardinality Estimation",
        "authors": [
            "Chaowei He",
            "Yuanjun Liu",
            "Qingzhi Ma",
            "Shenyuan Ren",
            "Xizhao Luo",
            "Lei Zhao",
            "An Liu"
        ],
        "comments": "AAAI26",
        "subjects": "Databases (cs.DB); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20305",
        "abs_url": "https://arxiv.org/abs/2511.20305",
        "pdf_url": "https://arxiv.org/pdf/2511.20305",
        "title": "RIS-Assisted Downlink Pinching-Antenna Systems: GNN-Enabled Optimization Approaches",
        "authors": [
            "Changpeng He",
            "Yang Lu",
            "Yanqing Xu",
            "Chong-Yung Chi",
            "Bo Ai",
            "Arumugam Nallanathan"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI)",
        "abstract": "This paper investigates a reconfigurable intelligent surface (RIS)-assisted multi-waveguide pinching-antenna (PA) system (PASS) for multi-user downlink information transmission, motivated by the unknown impact of the integration of emerging PASS and RIS on wireless communications. First, we formulate sum rate (SR) and energy efficiency (EE) maximization problems in a unified framework, subject to constraints on the movable region of PAs, total power budget, and tunable phase of RIS elements. Then, by leveraging a graph-structured topology of the RIS-assisted PASS, a novel three-stage graph neural network (GNN) is proposed, which learns PA positions based on user locations, and RIS phase shifts according to composite channel conditions at the first two stages, respectively, and finally determines beamforming vectors. Specifically, the proposed GNN is achieved through unsupervised training, together with three implementation strategies for its integration with convex optimization, thus offering trade-offs between inference time and solution optimality. Extensive numerical results are provided to validate the effectiveness of the proposed GNN, and to support its unique attributes of viable generalization capability, good performance reliability, and real-time applicability. Moreover, the impact of key parameters on RIS-assisted PASS is illustrated and analyzed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20315",
        "abs_url": "https://arxiv.org/abs/2511.20315",
        "pdf_url": "https://arxiv.org/pdf/2511.20315",
        "title": "Geometry of Decision Making in Language Models",
        "authors": [
            "Abhinav Joshi",
            "Divyanshu Bhatt",
            "Ashutosh Modi"
        ],
        "comments": "Accepted at NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \\textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20347",
        "abs_url": "https://arxiv.org/abs/2511.20347",
        "pdf_url": "https://arxiv.org/pdf/2511.20347",
        "title": "Soft Adaptive Policy Optimization",
        "authors": [
            "Chang Gao",
            "Chujie Zheng",
            "Xiong-Hui Chen",
            "Kai Dang",
            "Shixuan Liu",
            "Bowen Yu",
            "An Yang",
            "Shuai Bai",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20399",
        "abs_url": "https://arxiv.org/abs/2511.20399",
        "pdf_url": "https://arxiv.org/pdf/2511.20399",
        "title": "BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali",
        "authors": [
            "Abdullah Al Sefat"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20403",
        "abs_url": "https://arxiv.org/abs/2511.20403",
        "pdf_url": "https://arxiv.org/pdf/2511.20403",
        "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework",
        "authors": [
            "Andrea Lops",
            "Fedelucio Narducci",
            "Azzurra Ragone",
            "Michelantonio Trizio",
            "Claudio Barto"
        ],
        "comments": "Accepted at 40th IEEE/ACM International Conference on Automated Software Engineering",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20406",
        "abs_url": "https://arxiv.org/abs/2511.20406",
        "pdf_url": "https://arxiv.org/pdf/2511.20406",
        "title": "Short-Range Oversquashing",
        "authors": [
            "Yaaqov Mishayev",
            "Yonatan Sverdlov",
            "Tal Amir",
            "Nadav Dym"
        ],
        "comments": "Accepted to Learning on Graphs (LoG) 2025. Version identical to the camera-ready paper",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques. In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks. We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20426",
        "abs_url": "https://arxiv.org/abs/2511.20426",
        "pdf_url": "https://arxiv.org/pdf/2511.20426",
        "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
        "authors": [
            "Hmrishav Bandyopadhyay",
            "Nikhil Pinnaparaju",
            "Rahim Entezari",
            "Jim Scott",
            "Yi-Zhe Song",
            "Varun Jampani"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20459",
        "abs_url": "https://arxiv.org/abs/2511.20459",
        "pdf_url": "https://arxiv.org/pdf/2511.20459",
        "title": "Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts",
        "authors": [
            "Mosab Rezaei",
            "Mina Rajaei Moghadam",
            "Abdul Rahman Shaikh",
            "Hamed Alhoori",
            "Reva Freedman"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20470",
        "abs_url": "https://arxiv.org/abs/2511.20470",
        "pdf_url": "https://arxiv.org/pdf/2511.20470",
        "title": "Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model",
        "authors": [
            "Genís Plaja-Roglans",
            "Yun-Ning Hung",
            "Xavier Serra",
            "Igor Pereira"
        ],
        "comments": "Accepted for oral presentation at IJCNN 2025",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Extracting individual elements from music mixtures is a valuable tool for music production and practice. While neural networks optimized to mask or transform mixture spectrograms into the individual source(s) have been the leading approach, the source overlap and correlation in music signals poses an inherent challenge. Also, accessing all sources in the mixture is crucial to train these systems, while complicated. Attempts to address these challenges in a generative fashion exist, however, the separation performance and inference efficiency remain limited. In this work, we study the potential of diffusion models to advance toward bridging this gap, focusing on generative singing voice separation relying only on corresponding pairs of isolated vocals and mixtures for training. To align with creative workflows, we leverage latent diffusion: the system generates samples encoded in a compact latent space, and subsequently decodes these into audio. This enables efficient optimization and faster inference. Our system is trained using only open data. We outperform existing generative separation systems, and level the compared non-generative systems on a list of signal quality measures and on interference removal. We provide a noise robustness study on the latent encoder, providing insights on its potential for the task. We release a modular toolkit for further research on the topic.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20480",
        "abs_url": "https://arxiv.org/abs/2511.20480",
        "pdf_url": "https://arxiv.org/pdf/2511.20480",
        "title": "Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention Adversarial Dual AutoEncoders",
        "authors": [
            "Sidahmed Benabderrahmane",
            "James Cheney",
            "Talal Rahwan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Advanced Persistent Threats (APTs) pose a significant challenge in cybersecurity due to their stealthy and long-term nature. Modern supervised learning methods require extensive labeled data, which is often scarce in real-world cybersecurity environments. In this paper, we propose an innovative approach that leverages AutoEncoders for unsupervised anomaly detection, augmented by active learning to iteratively improve the detection of APT anomalies. By selectively querying an oracle for labels on uncertain or ambiguous samples, we minimize labeling costs while improving detection rates, enabling the model to improve its detection accuracy with minimal data while reducing the need for extensive manual labeling. We provide a detailed formulation of the proposed Attention Adversarial Dual AutoEncoder-based anomaly detection framework and show how the active learning loop iteratively enhances the model. The framework is evaluated on real-world imbalanced provenance trace databases produced by the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\\% of the data. The datasets span multiple operating systems, including Android, Linux, BSD, and Windows, and cover two attack scenarios. The results have shown significant improvements in detection rates during active learning and better performance compared to other existing approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20490",
        "abs_url": "https://arxiv.org/abs/2511.20490",
        "pdf_url": "https://arxiv.org/pdf/2511.20490",
        "title": "MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology",
        "authors": [
            "Kiril Vasilev",
            "Alexandre Misrahi",
            "Eeshaan Jain",
            "Phil F Cheng",
            "Petros Liakopoulos",
            "Olivier Michielin",
            "Michael Moor",
            "Charlotte Bunne"
        ],
        "comments": "Accepted to NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20500",
        "abs_url": "https://arxiv.org/abs/2511.20500",
        "pdf_url": "https://arxiv.org/pdf/2511.20500",
        "title": "From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection",
        "authors": [
            "Sidahmed Benabderrahmane",
            "Talal Rahwan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20507",
        "abs_url": "https://arxiv.org/abs/2511.20507",
        "pdf_url": "https://arxiv.org/pdf/2511.20507",
        "title": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models",
        "authors": [
            "Nathan Roll",
            "Jill Kries",
            "Flora Jin",
            "Catherine Wang",
            "Ann Marie Finley",
            "Meghan Sumner",
            "Cory Shain",
            "Laura Gwilliams"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) have emerged as a candidate \"model organism\" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20532",
        "abs_url": "https://arxiv.org/abs/2511.20532",
        "pdf_url": "https://arxiv.org/pdf/2511.20532",
        "title": "MIMIC-MJX: Neuromechanical Emulation of Animal Behavior",
        "authors": [
            "Charles Y. Zhang",
            "Yuanjia Yang",
            "Aidan Sirbu",
            "Elliott T.T. Abe",
            "Emil Wärnberg",
            "Eric J. Leonardis",
            "Diego E. Aldarondo",
            "Adam Lee",
            "Aaditya Prasad",
            "Jason Foat",
            "Kaiwen Bian",
            "Joshua Park",
            "Rusham Bhatt",
            "Hutton Saunders",
            "Akira Nagamori",
            "Ayesha R. Thanawalla",
            "Kee Wui Huang",
            "Fabian Plum",
            "Hendrik K. Beck",
            "Steven W. Flavell",
            "David Labonte",
            "Blake A. Richards",
            "Bingni W. Brunton",
            "Eiman Azim",
            "Bence P. Ölveczky",
            "Talmo D. Pereira"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "The primary output of the nervous system is movement and behavior. While recent advances have democratized pose tracking during complex behavior, kinematic trajectories alone provide only indirect access to the underlying control processes. Here we present MIMIC-MJX, a framework for learning biologically-plausible neural control policies from kinematics. MIMIC-MJX models the generative process of motor control by training neural controllers that learn to actuate biomechanically-realistic body models in physics simulation to reproduce real kinematic trajectories. We demonstrate that our implementation is accurate, fast, data-efficient, and generalizable to diverse animal body models. Policies trained with MIMIC-MJX can be utilized to both analyze neural control strategies and simulate behavioral experiments, illustrating its potential as an integrative modeling framework for neuroscience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20540",
        "abs_url": "https://arxiv.org/abs/2511.20540",
        "pdf_url": "https://arxiv.org/pdf/2511.20540",
        "title": "Proceedings Twentieth Conference on Theoretical Aspects of Rationality and Knowledge",
        "authors": [
            "Adam Bjorndahl"
        ],
        "comments": "",
        "subjects": "Logic in Computer Science (cs.LO); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)",
        "abstract": "The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge. Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, uncertainty, awareness, bounded rationality, common sense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems. Information about TARK is available at this http URL. These proceedings contain the papers that have been accepted for presentation at the Twentieth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2025), held July 14--16, 2025, at Heinrich-Heine-Universität, Düsseldorf, Germany. The conference website can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20544",
        "abs_url": "https://arxiv.org/abs/2511.20544",
        "pdf_url": "https://arxiv.org/pdf/2511.20544",
        "title": "New York Smells: A Large Multimodal Dataset for Olfaction",
        "authors": [
            "Ege Ozguroglu",
            "Junbang Liang",
            "Ruoshi Liu",
            "Mia Chiquier",
            "Michael DeTienne",
            "Wesley Wei Qian",
            "Alexandra Horowitz",
            "Andrew Owens",
            "Carl Vondrick"
        ],
        "comments": "Project website at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20551",
        "abs_url": "https://arxiv.org/abs/2511.20551",
        "pdf_url": "https://arxiv.org/pdf/2511.20551",
        "title": "Time-Domain Linear Model-based Framework for Passive Acoustic Mapping of Cavitation Activity",
        "authors": [
            "Tatiana Gelvez-Barrera",
            "Barbara Nicolas",
            "Denis Kouamé",
            "Bruno Gilles",
            "Adrian Basarab"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Passive acoustic mapping enables the spatial mapping and temporal monitoring of cavitation activity, playing a crucial role in therapeutic ultrasound applications. Most conventional beamforming methods, whether implemented in the time or frequency domains, suffer from limited axial resolution due to the absence of a reference emission onset time. While frequency-domain methods, the most efficient of which are based on the cross-spectral matrix, require long signals for accurate estimation, time-domain methods typically achieve lower spatial resolution. To address these limitations, we propose a linear model-based beamforming framework fully formulated in the time domain. The linear forward model relates a discretized spatiotemporal distribution of cavitation activity to the temporal signals recorded by a probe, explicitly accounting for time-of-flight delays dictated by the acquisition geometry. This model is then inverted using regularization techniques that exploit prior knowledge of cavitation activity in both spatial and temporal domains. Experimental results show that the proposed framework achieves enhanced or competitive cavitation map quality while using only 20\\% of the data typically required by frequency-domain methods. This highlights the substantial gain in data efficiency and the flexibility of our spatiotemporal regularization to adapt to diverse passive cavitation scenarios, outperforming state-of-the-art techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20570",
        "abs_url": "https://arxiv.org/abs/2511.20570",
        "pdf_url": "https://arxiv.org/pdf/2511.20570",
        "title": "Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics",
        "authors": [
            "Tasha Kim",
            "Oiwi Parker Jones"
        ],
        "comments": "Embodied and Safe-Assured Robotic Systems workshop at NeurIPS 2025",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20590",
        "abs_url": "https://arxiv.org/abs/2511.20590",
        "pdf_url": "https://arxiv.org/pdf/2511.20590",
        "title": "EnergyTwin: A Multi-Agent System for Simulating and Coordinating Energy Microgrids",
        "authors": [
            "Jakub Muszyński",
            "Ignacy Walużenicz",
            "Patryk Zan",
            "Zofia Wrona",
            "Maria Ganzha",
            "Marcin Paprzycki",
            "Costin Bădică"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "Microgrids are deployed to reduce purchased grid energy, limit exposure to volatile tariffs, and ensure service continuity during disturbances. This requires coordinating heterogeneous distributed energy resources across multiple time scales and under variable conditions. Among existing tools, typically, power-system simulators capture physical behaviour but assume centralized control, while multi-agent frameworks model decentralized decision-making but represent energy with no physical grounding. In this context, the EnergyTwin is introduced, an agent-based microgrid simulation environment that couples physically grounded models with forecast-informed, rolling-horizon planning, and negotiations. Each asset is modeled as an agent, interacting with a central agent that obtains forecasts, formulates predictions, and allocates energy through contract-based interactions. EnergyTwin targets tertiary-layer decision making and is extensible for digital-twin use. Its feasibility was evaluated in a university campus microgrid scenario where multiple planning strategies were compared. Achieved results show that forecast-driven rolling-horizon planning increases local energy self-sufficiency, maintains higher battery reserves, and reduces exposure to low-resilience operating states. They demonstrate also potential of EnergyTwin as platform supporting research on resilient, negotiation-driven microgrids.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20597",
        "abs_url": "https://arxiv.org/abs/2511.20597",
        "pdf_url": "https://arxiv.org/pdf/2511.20597",
        "title": "BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents",
        "authors": [
            "Kaiyuan Zhang",
            "Mark Tenenholtz",
            "Kyle Polley",
            "Jerry Ma",
            "Denis Yarats",
            "Ninghui Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood. In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20601",
        "abs_url": "https://arxiv.org/abs/2511.20601",
        "pdf_url": "https://arxiv.org/pdf/2511.20601",
        "title": "The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting",
        "authors": [
            "Heman Shakeri"
        ],
        "comments": "7 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $\\Delta_{\\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $\\Delta_{\\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $\\Delta_{\\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20604",
        "abs_url": "https://arxiv.org/abs/2511.20604",
        "pdf_url": "https://arxiv.org/pdf/2511.20604",
        "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges",
        "authors": [
            "Yixin Liu",
            "Pengfei Liu",
            "Arman Cohan"
        ],
        "comments": "NeurIPS 2025 Camera Ready",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20613",
        "abs_url": "https://arxiv.org/abs/2511.20613",
        "pdf_url": "https://arxiv.org/pdf/2511.20613",
        "title": "Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning",
        "authors": [
            "Panayiotis Danassis",
            "Naman Goel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20621",
        "abs_url": "https://arxiv.org/abs/2511.20621",
        "pdf_url": "https://arxiv.org/pdf/2511.20621",
        "title": "DiFR: Inference Verification Despite Nondeterminism",
        "authors": [
            "Adam Karvonen",
            "Daniel Reuter",
            "Roy Rinberg",
            "Luke Marks",
            "Adrià Garriga-Alonso",
            "Keri Warr"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20626",
        "abs_url": "https://arxiv.org/abs/2511.20626",
        "pdf_url": "https://arxiv.org/pdf/2511.20626",
        "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training",
        "authors": [
            "Wei He",
            "Kai Han",
            "Hang Zhou",
            "Hanting Chen",
            "Zhicheng Liu",
            "Xinghao Chen",
            "Yunhe Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20639",
        "abs_url": "https://arxiv.org/abs/2511.20639",
        "pdf_url": "https://arxiv.org/pdf/2511.20639",
        "title": "Latent Collaboration in Multi-Agent Systems",
        "authors": [
            "Jiaru Zou",
            "Xiyuan Yang",
            "Ruizhong Qiu",
            "Gaotang Li",
            "Katherine Tieu",
            "Pan Lu",
            "Ke Shen",
            "Hanghang Tong",
            "Yejin Choi",
            "Jingrui He",
            "James Zou",
            "Mengdi Wang",
            "Ling Yang"
        ],
        "comments": "Project: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-11-26",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True",
        "arxiv_id": "2511.20650",
        "abs_url": "https://arxiv.org/abs/2511.20650",
        "pdf_url": "https://arxiv.org/pdf/2511.20650",
        "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities",
        "authors": [
            "Tooba Tehreem Sheikh",
            "Jean Lahoud",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Salman Khan",
            "Hisham Cholakkal"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]