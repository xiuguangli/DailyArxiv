[
    {
        "order": 1,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00831",
        "abs_url": "https://arxiv.org/abs/2601.00831",
        "pdf_url": "https://arxiv.org/pdf/2601.00831",
        "title": "Horizon Reduction as Information Loss in Offline Reinforcement Learning",
        "authors": [
            "Uday Kumar Nidadala",
            "Venkata Bhumika Guthi"
        ],
        "comments": "13 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00834",
        "abs_url": "https://arxiv.org/abs/2601.00834",
        "pdf_url": "https://arxiv.org/pdf/2601.00834",
        "title": "Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds",
        "authors": [
            "Julian Evan Chrisnanto",
            "Salsabila Rahma Alia",
            "Nurfauzi Fadillah",
            "Yulison Herry Chrisnanto"
        ],
        "comments": "19 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a \"Stochastic Cloth\" manifold with extreme Gaussian curvature fluctuations ($K \\in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the \"splitting spot\" and \"labyrinthine\" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\\mathcal{E}_{mass} \\approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00841",
        "abs_url": "https://arxiv.org/abs/2601.00841",
        "pdf_url": "https://arxiv.org/pdf/2601.00841",
        "title": "SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes",
        "authors": [
            "Bharath Nunepalli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00847",
        "abs_url": "https://arxiv.org/abs/2601.00847",
        "pdf_url": "https://arxiv.org/pdf/2601.00847",
        "title": "You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference",
        "authors": [
            "Ryan Shamim"
        ],
        "comments": "24 pages, 5 figures. Deterministic evaluation protocol. Includes theoretical analysis and empirical validation on GPT-2 and Gemma 2 9B",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00850",
        "abs_url": "https://arxiv.org/abs/2601.00850",
        "pdf_url": "https://arxiv.org/pdf/2601.00850",
        "title": "EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference",
        "authors": [
            "Aayush Kumar"
        ],
        "comments": "24 pages,3 Figures, Submitting to IEEE Access",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00857",
        "abs_url": "https://arxiv.org/abs/2601.00857",
        "pdf_url": "https://arxiv.org/pdf/2601.00857",
        "title": "Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks",
        "authors": [
            "Yuchi Ma",
            "Yawen Shen",
            "Anu Swatantran",
            "David B. Lobell"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00862",
        "abs_url": "https://arxiv.org/abs/2601.00862",
        "pdf_url": "https://arxiv.org/pdf/2601.00862",
        "title": "Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions",
        "authors": [
            "Joey Chan",
            "Huan Wang",
            "Haoyu Pan",
            "Wei Wu",
            "Zirong Wang",
            "Zhen Chen",
            "Ershun Pan",
            "Min Xie",
            "Lifeng Xi"
        ],
        "comments": "Due to space limitations, the open-source method for supporting materials is currently under discussion",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\\,^{\\circ}\\mathrm{C}$ to $45\\,^{\\circ}\\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00863",
        "abs_url": "https://arxiv.org/abs/2601.00863",
        "pdf_url": "https://arxiv.org/pdf/2601.00863",
        "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
        "authors": [
            "Markus J. Buehler"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Disordered Systems and Neural Networks (cond-mat.dis-nn); Biomolecules (q-bio.BM)",
        "abstract": "We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00864",
        "abs_url": "https://arxiv.org/abs/2601.00864",
        "pdf_url": "https://arxiv.org/pdf/2601.00864",
        "title": "Distribution Matching for Graph Quantification Under Structural Covariate Shift",
        "authors": [
            "Clemens Damke",
            "Eyke Hüllermeier"
        ],
        "comments": "17 pages, presented at ECML-PKDD 2025",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00868",
        "abs_url": "https://arxiv.org/abs/2601.00868",
        "pdf_url": "https://arxiv.org/pdf/2601.00868",
        "title": "SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation",
        "authors": [
            "Aditya Sreevatsa K",
            "Arun Kumar Raveendran",
            "Jesrael K Mani",
            "Prakash G Shigli",
            "Rajkumar Rangadore",
            "Narayana Darapaneni",
            "Anwesh Reddy Paduri"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00873",
        "abs_url": "https://arxiv.org/abs/2601.00873",
        "pdf_url": "https://arxiv.org/pdf/2601.00873",
        "title": "Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems",
        "authors": [
            "Osasumwen Cedric Ogiesoba-Eguakun",
            "Suman Rath"
        ],
        "comments": "10 pages",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00877",
        "abs_url": "https://arxiv.org/abs/2601.00877",
        "pdf_url": "https://arxiv.org/pdf/2601.00877",
        "title": "LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification",
        "authors": [
            "Thomas Andrews",
            "Mark Law",
            "Sara Ahmadi-Abhari",
            "Alessandra Russo"
        ],
        "comments": "NeurIPS 2025, Data on the Brain & Mind Workshop",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00883",
        "abs_url": "https://arxiv.org/abs/2601.00883",
        "pdf_url": "https://arxiv.org/pdf/2601.00883",
        "title": "Outlier Detection Using Vector Cosine Similarity by Adding a Dimension",
        "authors": [
            "Zhongyang Shen"
        ],
        "comments": "This is an updated version of the paper originally published in ICAIIC 2024 (DOI: https://doi.org/10.1109/ICAIIC60209.2024.10463442). Changes include minor typographical and grammatical corrections, as well as an added description of an optimized open-source Python implementation (MDOD) available on PyPI at this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00889",
        "abs_url": "https://arxiv.org/abs/2601.00889",
        "pdf_url": "https://arxiv.org/pdf/2601.00889",
        "title": "FANoS: Friction-Adaptive Nosé--Hoover Symplectic Momentum for Stiff Objectives",
        "authors": [
            "Nalin Dhiman"
        ],
        "comments": "13 pages, 5 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study a physics-inspired optimizer, \\emph{FANoS} (Friction-Adaptive Nosé--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nosé--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic. We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\\times 10^{-3}$, and L-BFGS reaches $\\approx 4.4\\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance. Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00894",
        "abs_url": "https://arxiv.org/abs/2601.00894",
        "pdf_url": "https://arxiv.org/pdf/2601.00894",
        "title": "When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training",
        "authors": [
            "Gihyeon Sim"
        ],
        "comments": "14 pages, 1 figure, 14 tables, code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00898",
        "abs_url": "https://arxiv.org/abs/2601.00898",
        "pdf_url": "https://arxiv.org/pdf/2601.00898",
        "title": "Dichotomous Diffusion Policy Optimization",
        "authors": [
            "Ruiming Liang",
            "Yinan Zheng",
            "Kexin Zheng",
            "Tianyi Tan",
            "Jianxiong Li",
            "Liyuan Mao",
            "Zhihao Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Jingjing Liu",
            "Jinqiao Wang",
            "Xianyuan Zhan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of this http URL in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00908",
        "abs_url": "https://arxiv.org/abs/2601.00908",
        "pdf_url": "https://arxiv.org/pdf/2601.00908",
        "title": "Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment",
        "authors": [
            "Chorok Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00915",
        "abs_url": "https://arxiv.org/abs/2601.00915",
        "pdf_url": "https://arxiv.org/pdf/2601.00915",
        "title": "Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles",
        "authors": [
            "Jacquelyn Shelton",
            "Przemyslaw Polewski",
            "Alexander Robel",
            "Matthew Hoffman",
            "Stephen Price"
        ],
        "comments": "draft / preliminary",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00920",
        "abs_url": "https://arxiv.org/abs/2601.00920",
        "pdf_url": "https://arxiv.org/pdf/2601.00920",
        "title": "MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs",
        "authors": [
            "Xingsheng Chen",
            "Regina Zhang",
            "Bo Gao",
            "Xingwei He",
            "Xiaofeng Liu",
            "Pietro Lio",
            "Kwok-Yan Lam",
            "Siu-Ming Yiu"
        ],
        "comments": "12 pages, 6 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00924",
        "abs_url": "https://arxiv.org/abs/2601.00924",
        "pdf_url": "https://arxiv.org/pdf/2601.00924",
        "title": "Complexity-based code embeddings",
        "authors": [
            "Rares Folea",
            "Radu Iacob",
            "Emil Slusanschi",
            "Traian Rebedea"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00932",
        "abs_url": "https://arxiv.org/abs/2601.00932",
        "pdf_url": "https://arxiv.org/pdf/2601.00932",
        "title": "Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation",
        "authors": [
            "Andrea Thomas Nava",
            "Lijo Johny",
            "Fabio Azzalini",
            "Johannes Schneider",
            "Arianna Casanova"
        ],
        "comments": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \\emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00942",
        "abs_url": "https://arxiv.org/abs/2601.00942",
        "pdf_url": "https://arxiv.org/pdf/2601.00942",
        "title": "Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures",
        "authors": [
            "Kabir Grover"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00968",
        "abs_url": "https://arxiv.org/abs/2601.00968",
        "pdf_url": "https://arxiv.org/pdf/2601.00968",
        "title": "Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks",
        "authors": [
            "Longwei Wang",
            "Mohammad Navid Nayyem",
            "Abdullah Al Rakin",
            "KC Santosh",
            "Chaowei Zhang",
            "Yang Zhou"
        ],
        "comments": "8pages,4 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00970",
        "abs_url": "https://arxiv.org/abs/2601.00970",
        "pdf_url": "https://arxiv.org/pdf/2601.00970",
        "title": "Zero-shot Forecasting by Simulation Alone",
        "authors": [
            "Boris N. Oreshkin",
            "Mayank Jauhari",
            "Ravi Kiran Selvam",
            "Malcolm Wolff",
            "Wenhao Pan",
            "Shankar Ramasubramanian",
            "Kin G. Olivares",
            "Tatiana Konstantinova",
            "Andres Potapczynski",
            "Mengfei Cao",
            "Dmitry Efimov",
            "Michael W. Mahoney",
            "Andrew G. Wilson"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a \"student-beats-teacher\" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01003",
        "abs_url": "https://arxiv.org/abs/2601.01003",
        "pdf_url": "https://arxiv.org/pdf/2601.01003",
        "title": "Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations",
        "authors": [
            "Amin Abyaneh",
            "Charlotte Morissette",
            "Mohamad H. Danesh",
            "Anas El Houssaini",
            "David Meger",
            "Gregory Dudek",
            "Hsiu-Chin Lin"
        ],
        "comments": "Under review at ICLR 2026",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01014",
        "abs_url": "https://arxiv.org/abs/2601.01014",
        "pdf_url": "https://arxiv.org/pdf/2601.01014",
        "title": "Geometric and Dynamic Scaling in Deep Transformers",
        "authors": [
            "Haoran Su",
            "Chenyu You"
        ],
        "comments": "Research Proposal Only",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01021",
        "abs_url": "https://arxiv.org/abs/2601.01021",
        "pdf_url": "https://arxiv.org/pdf/2601.01021",
        "title": "Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations",
        "authors": [
            "Dai Shi",
            "Lequan Lin",
            "Andi Han",
            "Luke Thompson",
            "José Miguel Hernández-Lobato",
            "Zhiyong Wang",
            "Junbin Gao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01023",
        "abs_url": "https://arxiv.org/abs/2601.01023",
        "pdf_url": "https://arxiv.org/pdf/2601.01023",
        "title": "Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning",
        "authors": [
            "João Morais",
            "Sadjad Alikhani",
            "Akshay Malhotra",
            "Shahab Hamidi-Rad",
            "Ahmed Alkhateeb"
        ],
        "comments": "resources available in: this https URL",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01045",
        "abs_url": "https://arxiv.org/abs/2601.01045",
        "pdf_url": "https://arxiv.org/pdf/2601.01045",
        "title": "Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI",
        "authors": [
            "Tatsuaki Tsuruyama"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses. In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01062",
        "abs_url": "https://arxiv.org/abs/2601.01062",
        "pdf_url": "https://arxiv.org/pdf/2601.01062",
        "title": "SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models",
        "authors": [
            "Yunlin Zeng"
        ],
        "comments": "14 pages, 3 figures. Accepted to WVAQ 2026, WACV 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\\% win rate) and narrative depth (+50\\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01065",
        "abs_url": "https://arxiv.org/abs/2601.01065",
        "pdf_url": "https://arxiv.org/pdf/2601.01065",
        "title": "Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco",
        "authors": [
            "Achraf Hsain",
            "Yahya Zaki",
            "Othman Abaakil",
            "Hibat-allah Bekkar",
            "Yousra Chtouki"
        ],
        "comments": "Published in IEEE GCAIoT 2024",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP); Systems and Control (eess.SY)",
        "abstract": "Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01069",
        "abs_url": "https://arxiv.org/abs/2601.01069",
        "pdf_url": "https://arxiv.org/pdf/2601.01069",
        "title": "Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs",
        "authors": [
            "Jing Wang",
            "Peng Zhao",
            "Zhi-Hua Zhou"
        ],
        "comments": "accepted by IEEE Transactions on Information Theory. arXiv admin note: substantial text overlap with arXiv:2303.02691",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \\emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\\tilde{O}(k_\\mu^{5/4} c_\\mu^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\\tilde{O}(k_\\mu^{2} c_\\mu^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_\\mu$ and $c_\\mu$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01082",
        "abs_url": "https://arxiv.org/abs/2601.01082",
        "pdf_url": "https://arxiv.org/pdf/2601.01082",
        "title": "Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces",
        "authors": [
            "Bryon Tjanaka",
            "Henry Chen",
            "Matthew C. Fontaine",
            "Stefanos Nikolaidis"
        ],
        "comments": "Source code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01089",
        "abs_url": "https://arxiv.org/abs/2601.01089",
        "pdf_url": "https://arxiv.org/pdf/2601.01089",
        "title": "Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding",
        "authors": [
            "Nobuyuki Ota"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Genomics (q-bio.GN)",
        "abstract": "Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01119",
        "abs_url": "https://arxiv.org/abs/2601.01119",
        "pdf_url": "https://arxiv.org/pdf/2601.01119",
        "title": "Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings",
        "authors": [
            "Muhammad Ashad Kabir",
            "Sirajam Munira",
            "Dewan Tasnia Azad",
            "Saleh Mohammed Ikram",
            "Mohammad Habibur Rahman Sarker",
            "Syed Manzoor Ahmed Hanifi"
        ],
        "comments": "27 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01127",
        "abs_url": "https://arxiv.org/abs/2601.01127",
        "pdf_url": "https://arxiv.org/pdf/2601.01127",
        "title": "Wittgenstein's Family Resemblance Clustering Algorithm",
        "authors": [
            "Golbahar Amanpour",
            "Benyamin Ghojogh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01146",
        "abs_url": "https://arxiv.org/abs/2601.01146",
        "pdf_url": "https://arxiv.org/pdf/2601.01146",
        "title": "Self-Training the Neurochaos Learning Algorithm",
        "authors": [
            "Anusree M",
            "Akhila Henry",
            "Pramod P Nair"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01150",
        "abs_url": "https://arxiv.org/abs/2601.01150",
        "pdf_url": "https://arxiv.org/pdf/2601.01150",
        "title": "Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification",
        "authors": [
            "Wenbin Pei",
            "Ruohao Dai",
            "Bing Xue",
            "Mengjie Zhang",
            "Qiang Zhang",
            "Yiu-Ming Cheung"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01206",
        "abs_url": "https://arxiv.org/abs/2601.01206",
        "pdf_url": "https://arxiv.org/pdf/2601.01206",
        "title": "MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches",
        "authors": [
            "Soroush Elyasi",
            "Arya VarastehNezhad",
            "Fattaneh Taghiyareh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Software Engineering (cs.SE)",
        "abstract": "Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01207",
        "abs_url": "https://arxiv.org/abs/2601.01207",
        "pdf_url": "https://arxiv.org/pdf/2601.01207",
        "title": "Sparse Bayesian Message Passing under Structural Uncertainty",
        "authors": [
            "Yoonhyuk Choi",
            "Jiho Choi",
            "Chanran Kim",
            "Yumin Lee",
            "Hawon Shin",
            "Yeowon Jeon",
            "Minjeong Kim",
            "Jiwoo Kang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01223",
        "abs_url": "https://arxiv.org/abs/2601.01223",
        "pdf_url": "https://arxiv.org/pdf/2601.01223",
        "title": "Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data",
        "authors": [
            "Marzieh Amiri Shahbazi",
            "Ali Baheri",
            "Nasibeh Azadeh-Fard"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01231",
        "abs_url": "https://arxiv.org/abs/2601.01231",
        "pdf_url": "https://arxiv.org/pdf/2601.01231",
        "title": "The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context",
        "authors": [
            "Md Muhtasim Munif Fahim",
            "Humyra Ankona",
            "Md Monimul Huq",
            "Md Rezaul Karim"
        ],
        "comments": "Conference Paper",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations. Purpose: This study introduces the \"Dependency Divide\", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments. Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis. Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions. Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01268",
        "abs_url": "https://arxiv.org/abs/2601.01268",
        "pdf_url": "https://arxiv.org/pdf/2601.01268",
        "title": "Accelerated Full Waveform Inversion by Deep Compressed Learning",
        "authors": [
            "Maayan Gelboim",
            "Amir Adler",
            "Mauricio Araya-Polo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01290",
        "abs_url": "https://arxiv.org/abs/2601.01290",
        "pdf_url": "https://arxiv.org/pdf/2601.01290",
        "title": "The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification",
        "authors": [
            "Harshita Narnoli",
            "Mihai Surdeanu"
        ],
        "comments": "International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01295",
        "abs_url": "https://arxiv.org/abs/2601.01295",
        "pdf_url": "https://arxiv.org/pdf/2601.01295",
        "title": "Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space",
        "authors": [
            "Changhoon Song",
            "Seungchan Ko",
            "Youngjoon Hong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \\le 1/2$. In this paper, we introduce a log-weighted Barron space $\\mathscr{B}^{\\log}$, which requires a strictly weaker assumption than $\\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\\mathscr{B}^{\\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\\mathscr{B}^{s,\\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01298",
        "abs_url": "https://arxiv.org/abs/2601.01298",
        "pdf_url": "https://arxiv.org/pdf/2601.01298",
        "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware",
        "authors": [
            "Jorge L. Ruiz Williams"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)",
        "abstract": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01306",
        "abs_url": "https://arxiv.org/abs/2601.01306",
        "pdf_url": "https://arxiv.org/pdf/2601.01306",
        "title": "Towards a Principled Muon under $μ\\mathsf{P}$: Ensuring Spectral Conditions throughout Training",
        "authors": [
            "John Zhao"
        ],
        "comments": "21 pages, 0 figures",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "The $\\mu$-parameterization ($\\mu$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $\\mu$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $\\mu$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $\\mu$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $\\mu$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $\\mu$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01313",
        "abs_url": "https://arxiv.org/abs/2601.01313",
        "pdf_url": "https://arxiv.org/pdf/2601.01313",
        "title": "Spectral-Window Hybrid (SWH)",
        "authors": [
            "Vladimer Khasia"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \\textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \\textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\\mathcal{O}(T \\log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01347",
        "abs_url": "https://arxiv.org/abs/2601.01347",
        "pdf_url": "https://arxiv.org/pdf/2601.01347",
        "title": "From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion",
        "authors": [
            "Yuyan Pi",
            "Min Jin",
            "Wentao Xie",
            "Xinhua Liu"
        ],
        "comments": "34 pages,5 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01357",
        "abs_url": "https://arxiv.org/abs/2601.01357",
        "pdf_url": "https://arxiv.org/pdf/2601.01357",
        "title": "Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows",
        "authors": [
            "Ke Xiao",
            "Haoze Zhang",
            "Runze Mao",
            "Han Li",
            "Zhi X. Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01368",
        "abs_url": "https://arxiv.org/abs/2601.01368",
        "pdf_url": "https://arxiv.org/pdf/2601.01368",
        "title": "Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach",
        "authors": [
            "Mujin Zhou",
            "Junzhe Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01387",
        "abs_url": "https://arxiv.org/abs/2601.01387",
        "pdf_url": "https://arxiv.org/pdf/2601.01387",
        "title": "Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning",
        "authors": [
            "Yongzhe Li",
            "Lin Guan",
            "Zihan Cai",
            "Zuxian Lin",
            "Jiyu Huang",
            "Liukai Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01417",
        "abs_url": "https://arxiv.org/abs/2601.01417",
        "pdf_url": "https://arxiv.org/pdf/2601.01417",
        "title": "A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory",
        "authors": [
            "Itay Safran"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We consider the problem of exact computation of the maximum function over $d$ real inputs using ReLU neural networks. We prove a depth hierarchy, wherein width $\\Omega\\big(d^{1+\\frac{1}{2^{k-2}-1}}\\big)$ is necessary to represent the maximum for any depth $3\\le k\\le \\log_2(\\log_2(d))$. This is the first unconditional super-linear lower bound for this fundamental operator at depths $k\\ge3$, and it holds even if the depth scales with $d$. Our proof technique is based on a combinatorial argument and associates the non-differentiable ridges of the maximum with cliques in a graph induced by the first hidden layer of the computing network, utilizing Turán's theorem from extremal graph theory to show that a sufficiently narrow network cannot capture the non-linearities of the maximum. This suggests that despite its simple nature, the maximum function possesses an inherent complexity that stems from the geometric structure of its non-differentiable hyperplanes, and provides a novel approach for proving lower bounds for deep neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01424",
        "abs_url": "https://arxiv.org/abs/2601.01424",
        "pdf_url": "https://arxiv.org/pdf/2601.01424",
        "title": "Unveiling the Heart-Brain Connection: An Analysis of ECG in Cognitive Performance",
        "authors": [
            "Akshay Sasi",
            "Malavika Pradeep",
            "Nusaibah Farrukh",
            "Rahul Venugopal",
            "Elizabeth Sherly"
        ],
        "comments": "6 pages, 6 figures. Code available at this https URL. Presented at AIHC (not published)",
        "subjects": "Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)",
        "abstract": "Understanding the interaction of neural and cardiac systems during cognitive activity is critical to advancing physiological computing. Although EEG has been the gold standard for assessing mental workload, its limited portability restricts its real-world use. Widely available ECG through wearable devices proposes a pragmatic alternative. This research investigates whether ECG signals can reliably reflect cognitive load and serve as proxies for EEG-based indicators. In this work, we present multimodal data acquired from two different paradigms involving working-memory and passive-listening tasks. For each modality, we extracted ECG time-domain HRV metrics and Catch22 descriptors against EEG spectral and Catch22 features, respectively. We propose a cross-modal XGBoost framework to project the ECG features onto EEG-representative cognitive spaces, thereby allowing workload inferences using only ECG. Our results show that ECG-derived projections expressively capture variation in cognitive states and provide good support for accurate classification. Our findings underpin ECG as an interpretable, real-time, wearable solution for everyday cognitive monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01465",
        "abs_url": "https://arxiv.org/abs/2601.01465",
        "pdf_url": "https://arxiv.org/pdf/2601.01465",
        "title": "Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD",
        "authors": [
            "Ze Peng",
            "Jian Zhang",
            "Yisen Wang",
            "Lei Qi",
            "Yinghuan Shi",
            "Yang Gao"
        ],
        "comments": "Published as a conference paper at ICLR 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD's generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called \"omniscient trajectory\". When applied to Gradient Descent's minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds' $\\Omega(1)$ rates to $O(1/\\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01473",
        "abs_url": "https://arxiv.org/abs/2601.01473",
        "pdf_url": "https://arxiv.org/pdf/2601.01473",
        "title": "Accelerating Storage-Based Training for Graph Neural Networks",
        "authors": [
            "Myung-Hwan Jang",
            "Jeong-Min Park",
            "Yunyong Ko",
            "Sang-Wook Kim"
        ],
        "comments": "10 pages, 12 figures, 2 tables, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \\textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \\textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \\textsf{AGNES}, that employs a method of \\textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \\textsf{AGNES} employs a simple yet effective strategy, \\textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \\textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\\times$ faster than the best competitor. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01475",
        "abs_url": "https://arxiv.org/abs/2601.01475",
        "pdf_url": "https://arxiv.org/pdf/2601.01475",
        "title": "Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts",
        "authors": [
            "Ruofeng Yang",
            "Yongcan Li",
            "Bo Jiang",
            "Cheng Chen",
            "Shuai Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \\times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\\sqrt{\\Sigma_{k=1}^Kn_k}\\sqrt{\\Sigma_{k=1}^Kn_kd_k}/\\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01484",
        "abs_url": "https://arxiv.org/abs/2601.01484",
        "pdf_url": "https://arxiv.org/pdf/2601.01484",
        "title": "SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines",
        "authors": [
            "Itai Morad",
            "Nir Shlezinger",
            "Yonina C. Eldar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Knowledge Distillation (KD) is a central paradigm for transferring knowledge from a large teacher network to a typically smaller student model, often by leveraging soft probabilistic outputs. While KD has shown strong empirical success in numerous applications, its theoretical underpinnings remain only partially understood. In this work, we adopt a Bayesian perspective on KD to rigorously analyze the convergence behavior of students trained with Stochastic Gradient Descent (SGD). We study two regimes: $(i)$ when the teacher provides the exact Bayes Class Probabilities (BCPs); and $(ii)$ supervision with noisy approximations of the BCPs. Our analysis shows that learning from BCPs yields variance reduction and removes neighborhood terms in the convergence bounds compared to one-hot supervision. We further characterize how the level of noise affects generalization and accuracy. Motivated by these insights, we advocate the use of Bayesian deep learning models, which typically provide improved estimates of the BCPs, as teachers in KD. Consistent with our analysis, we experimentally demonstrate that students distilled from Bayesian teachers not only achieve higher accuracies (up to +4.27%), but also exhibit more stable convergence (up to 30% less noise), compared to students distilled from deterministic teachers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01493",
        "abs_url": "https://arxiv.org/abs/2601.01493",
        "pdf_url": "https://arxiv.org/pdf/2601.01493",
        "title": "Accelerating Decentralized Optimization via Overlapping Local Steps",
        "authors": [
            "Yijie Zhou",
            "Shi Pu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Decentralized optimization has emerged as a critical paradigm for distributed learning, enabling scalable training while preserving data privacy through peer-to-peer collaboration. However, existing methods often suffer from communication bottlenecks due to frequent synchronization between nodes. We present Overlapping Local Decentralized SGD (OLDSGD), a novel approach to accelerate decentralized training by computation-communication overlapping, significantly reducing network idle time. With a deliberately designed update, OLDSGD preserves the same average update as Local SGD while avoiding communication-induced stalls. Theoretically, we establish non-asymptotic convergence rates for smooth non-convex objectives, showing that OLDSGD retains the same iteration complexity as standard Local Decentralized SGD while improving per-iteration runtime. Empirical results demonstrate OLDSGD's consistent improvements in wall-clock time convergence under different levels of communication delays. With minimal modifications to existing frameworks, OLDSGD offers a practical solution for faster decentralized learning without sacrificing theoretical guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01501",
        "abs_url": "https://arxiv.org/abs/2601.01501",
        "pdf_url": "https://arxiv.org/pdf/2601.01501",
        "title": "Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE",
        "authors": [
            "Fan Xu",
            "Wei Gong",
            "Hao Wu",
            "Lilan Peng",
            "Nan Wang",
            "Qingsong Wen",
            "Xian Wu",
            "Kun Wang",
            "Xibin Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01580",
        "abs_url": "https://arxiv.org/abs/2601.01580",
        "pdf_url": "https://arxiv.org/pdf/2601.01580",
        "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs",
        "authors": [
            "Zibo Zhao",
            "Yuanting Zha",
            "Haipeng Zhang",
            "Xingcheng Xu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($\\pi_{sample}$) for generation and decision ($\\pi_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $\\pi_{sample}$ while leaving $\\pi_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($\\pi_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01616",
        "abs_url": "https://arxiv.org/abs/2601.01616",
        "pdf_url": "https://arxiv.org/pdf/2601.01616",
        "title": "Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry",
        "authors": [
            "Md Istiauk Hossain Rifat",
            "Moin Khan",
            "Mohammad Zunaed"
        ],
        "comments": "9 pages, 9 figures",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01649",
        "abs_url": "https://arxiv.org/abs/2601.01649",
        "pdf_url": "https://arxiv.org/pdf/2601.01649",
        "title": "Communication-Efficient Federated AUC Maximization with Cyclic Client Participation",
        "authors": [
            "Umesh Vangapally",
            "Wenhan Wu",
            "Chen Chen",
            "Zhishuai Guo"
        ],
        "comments": "Accepted to Transactions on Machine Learning Research (TMLR)",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\\widetilde{O}(1/\\epsilon^{1/2})$ and iteration complexity of $\\widetilde{O}(1/\\epsilon)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/\\epsilon^3)$ and an iteration complexity of $O(1/\\epsilon^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\\widetilde{O}(1/\\epsilon^{1/2})$ and iteration complexity of $\\widetilde{O}(1/\\epsilon)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01663",
        "abs_url": "https://arxiv.org/abs/2601.01663",
        "pdf_url": "https://arxiv.org/pdf/2601.01663",
        "title": "Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths",
        "authors": [
            "He Sun",
            "Jiwoong Shin",
            "Ravi Dhar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We study generative modeling of \\emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \\emph{distribution matching} for trajectory-derived statistics. We propose \\textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01664",
        "abs_url": "https://arxiv.org/abs/2601.01664",
        "pdf_url": "https://arxiv.org/pdf/2601.01664",
        "title": "Who is the Winning Algorithm? Rank Aggregation for Comparative Studies",
        "authors": [
            "Amichai Painsky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01678",
        "abs_url": "https://arxiv.org/abs/2601.01678",
        "pdf_url": "https://arxiv.org/pdf/2601.01678",
        "title": "HeurekaBench: A Benchmarking Framework for AI Co-scientist",
        "authors": [
            "Siba Smarak Panigrahi",
            "Jovana Videnović",
            "Maria Brbić"
        ],
        "comments": "33 pages, 5 figures, 7 tables. Code available at this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01688",
        "abs_url": "https://arxiv.org/abs/2601.01688",
        "pdf_url": "https://arxiv.org/pdf/2601.01688",
        "title": "DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors",
        "authors": [
            "Yash Thesia",
            "Meera Suthar"
        ],
        "comments": "8 pages, 3 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the \"Cold Start\" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique \"optimization trajectory\" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01692",
        "abs_url": "https://arxiv.org/abs/2601.01692",
        "pdf_url": "https://arxiv.org/pdf/2601.01692",
        "title": "Enhanced Multi-model Online Conformal Prediction",
        "authors": [
            "Erfan Hajihashemi",
            "Yanning Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01701",
        "abs_url": "https://arxiv.org/abs/2601.01701",
        "pdf_url": "https://arxiv.org/pdf/2601.01701",
        "title": "Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT",
        "authors": [
            "Mohammed Ayalew Belay",
            "Adil Rasheed",
            "Pierluigi Salvo Rossi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01714",
        "abs_url": "https://arxiv.org/abs/2601.01714",
        "pdf_url": "https://arxiv.org/pdf/2601.01714",
        "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning",
        "authors": [
            "Kareem Ahmed",
            "Sameer Singh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01754",
        "abs_url": "https://arxiv.org/abs/2601.01754",
        "pdf_url": "https://arxiv.org/pdf/2601.01754",
        "title": "Context-Free Recognition with Transformers",
        "authors": [
            "Selim Jerad",
            "Anej Svete",
            "Sophie Hao",
            "Ryan Cotterell",
            "William Merrill"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Complexity (cs.CC); Computation and Language (cs.CL); Formal Languages and Automata Theory (cs.FL)",
        "abstract": "Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\\mathcal{O}(\\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\\mathcal{O}(\\log n)$ looping layers and $\\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01786",
        "abs_url": "https://arxiv.org/abs/2601.01786",
        "pdf_url": "https://arxiv.org/pdf/2601.01786",
        "title": "UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk",
        "authors": [
            "Intae Jeon",
            "Yujeong Kwon",
            "Hyungjoon Koo"
        ],
        "comments": "11 pages, 7 Tables, 6 Figures To appear in the Software Engineering in Practice (SEIP) track of ICSE",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01792",
        "abs_url": "https://arxiv.org/abs/2601.01792",
        "pdf_url": "https://arxiv.org/pdf/2601.01792",
        "title": "HyperCLOVA X 8B Omni",
        "authors": [
            "NAVER Cloud HyperCLOVA X Team"
        ],
        "comments": "Technical Report",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Sound (cs.SD)",
        "abstract": "In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01793",
        "abs_url": "https://arxiv.org/abs/2601.01793",
        "pdf_url": "https://arxiv.org/pdf/2601.01793",
        "title": "Distributed Federated Learning by Alternating Periods of Training",
        "authors": [
            "Shamik Bhattacharyya",
            "Rachel Kalpana Kalaimani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01800",
        "abs_url": "https://arxiv.org/abs/2601.01800",
        "pdf_url": "https://arxiv.org/pdf/2601.01800",
        "title": "Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving",
        "authors": [
            "Qi Wei",
            "Junchao Fan",
            "Zhao Yang",
            "Jianhua Wang",
            "Jingkai Mao",
            "Xiaolin Chang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\\% across all cases compared to state-of-the-art baseline methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01829",
        "abs_url": "https://arxiv.org/abs/2601.01829",
        "pdf_url": "https://arxiv.org/pdf/2601.01829",
        "title": "RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data",
        "authors": [
            "Peiyan Hu",
            "Haodong Feng",
            "Hongyuan Liu",
            "Tongtong Yan",
            "Wenhao Deng",
            "Tianrun Gao",
            "Rong Zheng",
            "Haoren Zheng",
            "Chenglei Yu",
            "Chuanrui Wang",
            "Kaiwen Li",
            "Zhi-Ming Ma",
            "Dezhi Zhou",
            "Xingcai Lu",
            "Dixia Fan",
            "Tailin Wu"
        ],
        "comments": "46 pages, 21 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01833",
        "abs_url": "https://arxiv.org/abs/2601.01833",
        "pdf_url": "https://arxiv.org/pdf/2601.01833",
        "title": "FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks",
        "authors": [
            "Chenyu Hu",
            "Qiming Hu",
            "Sinan Chen",
            "Nianyu Li",
            "Mingyue Zhang",
            "Jialong Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01840",
        "abs_url": "https://arxiv.org/abs/2601.01840",
        "pdf_url": "https://arxiv.org/pdf/2601.01840",
        "title": "Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack",
        "authors": [
            "Qiantao Yang",
            "Liquan Chen",
            "Mingfu Xue",
            "Songze Li"
        ],
        "comments": "Accepted in AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01860",
        "abs_url": "https://arxiv.org/abs/2601.01860",
        "pdf_url": "https://arxiv.org/pdf/2601.01860",
        "title": "High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation",
        "authors": [
            "Shuta Kikuchi",
            "Shu Tanaka"
        ],
        "comments": "6 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01887",
        "abs_url": "https://arxiv.org/abs/2601.01887",
        "pdf_url": "https://arxiv.org/pdf/2601.01887",
        "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
        "authors": [
            "Jiawen Zhang",
            "Lipeng He",
            "Kejia Chen",
            "Jian Lou",
            "Jian Liu",
            "Xiaohu Yang",
            "Ruoxi Jia"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01901",
        "abs_url": "https://arxiv.org/abs/2601.01901",
        "pdf_url": "https://arxiv.org/pdf/2601.01901",
        "title": "FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data",
        "authors": [
            "Yuexuan Xia",
            "Yinghao Zhang",
            "Yalin Liu",
            "Hong-Ning Dai",
            "Yong Xia"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01903",
        "abs_url": "https://arxiv.org/abs/2601.01903",
        "pdf_url": "https://arxiv.org/pdf/2601.01903",
        "title": "TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train",
        "authors": [
            "Ungsik Kim",
            "Suwon Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\\ell \\cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \\mapsto \\text{FSI}(v)$ admits an MPO representation with TT-rank $O(\\ell d)$, enabling an efficient sweep algorithm with $O(\\ell^2 d^3 \\cdot 2^d)$ time and $O(\\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\\times$ speedup over baseline, 85$\\times$ over SHAP-IQ, and 290$\\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01917",
        "abs_url": "https://arxiv.org/abs/2601.01917",
        "pdf_url": "https://arxiv.org/pdf/2601.01917",
        "title": "Distorted Distributional Policy Evaluation for Offline Reinforcement Learning",
        "authors": [
            "Ryo Iwaki",
            "Takayuki Osogami"
        ],
        "comments": "The preprint version of the paper accepted to ICONIP2025. The Version of Record is available online at this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01931",
        "abs_url": "https://arxiv.org/abs/2601.01931",
        "pdf_url": "https://arxiv.org/pdf/2601.01931",
        "title": "DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems",
        "authors": [
            "Willem Röpke",
            "Samuel Coward",
            "Andrei Lupu",
            "Thomas Foster",
            "Tim Rocktäschel",
            "Jakob Foerster"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01943",
        "abs_url": "https://arxiv.org/abs/2601.01943",
        "pdf_url": "https://arxiv.org/pdf/2601.01943",
        "title": "SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling",
        "authors": [
            "Tieu-Long Phan",
            "Nhu-Ngoc Nguyen Song",
            "Peter F. Stadler"
        ],
        "comments": "31 pages (including references), 3 figures, 7 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01979",
        "abs_url": "https://arxiv.org/abs/2601.01979",
        "pdf_url": "https://arxiv.org/pdf/2601.01979",
        "title": "SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition",
        "authors": [
            "Julie Keisler",
            "Anastase Alexandre Charantonis",
            "Yannig Goude",
            "Boutheina Oueslati",
            "Claire Monteleoni"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02022",
        "abs_url": "https://arxiv.org/abs/2601.02022",
        "pdf_url": "https://arxiv.org/pdf/2601.02022",
        "title": "Prior Diffusiveness and Regret in the Linear-Gaussian Bandit",
        "authors": [
            "Yifan Zhu",
            "John C. Duchi",
            "Benjamin Van Roy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We prove that Thompson sampling exhibits $\\tilde{O}(\\sigma d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\\Sigma_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\\mathcal{N}(\\mu_0, \\Sigma_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\\ell_2$ norm of the actions, and $\\sigma^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \\sqrt{\\mathrm{Tr}(\\Sigma_0)}$ decouples additively from the minimax (long run) regret $\\sigma d \\sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02036",
        "abs_url": "https://arxiv.org/abs/2601.02036",
        "pdf_url": "https://arxiv.org/pdf/2601.02036",
        "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models",
        "authors": [
            "Yiyang Wang",
            "Xi Chen",
            "Xiaogang Xu",
            "Yu Liu",
            "Hengshuang Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02037",
        "abs_url": "https://arxiv.org/abs/2601.02037",
        "pdf_url": "https://arxiv.org/pdf/2601.02037",
        "title": "Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling",
        "authors": [
            "Wei Hu",
            "Zewei Yu",
            "Jianqiu Xu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Databases (cs.DB)",
        "abstract": "Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02050",
        "abs_url": "https://arxiv.org/abs/2601.02050",
        "pdf_url": "https://arxiv.org/pdf/2601.02050",
        "title": "Explore the Ideology of Deep Learning in ENSO Forecasts",
        "authors": [
            "Yanhai Gan",
            "Yipeng Chen",
            "Ning Li",
            "Xingguo Liu",
            "Junyu Dong",
            "Xianyao Chen"
        ],
        "comments": "5 figures. Code available at this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02081",
        "abs_url": "https://arxiv.org/abs/2601.02081",
        "pdf_url": "https://arxiv.org/pdf/2601.02081",
        "title": "A Differentiable Adversarial Framework for Task-Aware Data Subsampling",
        "authors": [
            "Jiacheng Lyu",
            "Bihua Bao"
        ],
        "comments": "14 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02094",
        "abs_url": "https://arxiv.org/abs/2601.02094",
        "pdf_url": "https://arxiv.org/pdf/2601.02094",
        "title": "Horizon Activation Mapping for Neural Networks in Time Series Forecasting",
        "authors": [
            "Hans Krupakar",
            "V A Kandappan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Functional Analysis (math.FA)",
        "abstract": "Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02106",
        "abs_url": "https://arxiv.org/abs/2601.02106",
        "pdf_url": "https://arxiv.org/pdf/2601.02106",
        "title": "Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI",
        "authors": [
            "Ashish Rana",
            "Ammar Shaker",
            "Sascha Saralajew",
            "Takashi Suzuki",
            "Kosuke Yasuda",
            "Shintaro Kato",
            "Toshikazu Wada",
            "Toshiyuki Fujikawa",
            "Toru Kikutsuji"
        ],
        "comments": "Accepted to the Demo Track at the IEEE International Conference on Data Mining (ICDM) 2025, where it received the Best Demo Award",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02138",
        "abs_url": "https://arxiv.org/abs/2601.02138",
        "pdf_url": "https://arxiv.org/pdf/2601.02138",
        "title": "Edge-aware GAT-based protein binding site prediction",
        "authors": [
            "Weisen Yang",
            "Hanqing Zhang",
            "Wangren Qiu",
            "Xuan Xiao",
            "Weizhong Lin"
        ],
        "comments": "24 pages, 10 figures, 6 tables",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at this http URL. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02193",
        "abs_url": "https://arxiv.org/abs/2601.02193",
        "pdf_url": "https://arxiv.org/pdf/2601.02193",
        "title": "Learning with Monotone Adversarial Corruptions",
        "authors": [
            "Kasper Green Larsen",
            "Chirag Pabbaraju",
            "Abhishek Shetty"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)",
        "abstract": "We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a \"clean\" i.i.d. dataset, inserts additional \"corrupted\" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02196",
        "abs_url": "https://arxiv.org/abs/2601.02196",
        "pdf_url": "https://arxiv.org/pdf/2601.02196",
        "title": "ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense",
        "authors": [
            "Yu Li",
            "Sizhe Tang",
            "Rongqian Chen",
            "Fei Xu Yu",
            "Guangyu Jiang",
            "Mahdi Imani",
            "Nathaniel D. Bastian",
            "Tian Lan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02213",
        "abs_url": "https://arxiv.org/abs/2601.02213",
        "pdf_url": "https://arxiv.org/pdf/2601.02213",
        "title": "Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction",
        "authors": [
            "Haoyu Zhou",
            "Ping Xue",
            "Tianfan Fu",
            "Hao Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02232",
        "abs_url": "https://arxiv.org/abs/2601.02232",
        "pdf_url": "https://arxiv.org/pdf/2601.02232",
        "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models",
        "authors": [
            "Shristi Das Biswas",
            "Yue Zhang",
            "Anwesan Pal",
            "Radhika Bhargava",
            "Kaushik Roy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02264",
        "abs_url": "https://arxiv.org/abs/2601.02264",
        "pdf_url": "https://arxiv.org/pdf/2601.02264",
        "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network",
        "authors": [
            "Boris Kriuk",
            "Fedor Kriuk"
        ],
        "comments": "8 pages, 14 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at this https URL, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02307",
        "abs_url": "https://arxiv.org/abs/2601.02307",
        "pdf_url": "https://arxiv.org/pdf/2601.02307",
        "title": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck",
        "authors": [
            "Dina El Zein",
            "James Henderson"
        ],
        "comments": "11 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02310",
        "abs_url": "https://arxiv.org/abs/2601.02310",
        "pdf_url": "https://arxiv.org/pdf/2601.02310",
        "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay",
        "authors": [
            "Ahmad Makinde"
        ],
        "comments": "8 pages, 5 figures, Proposes T-KAN architecture for HFT. Achieves 19.1% F1-score improvement on FI-2010 and 132.48% return in cost-adjusted this http URL T-KAN architecture for HFT. Achieves 19.1% F1-score improvement on FI-2010 and 132.48% return in cost-adjusted backtests",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02313",
        "abs_url": "https://arxiv.org/abs/2601.02313",
        "pdf_url": "https://arxiv.org/pdf/2601.02313",
        "title": "Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning",
        "authors": [
            "Hanzaleh Akbari Nodehi",
            "Viveck R. Cadambe",
            "Mohammad Ali Maddah-Ali"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways. In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02360",
        "abs_url": "https://arxiv.org/abs/2601.02360",
        "pdf_url": "https://arxiv.org/pdf/2601.02360",
        "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs",
        "authors": [
            "Yazan Obeidi",
            "Amir Sarfi",
            "Joel Lidin",
            "Paul Janson",
            "Eugene Belilovsky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00805",
        "abs_url": "https://arxiv.org/abs/2601.00805",
        "pdf_url": "https://arxiv.org/pdf/2601.00805",
        "title": "ChronoPlastic Spiking Neural Networks",
        "authors": [
            "Sarim Chaudhry"
        ],
        "comments": "21 pages, 6 figures",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)",
        "abstract": "Spiking neural networks (SNNs) offer a biologically grounded and energy-efficient alternative to conventional neural architectures; however, they struggle with long-range temporal dependencies due to fixed synaptic and membrane time constants. This paper introduces ChronoPlastic Spiking Neural Networks (CPSNNs), a novel architectural principle that enables adaptive temporal credit assignment by dynamically modulating synaptic decay rates conditioned on the state of the network. CPSNNs maintain multiple internal temporal traces and learn a continuous time-warping function that selectively preserves task-relevant information while rapidly forgetting noise. Unlike prior approaches based on adaptive membrane constants, attention mechanisms, or external memory, CPSNNs embed temporal control directly within local synaptic dynamics, preserving linear-time complexity and neuromorphic compatibility. We provide a formal description of the model, analyze its computational properties, and demonstrate empirically that CPSNNs learn long-gap temporal dependencies significantly faster and more reliably than standard SNN baselines. Our results suggest that adaptive temporal modulation is a key missing ingredient for scalable temporal learning in spiking systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00806",
        "abs_url": "https://arxiv.org/abs/2601.00806",
        "pdf_url": "https://arxiv.org/pdf/2601.00806",
        "title": "Energy-Efficient Eimeria Parasite Detection Using a Two-Stage Spiking Neural Network Architecture",
        "authors": [
            "Ángel Miguel García-Vico",
            "Huseyin Seker",
            "Muhammad Afzal"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)",
        "abstract": "Coccidiosis, a disease caused by the Eimeria parasite, represents a major threat to the poultry and rabbit industries, demanding rapid and accurate diagnostic tools. While deep learning models offer high precision, their significant energy consumption limits their deployment in resource-constrained environments. This paper introduces a novel two-stage Spiking Neural Network (SNN) architecture, where a pre-trained Convolutional Neural Network is first converted into a spiking feature extractor and then coupled with a lightweight, unsupervised SNN classifier trained with Spike-Timing-Dependent Plasticity (STDP). The proposed model sets a new state-of-the-art, achieving 98.32\\% accuracy in Eimeria classification. Remarkably, this performance is accomplished with a significant reduction in energy consumption, showing an improvement of more than 223 times compared to its traditional ANN counterpart. This work demonstrates a powerful synergy between high accuracy and extreme energy efficiency, paving the way for autonomous, low-power diagnostic systems on neuromorphic hardware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00816",
        "abs_url": "https://arxiv.org/abs/2601.00816",
        "pdf_url": "https://arxiv.org/pdf/2601.00816",
        "title": "MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback",
        "authors": [
            "Ismail Ahmad Abdullah"
        ],
        "comments": "14 pages, 1 figure, 2 tables, 2 appendices with full proofs. Documents v0.9.4-pilot-audit-hardened audit surface with fail-closed governance, canonical JSON hashing, and artifact classification. Phase I infrastructure validation; no capability claims",
        "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss. Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale. Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00851",
        "abs_url": "https://arxiv.org/abs/2601.00851",
        "pdf_url": "https://arxiv.org/pdf/2601.00851",
        "title": "Autonomous battery research: Principles of heuristic operando experimentation",
        "authors": [
            "Emily Lu",
            "Gabriel Perez",
            "Peter Baker",
            "Daniel Irving",
            "Santosh Kumar",
            "Veronica Celorrio",
            "Sylvia Britto",
            "Thomas F. Headen",
            "Miguel Gomez-Gonzalez",
            "Connor Wright",
            "Calum Green",
            "Robert Scott Young",
            "Oleg Kirichek",
            "Ali Mortazavi",
            "Sarah Day",
            "Isabel Antony",
            "Zoe Wright",
            "Thomas Wood",
            "Tim Snow",
            "Jeyan Thiyagalingam",
            "Paul Quinn",
            "Martin Owen Jones",
            "William David",
            "James Le Houx"
        ],
        "comments": "38 pages, 14 figures. Includes a detailed technical review of the POLARIS, BAM, DRIX, M-Series, and B18 electrochemical cells in the Supplementary Information",
        "subjects": "Instrumentation and Detectors (physics.ins-det); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Unravelling the complex processes governing battery degradation is critical to the energy transition, yet the efficacy of operando characterisation is severely constrained by a lack of Reliability, Representativeness, and Reproducibility (the 3Rs). Current methods rely on bespoke hardware and passive, pre-programmed methodologies that are ill-equipped to capture stochastic failure events. Here, using the Rutherford Appleton Laboratory's multi-modal toolkit as a case study, we expose the systemic inability of conventional experiments to capture transient phenomena like dendrite initiation. To address this, we propose Heuristic Operando experiments: a framework where an AI pilot leverages physics-based digital twins to actively steer the beamline to predict and deterministically capture these rare events. Distinct from uncertainty-driven active learning, this proactive search anticipates failure precursors, redefining experimental efficiency via an entropy-based metric that prioritises scientific insight per photon, neutron, or muon. By focusing measurements only on mechanistically decisive moments, this framework simultaneously mitigates beam damage and drastically reduces data redundancy. When integrated with FAIR data principles, this approach serves as a blueprint for the trusted autonomous battery laboratories of the future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00855",
        "abs_url": "https://arxiv.org/abs/2601.00855",
        "pdf_url": "https://arxiv.org/pdf/2601.00855",
        "title": "Physically-Constrained Autoencoder-Assisted Bayesian Optimization for Refinement of High-Dimensional Defect-Sensitive Single Crystalline Structure",
        "authors": [
            "Joseph Oche Agada",
            "Andrew McAninch",
            "Haley Day",
            "Yasemin Tanyu",
            "Ewan McCombs",
            "Seyed M. Koohpayeh",
            "Brian H. Toby",
            "Yishu Wang",
            "Arpan Biswas"
        ],
        "comments": "15 pages, 8 figures",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Physical properties and functionalities of materials are dictated by global crystal structures as well as local defects. To establish a structure-property relationship, not only the crystallographic symmetry but also quantitative knowledge about defects are required. Here we present a hybrid Machine Learning framework that integrates a physically-constrained variational-autoencoder (pcVAE) with different Bayesian Optimization (BO) methods to systematically accelerate and improve crystal structure refinement with resolution of defects. We chose the pyrochlore structured Ho2Ti2O7 as a model system and employed the GSAS2 package for benchmarking crystallographic parameters from Rietveld refinement. However, the function space of these material systems is highly nonlinear, which limits optimizers like traditional Rietveld refinement, into trapping at local minima. Also, these naive methods don't provide an extensive learning about the overall function space, which is essential for large space, large time consuming explorations to identify various potential regions of interest. Thus, we present the approach of exploring the high Dimensional structure parameters of defect sensitive systems via pretrained pcVAE assisted BO and Sparse Axis Aligned BO. The pcVAE projects high-Dimensional diffraction data consisting of thousands of independently measured diffraction orders into a lowD latent space while enforcing scaling invariance and physical relevance. Then via BO methods, we aim to minimize the L2 norm based chisq errors in the real and latent spaces separately between experimental and simulated diffraction patterns, thereby steering the refinement towards potential optimum crystal structure parameters. We investigated and compared the results among different pcVAE assisted BO, non pcVAE assisted BO, and Rietveld refinement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00871",
        "abs_url": "https://arxiv.org/abs/2601.00871",
        "pdf_url": "https://arxiv.org/pdf/2601.00871",
        "title": "Deep versus Broad Technology Search and the Timing of Innovation Impact",
        "authors": [
            "Likun Cao",
            "James Evans"
        ],
        "comments": "47 pages, 8 figures, 3 tables",
        "subjects": "Physics and Society (physics.soc-ph); Digital Libraries (cs.DL); Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "This study offers a new perspective on the depth-versus-breadth debate in innovation strategy, by modeling inventive search within dynamic collective knowledge systems, and underscoring the importance of timing for technological impact. Using frontier machine learning to project patent citation networks in hyperbolic space, we analyze 4.9 million U.S. patents to examine how search strategies give rise to distinct temporal patterns in impact accumulation. We find that inventions based on deep search, which relies on a specialized understanding of complex recombination structures, drive higher short-term impact through early adoption within specialized communities, but face diminishing returns as innovations become \"locked-in\" with limited diffusion potential. Conversely, when inventions are grounded in broad search that spans disparate domains, they encounter initial resistance but achieve wider diffusion and greater long-term impact by reaching cognitively diverse audiences. Individual inventions require both depth and breadth for stable impact. Organizations can strategically balance approaches across multiple inventions: using depth to build reliable technological infrastructure while pursuing breadth to expand applications. We advance innovation theory by demonstrating how deep and broad search strategies distinctly shape the timing and trajectory of technological impact, and how individual inventors and organizations can leverage these mechanisms to balance exploitation and exploration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00893",
        "abs_url": "https://arxiv.org/abs/2601.00893",
        "pdf_url": "https://arxiv.org/pdf/2601.00893",
        "title": "Towards eco friendly cybersecurity: machine learning based anomaly detection with carbon and energy metrics",
        "authors": [
            "KC Aashish",
            "Md Zakir Hossain Zamil",
            "Md Shafiqul Islam Mridul",
            "Lamia Akter",
            "Farmina Sharmin",
            "Eftekhar Hossain Ayon",
            "Md Maruf Bin Reza",
            "Ali Hassan",
            "Abdur Rahim",
            "Sirapa Malla"
        ],
        "comments": "International Journal of Applied Mathematics 2025",
        "subjects": "Cryptography and Security (cs.CR); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "The rising energy footprint of artificial intelligence has become a measurable component of US data center emissions, yet cybersecurity research seldom considers its environmental cost. This study introduces an eco aware anomaly detection framework that unifies machine learning based network monitoring with real time carbon and energy tracking. Using the publicly available Carbon Aware Cybersecurity Traffic Dataset comprising 2300 flow level observations, we benchmark Logistic Regression, Random Forest, Support Vector Machine, Isolation Forest, and XGBoost models across energy, carbon, and performance dimensions. Each experiment is executed in a controlled Colab environment instrumented with the CodeCarbon toolkit to quantify power draw and equivalent CO2 output during both training and inference. We construct an Eco Efficiency Index that expresses F1 score per kilowatt hour to capture the trade off between detection quality and environmental impact. Results reveal that optimized Random Forest and lightweight Logistic Regression models achieve the highest eco efficiency, reducing energy consumption by more than forty percent compared to XGBoost while sustaining competitive detection accuracy. Principal Component Analysis further decreases computational load with negligible loss in recall. Collectively, these findings establish that integrating carbon and energy metrics into cybersecurity workflows enables environmentally responsible machine learning without compromising operational protection. The proposed framework offers a reproducible path toward sustainable carbon accountable cybersecurity aligned with emerging US green computing and federal energy efficiency initiatives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00895",
        "abs_url": "https://arxiv.org/abs/2601.00895",
        "pdf_url": "https://arxiv.org/pdf/2601.00895",
        "title": "Deep Learning Framework for RNA Inverse Folding with Geometric Structure Potentials",
        "authors": [
            "Annabelle Yao"
        ],
        "comments": "",
        "subjects": "Quantitative Methods (q-bio.QM); Machine Learning (cs.LG)",
        "abstract": "RNA's diverse biological functions stem from its structural versatility, yet accurately predicting and designing RNA sequences given a 3D conformation (inverse folding) remains a challenge. Here, I introduce a deep learning framework that integrates Geometric Vector Perceptron (GVP) layers with a Transformer architecture to enable end-to-end RNA design. I construct a dataset consisting of experimentally solved RNA 3D structures, filtered and deduplicated from the BGSU RNA list, and evaluate performance using both sequence recovery rate and TM-score to assess sequence and structural fidelity, respectively. On standard benchmarks and RNA-Puzzles, my model achieves state-of-the-art performance, with recovery and TM-scores of 0.481 and 0.332, surpassing existing methods across diverse RNA families and length scales. Masked family-level validation using Rfam annotations confirms strong generalization beyond seen families. Furthermore, inverse-folded sequences, when refolded using AlphaFold3, closely resemble native structures, highlighting the critical role of geometric features captured by GVP layers in enhancing Transformer-based RNA design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00896",
        "abs_url": "https://arxiv.org/abs/2601.00896",
        "pdf_url": "https://arxiv.org/pdf/2601.00896",
        "title": "Investigation into U.S. Citizen and Non-Citizen Worker Health Insurance and Employment",
        "authors": [
            "Annabelle Yao"
        ],
        "comments": "",
        "subjects": "General Economics (econ.GN); Machine Learning (cs.LG)",
        "abstract": "Socioeconomic integration is a critical dimension of social equity, yet persistent disparities remain in access to health insurance, education, and employment across different demographic groups. While previous studies have examined isolated aspects of inequality, there is limited research that integrates both statistical analysis and advanced machine learning to uncover hidden structures within population data. This study leverages statistical analysis ($\\chi^2$ test of independence and Two Proportion Z-Test) and machine learning clustering techniques -- K-Modes and K-Prototypes -- along with t-SNE visualization and CatBoost classification to analyze socioeconomic integration and inequality. Using statistical tests, we identified the proportion of the population with healthcare insurance, quality education, and employment. With this data, we concluded that there was an association between employment and citizenship status. Moreover, we were able to determine 5 distinct population groups using Machine Learning classification. The five clusters our analysis identifies reveal that while citizenship status shows no association with workforce participation, significant disparities exist in access to employer-sponsored health insurance. Each cluster represents a distinct demographic of the population, showing that there is a primary split along the lines of educational attainment which separates Clusters 0 and 4 from Clusters 1, 2, and 3. Furthermore, labor force status and nativity serve as secondary differentiators. Non-citizens are also disproportionately concentrated in precarious employment without benefits, highlighting systemic inequalities in healthcare access. By uncovering demographic clusters that face compounded disadvantages, this research contributes to a more nuanced understanding of socioeconomic stratification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00904",
        "abs_url": "https://arxiv.org/abs/2601.00904",
        "pdf_url": "https://arxiv.org/pdf/2601.00904",
        "title": "Deep Deterministic Nonlinear ICA via Total Correlation Minimization with Matrix-Based Entropy Functional",
        "authors": [
            "Qiang Li",
            "Shujian Yu",
            "Liang Ma",
            "Chen Ma",
            "Jingyu Liu",
            "Tulay Adali",
            "Vince D. Calhoun"
        ],
        "comments": "16 pages, 9 figures",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Blind source separation, particularly through independent component analysis (ICA), is widely utilized across various signal processing domains for disentangling underlying components from observed mixed signals, owing to its fully data-driven nature that minimizes reliance on prior assumptions. However, conventional ICA methods rely on an assumption of linear mixing, limiting their ability to capture complex nonlinear relationships and to maintain robustness in noisy environments. In this work, we present deep deterministic nonlinear independent component analysis (DDICA), a novel deep neural network-based framework designed to address these limitations. DDICA leverages a matrix-based entropy function to directly optimize the independence criterion via stochastic gradient descent, bypassing the need for variational approximations or adversarial schemes. This results in a streamlined training process and improved resilience to noise. We validated the effectiveness and generalizability of DDICA across a range of applications, including simulated signal mixtures, hyperspectral image unmixing, modeling of primary visual receptive fields, and resting-state functional magnetic resonance imaging (fMRI) data analysis. Experimental results demonstrate that DDICA effectively separates independent components with high accuracy across a range of applications. These findings suggest that DDICA offers a robust and versatile solution for blind source separation in diverse signal processing tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00909",
        "abs_url": "https://arxiv.org/abs/2601.00909",
        "pdf_url": "https://arxiv.org/pdf/2601.00909",
        "title": "Security Hardening Using FABRIC: Implementing a Unified Compliance Aggregator for Linux Servers",
        "authors": [
            "Sheldon Paul",
            "Izzat Alsmadi"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "This paper presents a unified framework for evaluating Linux security hardening on the FABRIC testbed through aggregation of heterogeneous security auditing tools. We deploy three Ubuntu 22.04 nodes configured at baseline, partial, and full hardening levels, and evaluate them using Lynis, OpenSCAP, and AIDE across 108 audit runs. To address the lack of a consistent interpretation across tools, we implement a Unified Compliance Aggregator (UCA) that parses tool outputs, normalizes scores to a common 0--100 scale, and combines them into a weighted metric augmented by a customizable rule engine for organization-specific security policies. Experimental results show that full hardening increases OpenSCAP compliance from 39.7 to 71.8, while custom rule compliance improves from 39.3\\% to 83.6\\%. The results demonstrate that UCA provides a clearer and more reproducible assessment of security posture than individual tools alone, enabling systematic evaluation of hardening effectiveness in programmable testbed environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00913",
        "abs_url": "https://arxiv.org/abs/2601.00913",
        "pdf_url": "https://arxiv.org/pdf/2601.00913",
        "title": "Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting",
        "authors": [
            "Subhankar Mishra"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.00999",
        "abs_url": "https://arxiv.org/abs/2601.00999",
        "pdf_url": "https://arxiv.org/pdf/2601.00999",
        "title": "Dynamic Accuracy Estimation in a Wi-Fi-based Positioning System",
        "authors": [
            "Marcin Kolakowski",
            "Vitomir Djaja-Josko"
        ],
        "comments": "Originally presented at 2025 33rd Telecommunications Forum (TELFOR), Belgrade, Serbia",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "The paper presents a concept of a dynamic accuracy estimation method, in which the localization errors are derived based on the measurement results used by the positioning algorithm. The concept was verified experimentally in a Wi\\nobreakdash-Fi based indoor positioning system, where several regression methods were tested (linear regression, random forest, k-nearest neighbors, and neural networks). The highest positioning error estimation accuracy was achieved for random forest regression, with a mean absolute error of 0.72 m.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01026",
        "abs_url": "https://arxiv.org/abs/2601.01026",
        "pdf_url": "https://arxiv.org/pdf/2601.01026",
        "title": "Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation",
        "authors": [
            "Douglas Costa Braga",
            "Daniel Oliveira Dantas"
        ],
        "comments": "9 pages, 5 figures, 4 tables. Submitted to VISAPP 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01044",
        "abs_url": "https://arxiv.org/abs/2601.01044",
        "pdf_url": "https://arxiv.org/pdf/2601.01044",
        "title": "Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data",
        "authors": [
            "Jin Wang",
            "Angelo De Castro",
            "Yuxi Zhang",
            "Lucas Basolli Borsatto",
            "Yuechen Guo",
            "Victoria Bastos Primo",
            "Ana Beatriz Montevecchio Bernardino",
            "Gota Morota",
            "Ricardo C Chebel",
            "Haipeng Yu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01053",
        "abs_url": "https://arxiv.org/abs/2601.01053",
        "pdf_url": "https://arxiv.org/pdf/2601.01053",
        "title": "Byzantine-Robust Federated Learning Framework with Post-Quantum Secure Aggregation for Real-Time Threat Intelligence Sharing in Critical IoT Infrastructure",
        "authors": [
            "Milad Rahmati",
            "Nima Rahmati"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "The proliferation of Internet of Things devices in critical infrastructure has created unprecedented cybersecurity challenges, necessitating collaborative threat detection mechanisms that preserve data privacy while maintaining robustness against sophisticated attacks. Traditional federated learning approaches for IoT security suffer from two critical vulnerabilities: susceptibility to Byzantine attacks where malicious participants poison model updates, and inadequacy against future quantum computing threats that can compromise cryptographic aggregation protocols. This paper presents a novel Byzantine-robust federated learning framework integrated with post-quantum secure aggregation specifically designed for real-time threat intelligence sharing across critical IoT infrastructure. The proposed framework combines a adaptive weighted aggregation mechanism with lattice-based cryptographic protocols to simultaneously defend against model poisoning attacks and quantum adversaries. We introduce a reputation-based client selection algorithm that dynamically identifies and excludes Byzantine participants while maintaining differential privacy guarantees. The secure aggregation protocol employs CRYSTALS-Kyber for key encapsulation and homomorphic encryption to ensure confidentiality during parameter updates. Experimental evaluation on industrial IoT intrusion detection datasets demonstrates that our framework achieves 96.8% threat detection accuracy while successfully mitigating up to 40% Byzantine attackers, with only 18% computational overhead compared to non-secure federated approaches. The framework maintains sub-second aggregation latency suitable for real-time applications and provides 256-bit post-quantum security level.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01055",
        "abs_url": "https://arxiv.org/abs/2601.01055",
        "pdf_url": "https://arxiv.org/pdf/2601.01055",
        "title": "Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics",
        "authors": [
            "Ernest Fokoué"
        ],
        "comments": "19 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01076",
        "abs_url": "https://arxiv.org/abs/2601.01076",
        "pdf_url": "https://arxiv.org/pdf/2601.01076",
        "title": "Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees",
        "authors": [
            "Devesh Nath",
            "Haoran Yin",
            "Glen Chou"
        ],
        "comments": "Under review, 28 pages, 12 figures",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO); Optimization and Control (math.OC)",
        "abstract": "We propose a scalable reachability-based framework for probabilistic, data-driven safety verification of unknown nonlinear dynamics. We use Koopman theory with a neural network (NN) lifting function to learn an approximate linear representation of the dynamics and design linear controllers in this space to enable closed-loop tracking of a reference trajectory distribution. Closed-loop reachable sets are efficiently computed in the lifted space and mapped back to the original state space via NN verification tools. To capture model mismatch between the Koopman dynamics and the true system, we apply conformal prediction to produce statistically-valid error bounds that inflate the reachable sets to ensure the true trajectories are contained with a user-specified probability. These bounds generalize across references, enabling reuse without recomputation. Results on high-dimensional MuJoCo tasks (11D Hopper, 28D Swimmer) and 12D quadcopters show improved reachable set coverage rate, computational efficiency, and conservativeness over existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01097",
        "abs_url": "https://arxiv.org/abs/2601.01097",
        "pdf_url": "https://arxiv.org/pdf/2601.01097",
        "title": "Neural Networks on Symmetric Spaces of Noncompact Type",
        "authors": [
            "Xuan Son Nguyen",
            "Shuo Yang",
            "Aymeric Histace"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01132",
        "abs_url": "https://arxiv.org/abs/2601.01132",
        "pdf_url": "https://arxiv.org/pdf/2601.01132",
        "title": "Generating Diverse TSP Tours via a Combination of Graph Pointer Network and Dispersion",
        "authors": [
            "Hao-Hsung Yang",
            "Ssu-Yuan Lo",
            "Kuan-Lun Chen",
            "Ching-Kai Wang"
        ],
        "comments": "",
        "subjects": "Computational Geometry (cs.CG); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We address the Diverse Traveling Salesman Problem (D-TSP), a bi-criteria optimization challenge that seeks a set of $k$ distinct TSP tours. The objective requires every selected tour to have a length at most $c|T^*|$ (where $|T^*|$ is the optimal tour length) while minimizing the average Jaccard similarity across all tour pairs. This formulation is crucial for applications requiring both high solution quality and fault tolerance, such as logistics planning, robotics pathfinding or strategic patrolling. Current methods are limited: traditional heuristics, such as the Niching Memetic Algorithm (NMA) or bi-criteria optimization, incur high computational complexity $O(n^3)$, while modern neural approaches (e.g., RF-MA3S) achieve limited diversity quality and rely on complex, external mechanisms. To overcome these limitations, we propose a novel hybrid framework that decomposes D-TSP into two efficient steps. First, we utilize a simple Graph Pointer Network (GPN), augmented with an approximated sequence entropy loss, to efficiently sample a large, diverse pool of high-quality tours. This simple modification effectively controls the quality-diversity trade-off without complex external mechanisms. Second, we apply a greedy algorithm that yields a 2-approximation for the dispersion problem to select the final $k$ maximally diverse tours from the generated pool. Our results demonstrate state-of-the-art performance. On the Berlin instance, our model achieves an average Jaccard index of $0.015$, significantly outperforming NMA ($0.081$) and RF-MA3S. By leveraging GPU acceleration, our GPN structure achieves a near-linear empirical runtime growth of $O(n)$. While maintaining solution diversity comparable to complex bi-criteria algorithms, our approach is over 360 times faster on large-scale instances (783 cities), delivering high-quality TSP solutions with unprecedented efficiency and simplicity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01147",
        "abs_url": "https://arxiv.org/abs/2601.01147",
        "pdf_url": "https://arxiv.org/pdf/2601.01147",
        "title": "Conformal Blindness: A Note on $A$-Cryptic change-points",
        "authors": [
            "Johan Hallberg Szabadváry"
        ],
        "comments": "6 pages, 3 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \\emph{conformal blindness}. Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \\emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values. Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01160",
        "abs_url": "https://arxiv.org/abs/2601.01160",
        "pdf_url": "https://arxiv.org/pdf/2601.01160",
        "title": "Gradient-Free Approaches is a Key to an Efficient Interaction with Markovian Stochasticity",
        "authors": [
            "Boris Prokhorov",
            "Semyon Chebykin",
            "Alexander Gasnikov",
            "Aleksandr Beznosikov"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "This paper deals with stochastic optimization problems involving Markovian noise with a zero-order oracle. We present and analyze a novel derivative-free method for solving such problems in strongly convex smooth and non-smooth settings with both one-point and two-point feedback oracles. Using a randomized batching scheme, we show that when mixing time $\\tau$ of the underlying noise sequence is less than the dimension of the problem $d$, the convergence estimates of our method do not depend on $\\tau$. This observation provides an efficient way to interact with Markovian stochasticity: instead of invoking the expensive first-order oracle, one should use the zero-order oracle. Finally, we complement our upper bounds with the corresponding lower bounds. This confirms the optimality of our results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01229",
        "abs_url": "https://arxiv.org/abs/2601.01229",
        "pdf_url": "https://arxiv.org/pdf/2601.01229",
        "title": "NeuroSSM: Multiscale Differential State-Space Modeling for Context-Aware fMRI Analysis",
        "authors": [
            "Furkan Genç",
            "Boran İsmet Macun",
            "Sait Sarper Özaslan",
            "Emine U. Saritas",
            "Tolga Çukur"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "Accurate fMRI analysis requires sensitivity to temporal structure across multiple scales, as BOLD signals encode cognitive processes that emerge from fast transient dynamics to slower, large-scale fluctuations. Existing deep learning (DL) approaches to temporal modeling face challenges in jointly capturing these dynamics over long fMRI time series. Among current DL models, transformers address long-range dependencies by explicitly modeling pairwise interactions through attention, but the associated quadratic computational cost limits effective integration of temporal dependencies across long fMRI sequences. Selective state-space models (SSMs) instead model long-range temporal dependencies implicitly through latent state evolution in a dynamical system, enabling efficient propagation of dependencies over time. However, recent SSM-based approaches for fMRI commonly operate on derived functional connectivity representations and employ single-scale temporal processing. These design choices constrain the ability to jointly represent fast transient dynamics and slower global trends within a single model. We propose NeuroSSM, a selective state-space architecture designed for end-to-end analysis of raw BOLD signals in fMRI time series. NeuroSSM addresses the above limitations through two complementary design components: a multiscale state-space backbone that captures fast and slow dynamics concurrently, and a parallel differencing branch that increases sensitivity to transient state changes. Experiments on clinical and non-clinical datasets demonstrate that NeuroSSM achieves competitive performance and efficiency against state-of-the-art fMRI analysis methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01238",
        "abs_url": "https://arxiv.org/abs/2601.01238",
        "pdf_url": "https://arxiv.org/pdf/2601.01238",
        "title": "Evidence Slopes and Effective Dimension in Singular Linear Models",
        "authors": [
            "Kalyaan Rao"
        ],
        "comments": "Preprint. 10 pages, 6 figures. Under review",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models. We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace. Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01248",
        "abs_url": "https://arxiv.org/abs/2601.01248",
        "pdf_url": "https://arxiv.org/pdf/2601.01248",
        "title": "Stochastic Control Methods for Optimization",
        "authors": [
            "Jinniao Qiu"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Numerical Analysis (math.NA); Probability (math.PR)",
        "abstract": "In this work, we investigate a stochastic control framework for global optimization over both finite-dimensional Euclidean spaces and the Wasserstein space of probability measures. In the Euclidean setting, the original minimization problem is approximated by a family of regularized stochastic control problems; using dynamic programming, we analyze the associated Hamilton--Jacobi--Bellman equations and obtain tractable representations via the Cole--Hopf transform and the Feynman--Kac formula. For optimization over probability measures, we formulate a regularized mean-field control problem characterized by a master equation, and further approximate it by controlled $N$-particle systems. We establish that, as the regularization parameter tends to zero (and as the particle number tends to infinity for the optimization over probability measures), the value of the control problem converges to the global minimum of the original objective. Building on the resulting probabilistic representations, Monte Carlo-based numerical schemes are proposed and numerical experiments are reported to illustrate the practical performance of the methods and to support the theoretical convergence rates.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01296",
        "abs_url": "https://arxiv.org/abs/2601.01296",
        "pdf_url": "https://arxiv.org/pdf/2601.01296",
        "title": "Aggressive Compression Enables LLM Weight Theft",
        "authors": [
            "Davis Brown",
            "Juan-Pablo Rivera",
            "Dan Hendrycks",
            "Mantas Mazeika"
        ],
        "comments": "An early version of this work was presented at the SoLAR Workshop at NeurIPS 2024",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "As frontier AIs become more powerful and costly to develop, adversaries have increasing incentives to steal model weights by mounting exfiltration attacks. In this work, we consider exfiltration attacks where an adversary attempts to sneak model weights out of a datacenter over a network. While exfiltration attacks are multi-step cyber attacks, we demonstrate that a single factor, the compressibility of model weights, significantly heightens exfiltration risk for large language models (LLMs). We tailor compression specifically for exfiltration by relaxing decompression constraints and demonstrate that attackers could achieve 16x to 100x compression with minimal trade-offs, reducing the time it would take for an attacker to illicitly transmit model weights from the defender's server from months to days. Finally, we study defenses designed to reduce exfiltration risk in three distinct ways: making models harder to compress, making them harder to 'find,' and tracking provenance for post-attack analysis using forensic watermarks. While all defenses are promising, the forensic watermark defense is both effective and cheap, and therefore is a particularly attractive lever for mitigating weight-exfiltration risk.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01310",
        "abs_url": "https://arxiv.org/abs/2601.01310",
        "pdf_url": "https://arxiv.org/pdf/2601.01310",
        "title": "Making MoE based LLM inference resilient with Tarragon",
        "authors": [
            "Songyu Zhang",
            "Aaron Tam",
            "Myungjin Lee",
            "Shixiong Qi",
            "K. K. Ramakrishnan"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services. We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01311",
        "abs_url": "https://arxiv.org/abs/2601.01311",
        "pdf_url": "https://arxiv.org/pdf/2601.01311",
        "title": "Concave Certificates: Geometric Framework for Distributionally Robust Risk and Complexity Analysis",
        "authors": [
            "Hong T.M. Chu"
        ],
        "comments": "30 pages, 7 figures",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Distributionally Robust (DR) optimization aims to certify worst-case risk within a Wasserstein uncertainty set. Current certifications typically rely either on global Lipschitz bounds, which are often conservative, or on local gradient information, which provides only a first-order approximation. This paper introduces a novel geometric framework based on the least concave majorants of the growth rate function. Our proposed concave certificate establishes a tight bound of DR risk that remains applicable to non-Lipschitz and non-differentiable losses. We extend this framework to complexity analysis, introducing a deterministic bound that complements standard statistical generalization bound. Furthermore, we utilize this certificate to bound the gap between adversarial and empirical Rademacher complexity, demonstrating that dependencies on input diameter, network width, and depth can be eliminated. For practical application in deep learning, we introduce the adversarial score as a tractable relaxation of the concave certificate that enables efficient and layer-wise analysis of neural networks. We validate our theoretical results in various numerical experiments on classification and regression tasks on real-world data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01331",
        "abs_url": "https://arxiv.org/abs/2601.01331",
        "pdf_url": "https://arxiv.org/pdf/2601.01331",
        "title": "AppellateGen: A Benchmark for Appellate Legal Judgment Generation",
        "authors": [
            "Hongkun Yang",
            "Lionel Z. Wang",
            "Wei Fan",
            "Yiran Hu",
            "Lixu Wang",
            "Chenyu Liu",
            "Shenghong Fu",
            "Haoyang Li",
            "Xin Xu",
            "Jiexin Zheng",
            "Wei Dong"
        ],
        "comments": "15 pages, 4 figures, 3 tables",
        "subjects": "Computers and Society (cs.CY); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Legal judgment generation is a critical task in legal intelligence. However, existing research in legal judgment generation has predominantly focused on first-instance trials, relying on static fact-to-verdict mappings while neglecting the dialectical nature of appellate (second-instance) review. To address this, we introduce AppellateGen, a benchmark for second-instance legal judgment generation comprising 7,351 case pairs. The task requires models to draft legally binding judgments by reasoning over the initial verdict and evidentiary updates, thereby modeling the causal dependency between trial stages. We further propose a judicial Standard Operating Procedure (SOP)-based Legal Multi-Agent System (SLMAS) to simulate judicial workflows, which decomposes the generation process into discrete stages of issue identification, retrieval, and drafting. Experimental results indicate that while SLMAS improves logical consistency, the complexity of appellate reasoning remains a substantial challenge for current LLMs. The dataset and code are publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01332",
        "abs_url": "https://arxiv.org/abs/2601.01332",
        "pdf_url": "https://arxiv.org/pdf/2601.01332",
        "title": "FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness",
        "authors": [
            "Hossam Amer",
            "Maryam Dialameh",
            "Hossein Rajabzadeh",
            "Walid Ahmed",
            "Weiwei Zhang",
            "Yang Liu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01358",
        "abs_url": "https://arxiv.org/abs/2601.01358",
        "pdf_url": "https://arxiv.org/pdf/2601.01358",
        "title": "A New Framework for Explainable Rare Cell Identification in Single-Cell Transcriptomics Data",
        "authors": [
            "Di Su",
            "Kai Ming Ting",
            "Jie Zhang",
            "Xiaorui Zhang",
            "Xinpeng Li"
        ],
        "comments": "",
        "subjects": "Genomics (q-bio.GN); Machine Learning (cs.LG)",
        "abstract": "The detection of rare cell types in single-cell transcriptomics data is crucial for elucidating disease pathogenesis and tissue development dynamics. However, a critical gap that persists in current methods is their inability to provide an explanation based on genes for each cell they have detected as rare. We identify three primary sources of this deficiency. First, the anomaly detectors often function as \"black boxes\", designed to detect anomalies but unable to explain why a cell is anomalous. Second, the standard analytical framework hinders interpretability by relying on dimensionality reduction techniques, such as Principal Component Analysis (PCA), which transform meaningful gene expression data into abstract, uninterpretable features. Finally, existing explanation algorithms cannot be readily applied to this domain, as single-cell data is characterized by high dimensionality, noise, and substantial sparsity. To overcome these limitations, we introduce a framework for explainable anomaly detection in single-cell transcriptomics data which not only identifies individual anomalies, but also provides a visual explanation based on genes that makes an instance anomalous. This framework has two key ingredients that are not existed in current methods applied in this domain. First, it eliminates the PCA step which is deemed to be an essential component in previous studies. Second, it employs the state-of-art anomaly detector and explainer as the efficient and effective means to find each rare cell and the relevant gene subspace in order to provide explanations for each rare cell as well as the typical normal cell associated with the rare cell's closest normal cells.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01362",
        "abs_url": "https://arxiv.org/abs/2601.01362",
        "pdf_url": "https://arxiv.org/pdf/2601.01362",
        "title": "Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning",
        "authors": [
            "Jerry Huang",
            "Peng Lu",
            "Qiuhao Zeng",
            "Yusuke Iwasawa",
            "Yutaka Matsuo",
            "Sarath Chandar",
            "Edison Marrese-Taylor",
            "Irene Li"
        ],
        "comments": "Accepted to The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL)",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01371",
        "abs_url": "https://arxiv.org/abs/2601.01371",
        "pdf_url": "https://arxiv.org/pdf/2601.01371",
        "title": "SGD with Dependent Data: Optimal Estimation, Regret, and Inference",
        "authors": [
            "Yinan Shen",
            "Yichen Zhang",
            "Wen-Xin Zhou"
        ],
        "comments": "",
        "subjects": "Statistics Theory (math.ST); Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)",
        "abstract": "This work investigates the performance of the final iterate produced by stochastic gradient descent (SGD) under temporally dependent data. We consider two complementary sources of dependence: $(i)$ martingale-type dependence in both the covariate and noise processes, which accommodates non-stationary and non-mixing time series data, and $(ii)$ dependence induced by sequential decision making. Our formulation runs in parallel with classical notions of (local) stationarity and strong mixing, while neither framework fully subsumes the other. Remarkably, SGD is shown to automatically accommodate both independent and dependent information under a broad class of stepsize schedules and exploration rate schemes. Non-asymptotically, we show that SGD simultaneously achieves statistically optimal estimation error and regret, extending and improving existing results. In particular, our tail bounds remain sharp even for potentially infinite horizon $T=+\\infty$. Asymptotically, the SGD iterates converge to a Gaussian distribution with only an $O_{\\PP}(1/\\sqrt{t})$ remainder, demonstrating that the supposed estimation-regret trade-off claimed in prior work can in fact be avoided. We further propose a new ``conic'' approximation of the decision region that allows the covariates to have unbounded support. For online sparse regression, we develop a new SGD-based algorithm that uses only $d$ units of storage and requires $O(d)$ flops per iteration, achieving the long term statistical optimality. Intuitively, each incoming observation contributes to estimation accuracy, while aggregated summary statistics guide support recovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01391",
        "abs_url": "https://arxiv.org/abs/2601.01391",
        "pdf_url": "https://arxiv.org/pdf/2601.01391",
        "title": "Bayesian Negative Binomial Regression of Afrobeats Chart Persistence",
        "authors": [
            "Ian Jacob Cabansag",
            "Paul Ntegeka"
        ],
        "comments": "",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Afrobeats songs compete for attention on streaming platforms, where chart visibility can influence both revenue and cultural impact. This paper examines whether collaborations help songs remain on the charts longer, using daily Nigeria Spotify Top 200 data from 2024. Each track is summarized by the number of days it appears in the Top 200 during the year and its total annual streams in Nigeria. A Bayesian negative binomial regression is applied, with days on chart as the outcome and collaboration status (solo versus multi-artist) and log total streams as predictors. This approach is well suited for overdispersed count data and allows the effect of collaboration to be interpreted while controlling for overall popularity. Posterior inference is conducted using Markov chain Monte Carlo, and results are assessed using rate ratios, posterior probabilities, and predictive checks. The findings indicate that, after accounting for total streams, collaboration tracks tend to spend slightly fewer days on the chart than comparable solo tracks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01401",
        "abs_url": "https://arxiv.org/abs/2601.01401",
        "pdf_url": "https://arxiv.org/pdf/2601.01401",
        "title": "LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs",
        "authors": [
            "Chenxu Wang",
            "Chaozhuo Li",
            "Pengbo Wang",
            "Litian Zhang",
            "Songyang Liu",
            "Ji Qi",
            "Jiahui Hu",
            "Yushan Cai",
            "Hao Zhao",
            "Rui Pu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01405",
        "abs_url": "https://arxiv.org/abs/2601.01405",
        "pdf_url": "https://arxiv.org/pdf/2601.01405",
        "title": "Efficient Cover Construction for Ball Mapper via Accelerated Range Queries",
        "authors": [
            "Jay-Anne Bulauan",
            "John Rick Manzanares"
        ],
        "comments": "",
        "subjects": "Computational Geometry (cs.CG); Machine Learning (cs.LG)",
        "abstract": "Ball Mapper is an widely used tool in topological data analysis for summarizing the structure of high-dimensional data through metric-based coverings and graph representations. A central computational bottleneck in Ball Mapper is the construction of the underlying cover, which requires repeated range queries to identify data points within a fixed distance of selected landmarks. As data sets grow in size and dimensionality, naive implementations of this step become increasingly inefficient. In this work, we study practical strategies for accelerating cover construction in Ball Mapper by improving the efficiency of range queries. We integrate two complementary approaches into the Ball Mapper pipeline: hierarchical geometric pruning using ball tree data structures, and hardware-aware distance computation using Facebook AI Similarity Search. We describe the underlying algorithms, discuss their trade-offs with respect to metric flexibility and dimensionality, and provide implementation details relevant to large-scale data analysis. Empirical benchmarks demonstrate that both approaches yield substantial speedups over the baseline implementation, with performance gains depending on data set size, dimensionality, and choice of distance function. These results improve the practical scalability of Ball Mapper without modifying its theoretical formulation and provide guidance for the efficient implementation of metric-based exploratory tools in modern data analysis workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01410",
        "abs_url": "https://arxiv.org/abs/2601.01410",
        "pdf_url": "https://arxiv.org/pdf/2601.01410",
        "title": "Reliable Grid Forecasting: State Space Models for Safety-Critical Energy Systems",
        "authors": [
            "Jisoo Lee",
            "Sunki Hong"
        ],
        "comments": "24 pages, 8 figures, 8 tables",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Accurate grid load forecasting is safety-critical: under-predictions risk supply shortfalls, while symmetric error metrics mask this operational asymmetry. We introduce a grid-specific evaluation framework--Asymmetric MAPE, Under-Prediction Rate, and Reserve Margin--that directly measures operational risk rather than statistical accuracy alone. Using this framework, we conduct a systematic evaluation of Mamba-based State Space Models for California grid forecasting on a weather-aligned CAISO TAC-area dataset spanning Nov 2023--Nov 2025 (84,498 hourly records across 5 transmission areas). Our analysis reveals that standard accuracy metrics are poor proxies for operational safety: models with identical MAPE can require vastly different reserve margins. We demonstrate that forecast errors are weakly but significantly associated with temperature (r = 0.16, p < 10^{-16}), motivating weather-aware modeling rather than loss function modification alone. The S-Mamba model achieves the lowest Reserve_{99.5}% margin (14.12%) compared to 16.66% for iTransformer, demonstrating superior forecast reliability under a 99.5th-percentile tail-risk reserve proxy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01442",
        "abs_url": "https://arxiv.org/abs/2601.01442",
        "pdf_url": "https://arxiv.org/pdf/2601.01442",
        "title": "Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations",
        "authors": [
            "Dongrong Li",
            "Tianwei Yu",
            "Xiaodan Fan"
        ],
        "comments": "45 pages, 2 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01446",
        "abs_url": "https://arxiv.org/abs/2601.01446",
        "pdf_url": "https://arxiv.org/pdf/2601.01446",
        "title": "iFlip: Iterative Feedback-driven Counterfactual Example Refinement",
        "authors": [
            "Yilong Wang",
            "Qianli Wang",
            "Nils Feldhus"
        ],
        "comments": "In submission",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01449",
        "abs_url": "https://arxiv.org/abs/2601.01449",
        "pdf_url": "https://arxiv.org/pdf/2601.01449",
        "title": "Segmentation and Processing of German Court Decisions from Open Legal Data",
        "authors": [
            "Harshil Darji",
            "Martin Heckelmann",
            "Christina Kratsch",
            "Gerard de Melo"
        ],
        "comments": "Accepted and published as a research article in Legal Knowledge and Information Systems (JURIX 2025 proceedings, IOS Press). Pages 276--281",
        "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgründe (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01480",
        "abs_url": "https://arxiv.org/abs/2601.01480",
        "pdf_url": "https://arxiv.org/pdf/2601.01480",
        "title": "Modeling Information Blackouts in Missing Not-At-Random Time Series Data",
        "authors": [
            "Aman Sunesh",
            "Allan Ma",
            "Siddarth Nilol"
        ],
        "comments": "8 pages, 7 figures, 3 tables",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01488",
        "abs_url": "https://arxiv.org/abs/2601.01488",
        "pdf_url": "https://arxiv.org/pdf/2601.01488",
        "title": "Four Quadrants of Difficulty: A Simple Categorisation and its Limits",
        "authors": [
            "Vanessa Toborek",
            "Sebastian Müller",
            "Christian Bauckhage"
        ],
        "comments": "prepared for ESANN 2026 submission",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01512",
        "abs_url": "https://arxiv.org/abs/2601.01512",
        "pdf_url": "https://arxiv.org/pdf/2601.01512",
        "title": "A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI",
        "authors": [
            "Wenhui Chu",
            "Aobo Jin",
            "Hardik A. Gohel"
        ],
        "comments": "9 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "This research aims to develop a novel deep learning network, GBU-Net, utilizing a group-batch-normalized U-Net framework, specifically designed for the precise semantic segmentation of the left ventricle in short-axis cine MRI scans. The methodology includes a down-sampling pathway for feature extraction and an up-sampling pathway for detail restoration, enhanced for medical imaging. Key modifications include techniques for better contextual understanding crucial in cardiac MRI segmentation. The dataset consists of 805 left ventricular MRI scans from 45 patients, with comparative analysis using established metrics such as the dice coefficient and mean perpendicular distance. GBU-Net significantly improves the accuracy of left ventricle segmentation in cine MRI scans. Its innovative design outperforms existing methods in tests, surpassing standard metrics like the dice coefficient and mean perpendicular distance. The approach is unique in its ability to capture contextual information, often missed in traditional CNN-based segmentation. An ensemble of the GBU-Net attains a 97% dice score on the SunnyBrook testing dataset. GBU-Net offers enhanced precision and contextual understanding in left ventricle segmentation for surgical robotics and medical analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01547",
        "abs_url": "https://arxiv.org/abs/2601.01547",
        "pdf_url": "https://arxiv.org/pdf/2601.01547",
        "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding",
        "authors": [
            "Tianjun Gu",
            "Chenghua Gong",
            "Jingyu Gong",
            "Zhizhong Zhang",
            "Yuan Xie",
            "Lizhuang Ma",
            "Xin Tan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01589",
        "abs_url": "https://arxiv.org/abs/2601.01589",
        "pdf_url": "https://arxiv.org/pdf/2601.01589",
        "title": "Learning Relationship between Quantum Walks and Underdamped Langevin Dynamics",
        "authors": [
            "Yazhen Wang"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "Fast computational algorithms are in constant demand, and their development has been driven by advances such as quantum speedup and classical acceleration. This paper intends to study search algorithms based on quantum walks in quantum computation and sampling algorithms based on Langevin dynamics in classical computation. On the quantum side, quantum walk-based search algorithms can achieve quadratic speedups over their classical counterparts. In classical computation, a substantial body of work has focused on gradient acceleration, with gradient-adjusted algorithms derived from underdamped Langevin dynamics providing quadratic acceleration over conventional Langevin algorithms. Since both search and sampling algorithms are designed to address learning tasks, we study learning relationship between coined quantum walks and underdamped Langevin dynamics. Specifically, we show that, in terms of the Le Cam deficiency distance, a quantum walk with randomization is asymptotically equivalent to underdamped Langevin dynamics, whereas the quantum walk without randomization is not asymptotically equivalent due to its high-frequency oscillatory behavior. We further discuss the implications of these equivalence and nonequivalence results for the computational and inferential properties of the associated algorithms in machine learning tasks. Our findings offer new insight into the relationship between quantum walks and underdamped Langevin dynamics, as well as the intrinsic mechanisms underlying quantum speedup and classical gradient acceleration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01590",
        "abs_url": "https://arxiv.org/abs/2601.01590",
        "pdf_url": "https://arxiv.org/pdf/2601.01590",
        "title": "Identifying recurrent flows in high-dimensional dissipative chaos from low-dimensional embeddings",
        "authors": [
            "Pierre Beck",
            "Tobias M. Schneider"
        ],
        "comments": "",
        "subjects": "Chaotic Dynamics (nlin.CD); Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Unstable periodic orbits (UPOs) are the non-chaotic, dynamical building blocks of spatio-temporal chaos, motivating a first-principles based theory for turbulence ever since the discovery of deterministic chaos. Despite their key role in the ergodic theory approach to fluid turbulence, identifying UPOs is challenging for two reasons: chaotic dynamics and the high-dimensionality of the spatial discretization. We address both issues at once by proposing a loop convergence algorithm for UPOs directly within a low-dimensional embedding of the chaotic attractor. The convergence algorithm circumvents time-integration, hence avoiding instabilities from exponential error amplification, and operates on a latent dynamics obtained by pulling back the physical equations using automatic differentiation through the learned embedding function. The interpretable latent dynamics is accurate in a statistical sense, and, crucially, the embedding preserves the internal structure of the attractor, which we demonstrate through an equivalence between the latent and physical UPOs of both a model PDE and the 2D Navier-Stokes equations. This allows us to exploit the collapse of high-dimensional dissipative systems onto a lower dimensional manifold, and identify UPOs in the low-dimensional embedding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01594",
        "abs_url": "https://arxiv.org/abs/2601.01594",
        "pdf_url": "https://arxiv.org/pdf/2601.01594",
        "title": "Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity",
        "authors": [
            "Alois Duston",
            "Tan Bui-Thanh"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We introduce and prove a \\textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01619",
        "abs_url": "https://arxiv.org/abs/2601.01619",
        "pdf_url": "https://arxiv.org/pdf/2601.01619",
        "title": "Deep Linear Discriminant Analysis Revisited",
        "authors": [
            "Maxat Tezekbayev",
            "Rustem Takhanov",
            "Arman Bolatov",
            "Zhenisbek Assylbekov"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \\emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01679",
        "abs_url": "https://arxiv.org/abs/2601.01679",
        "pdf_url": "https://arxiv.org/pdf/2601.01679",
        "title": "Simplex Deep Linear Discriminant Analysis",
        "authors": [
            "Maxat Tezekbayev",
            "Arman Bolatov",
            "Zhenisbek Assylbekov"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01698",
        "abs_url": "https://arxiv.org/abs/2601.01698",
        "pdf_url": "https://arxiv.org/pdf/2601.01698",
        "title": "Hidden costs for inference with deep network on embedded system devices",
        "authors": [
            "Chankyu Lee",
            "Woohyun Choi",
            "Sangwook Park"
        ],
        "comments": "published in Proc. of IEEE ICCE 2025",
        "subjects": "Computational Complexity (cs.CC); Machine Learning (cs.LG)",
        "abstract": "This study evaluates the inference performance of various deep learning models under an embedded system environment. In previous works, Multiply-Accumulate operation is typically used to measure computational load of a deep model. According to this study, however, this metric has a limitation to estimate inference time on embedded devices. This paper poses the question of what aspects are overlooked when expressed in terms of Multiply-Accumulate operations. In experiments, an image classification task is performed on an embedded system device using the CIFAR-100 dataset to compare and analyze the inference times of ten deep models with the theoretically calculated Multiply-Accumulate operations for each model. The results highlight the importance of considering additional computations between tensors when optimizing deep learning models for real-time performing in embedded systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01709",
        "abs_url": "https://arxiv.org/abs/2601.01709",
        "pdf_url": "https://arxiv.org/pdf/2601.01709",
        "title": "Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance",
        "authors": [
            "Ziheng Chen",
            "Minxuan Hu",
            "Jiayu Yi",
            "Wenxi Sun"
        ],
        "comments": "",
        "subjects": "Pricing of Securities (q-fin.PR); Machine Learning (cs.LG)",
        "abstract": "We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01741",
        "abs_url": "https://arxiv.org/abs/2601.01741",
        "pdf_url": "https://arxiv.org/pdf/2601.01741",
        "title": "Latent Space Element Method",
        "authors": [
            "Seung Whan Chung",
            "Youngsoo Choi",
            "Christopher Miller",
            "H. Keo Springer",
            "Kyle T. Sullivan"
        ],
        "comments": "17 pages, 10 figures",
        "subjects": "Dynamical Systems (math.DS); Machine Learning (cs.LG); Analysis of PDEs (math.AP); Numerical Analysis (math.NA)",
        "abstract": "How can we build surrogate solvers that train on small domains but scale to larger ones without intrusive access to PDE operators? Inspired by the Data-Driven Finite Element Method (DD-FEM) framework for modular data-driven solvers, we propose the Latent Space Element Method (LSEM), an element-based latent surrogate assembly approach in which a learned subdomain (\"element\") model can be tiled and coupled to form a larger computational domain. Each element is a LaSDI latent ODE surrogate trained from snapshots on a local patch, and neighboring elements are coupled through learned directional interaction terms in latent space, avoiding Schwarz iterations and interface residual evaluations. A smooth window-based blending reconstructs a global field from overlapping element predictions, yielding a scalable assembled latent dynamical system. Experiments on the 1D Burgers and Korteweg-de Vries equations show that LSEM maintains predictive accuracy while scaling to spatial domains larger than those seen in training. LSEM offers an interpretable and extensible route toward foundation-model surrogate solvers built from reusable local models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01757",
        "abs_url": "https://arxiv.org/abs/2601.01757",
        "pdf_url": "https://arxiv.org/pdf/2601.01757",
        "title": "Sparse Convex Biclustering",
        "authors": [
            "Jiakun Jiang",
            "Dewei Xiang",
            "Chenliang Gu",
            "Wei Liu",
            "Binhuan Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01779",
        "abs_url": "https://arxiv.org/abs/2601.01779",
        "pdf_url": "https://arxiv.org/pdf/2601.01779",
        "title": "Machine learning modularity",
        "authors": [
            "Yi Fan",
            "Vishnu Jejjala",
            "Yang Lei"
        ],
        "comments": "34 pages, 7 figures, 6 tables",
        "subjects": "High Energy Physics - Theory (hep-th); Machine Learning (cs.LG)",
        "abstract": "Based on a transformer based sequence-to-sequence architecture combined with a dynamic batching algorithm, this work introduces a machine learning framework for automatically simplifying complex expressions involving multiple elliptic Gamma functions, including the $q$-$\\theta$ function and the elliptic Gamma function. The model learns to apply algebraic identities, particularly the SL$(2,\\mathbb{Z})$ and SL$(3,\\mathbb{Z})$ modular transformations, to reduce heavily scrambled expressions to their canonical forms. Experimental results show that the model achieves over 99\\% accuracy on in-distribution tests and maintains robust performance (exceeding 90\\% accuracy) under significant extrapolation, such as with deeper scrambling depths. This demonstrates that the model has internalized the underlying algebraic rules of modular transformations rather than merely memorizing training patterns. Our work presents the first successful application of machine learning to perform symbolic simplification using modular identities, offering a new automated tool for computations with special functions in quantum field theory and the string theory.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01785",
        "abs_url": "https://arxiv.org/abs/2601.01785",
        "pdf_url": "https://arxiv.org/pdf/2601.01785",
        "title": "SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines",
        "authors": [
            "Rajiv Chaitanya Muttur"
        ],
        "comments": "Presented at ICEdge 2025; nominated for Best Paper Award",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01827",
        "abs_url": "https://arxiv.org/abs/2601.01827",
        "pdf_url": "https://arxiv.org/pdf/2601.01827",
        "title": "Aspect Extraction from E-Commerce Product and Service Reviews",
        "authors": [
            "Valiant Lance D. Dionela",
            "Fatima Kriselle S. Dy",
            "Robin James M. Hombrebueno",
            "Aaron Rae M. Nicolas",
            "Charibeth K. Cheng",
            "Raphael W. Gonda"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01877",
        "abs_url": "https://arxiv.org/abs/2601.01877",
        "pdf_url": "https://arxiv.org/pdf/2601.01877",
        "title": "Random-Matrix-Induced Simplicity Bias in Over-parameterized Variational Quantum Circuits",
        "authors": [
            "Jun Qi",
            "Chao-Han Huck Yang",
            "Pin-Yu Chen",
            "Min-Hsiu Hsieh"
        ],
        "comments": "20 pages, 4 figures",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG); Mathematical Physics (math-ph)",
        "abstract": "Over-parameterization is commonly used to increase the expressivity of variational quantum circuits (VQCs), yet deeper and more highly parameterized circuits often exhibit poor trainability and limited generalization. In this work, we provide a theoretical explanation for this phenomenon from a function-class perspective. We show that sufficiently expressive, unstructured variational ansatze enter a Haar-like universality class in which both observable expectation values and parameter gradients concentrate exponentially with system size. As a consequence, the hypothesis class induced by such circuits collapses with high probability to a narrow family of near-constant functions, a phenomenon we term simplicity bias, with barren plateaus arising as a consequence rather than the root cause. Using tools from random matrix theory and concentration of measure, we rigorously characterize this universality class and establish uniform hypothesis-class collapse over finite datasets. We further show that this collapse is not unavoidable: tensor-structured VQCs, including tensor-network-based and tensor-hypernetwork parameterizations, lie outside the Haar-like universality class. By restricting the accessible unitary ensemble through bounded tensor rank or bond dimension, these architectures prevent concentration of measure, preserve output variability for local observables, and retain non-degenerate gradient signals even in over-parameterized regimes. Together, our results unify barren plateaus, expressivity limits, and generalization collapse under a single structural mechanism rooted in random-matrix universality, highlighting the central role of architectural inductive bias in variational quantum algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01888",
        "abs_url": "https://arxiv.org/abs/2601.01888",
        "pdf_url": "https://arxiv.org/pdf/2601.01888",
        "title": "SafeLoad: Efficient Admission Control Framework for Identifying Memory-Overloading Queries in Cloud Data Warehouses",
        "authors": [
            "Yifan Wu",
            "Yuhan Li",
            "Zhenhua Wang",
            "Zhongle Xie",
            "Dingyu Yang",
            "Ke Chen",
            "Lidan Shou",
            "Bo Tang",
            "Liang Lin",
            "Huan Li",
            "Gang Chen"
        ],
        "comments": "This paper has been accepted for presentation at VLDB 2026",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Memory overload is a common form of resource exhaustion in cloud data warehouses. When database queries fail due to memory overload, it not only wastes critical resources such as CPU time but also disrupts the execution of core business processes, as memory-overloading (MO) queries are typically part of complex workflows. If such queries are identified in advance and scheduled to memory-rich serverless clusters, it can prevent resource wastage and query execution failure. Therefore, cloud data warehouses desire an admission control framework with high prediction precision, interpretability, efficiency, and adaptability to effectively identify MO queries. However, existing admission control frameworks primarily focus on scenarios like SLA satisfaction and resource isolation, with limited precision in identifying MO queries. Moreover, there is a lack of publicly available MO-labeled datasets with workloads for training and benchmarking. To tackle these challenges, we propose SafeLoad, the first query admission control framework specifically designed to identify MO queries. Alongside, we release SafeBench, an open-source, industrial-scale benchmark for this task, which includes 150 million real queries. SafeLoad first filters out memory-safe queries using the interpretable discriminative rule. It then applies a hybrid architecture that integrates both a global model and cluster-level models, supplemented by a misprediction correction module to identify MO queries. Additionally, a self-tuning quota management mechanism dynamically adjusts prediction quotas per cluster to improve precision. Experimental results show that SafeLoad achieves state-of-the-art prediction performance with low online and offline time overhead. Specifically, SafeLoad improves precision by up to 66% over the best baseline and reduces wasted CPU time by up to 8.09x compared to scenarios without SafeLoad.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01921",
        "abs_url": "https://arxiv.org/abs/2601.01921",
        "pdf_url": "https://arxiv.org/pdf/2601.01921",
        "title": "A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach",
        "authors": [
            "Mikel Robredo",
            "Matteo Esposito",
            "Fabio Palomba",
            "Rafael Peñaloza",
            "Valentina Lenarduzzi"
        ],
        "comments": "ACCEPTED REGISTERED REPORT AT SANER (CORE A*) 2026",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest. Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect. Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect. Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01922",
        "abs_url": "https://arxiv.org/abs/2601.01922",
        "pdf_url": "https://arxiv.org/pdf/2601.01922",
        "title": "Efficient temporal prediction of compressible flows in irregular domains using Fourier neural operators",
        "authors": [
            "Yifan Nie",
            "Qiaoxin Li"
        ],
        "comments": "18 pages, 15 figures",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Machine Learning (cs.LG)",
        "abstract": "This paper investigates the temporal evolution of high-speed compressible fluids in irregular flow fields using the Fourier Neural Operator (FNO). We reconstruct the irregular flow field point set into sequential format compatible with FNO input requirements, and then embed temporal bundling technique within a recurrent neural network (RNN) for multi-step prediction. We further employ a composite loss function to balance errors across different physical quantities. Experiments are conducted on three different types of irregular flow fields, including orthogonal and non-orthogonal grid configurations. Then we comprehensively analyze the physical component loss curves, flow field visualizations, and physical profiles. Results demonstrate that our approach significantly surpasses traditional numerical methods in computational efficiency while achieving high accuracy, with maximum relative $L_2$ errors of (0.78, 0.57, 0.35)% for ($p$, $T$, $\\mathbf{u}$) respectively. This verifies that the method can efficiently and accurately simulate the temporal evolution of high-speed compressible flows in irregular domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.01970",
        "abs_url": "https://arxiv.org/abs/2601.01970",
        "pdf_url": "https://arxiv.org/pdf/2601.01970",
        "title": "A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk",
        "authors": [
            "Ayomide Afolabi",
            "Ebere Ogburu",
            "Symon Kimitei"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02061",
        "abs_url": "https://arxiv.org/abs/2601.02061",
        "pdf_url": "https://arxiv.org/pdf/2601.02061",
        "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management",
        "authors": [
            "Faizan Ahmed",
            "Aniket Dixit",
            "James Brusey"
        ],
        "comments": "6 pages, accepted at NeurIPS workshop 2025",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02075",
        "abs_url": "https://arxiv.org/abs/2601.02075",
        "pdf_url": "https://arxiv.org/pdf/2601.02075",
        "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
        "authors": [
            "Zhuofan Shi",
            "Hubao A",
            "Yufei Shao",
            "Mengyan Dai",
            "Yadong Yu",
            "Pan Xiang",
            "Dongliang Huang",
            "Hongxu An",
            "Chunxiao Xin",
            "Haiyang Shen",
            "Zhenyu Wang",
            "Yunshan Na",
            "Gang Huang",
            "Xiang Jing"
        ],
        "comments": "24 pages,4 figures",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong this http URL work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02145",
        "abs_url": "https://arxiv.org/abs/2601.02145",
        "pdf_url": "https://arxiv.org/pdf/2601.02145",
        "title": "Feature-based Inversion of 2.5D Controlled Source Electromagnetic Data using Generative Priors",
        "authors": [
            "Hongyu Zhou",
            "Haoran Sun",
            "Rui Guo",
            "Maokun Li",
            "Fan Yang",
            "Shenheng Xu"
        ],
        "comments": "",
        "subjects": "Geophysics (physics.geo-ph); Machine Learning (cs.LG)",
        "abstract": "In this study, we investigate feature-based 2.5D controlled source marine electromagnetic (mCSEM) data inversion using generative priors. Two-and-half dimensional modeling using finite difference method (FDM) is adopted to compute the response of horizontal electric dipole (HED) excitation. Rather than using a neural network to approximate the entire inverse mapping in a black-box manner, we adopt a plug-andplay strategy in which a variational autoencoder (VAE) is used solely to learn prior information on conductivity distributions. During the inversion process, the conductivity model is iteratively updated using the Gauss Newton method, while the model space is constrained by projections onto the learned VAE decoder. This framework preserves explicit control over data misfit and enables flexible adaptation to different survey configurations. Numerical and field experiments demonstrate that the proposed approach effectively incorporates prior information, improves reconstruction accuracy, and exhibits good generalization performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02158",
        "abs_url": "https://arxiv.org/abs/2601.02158",
        "pdf_url": "https://arxiv.org/pdf/2601.02158",
        "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience",
        "authors": [
            "Almaz Ermilov"
        ],
        "comments": "24 pages, 8 figures, 10 tables; benchmark and code at this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Geophysics (physics.geo-ph)",
        "abstract": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02198",
        "abs_url": "https://arxiv.org/abs/2601.02198",
        "pdf_url": "https://arxiv.org/pdf/2601.02198",
        "title": "Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models",
        "authors": [
            "Alexander Möllers",
            "Julius Hense",
            "Florian Schulz",
            "Timo Milbich",
            "Maximilian Alber",
            "Lukas Ruff"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02241",
        "abs_url": "https://arxiv.org/abs/2601.02241",
        "pdf_url": "https://arxiv.org/pdf/2601.02241",
        "title": "From Mice to Trains: Amortized Bayesian Inference on Graph Data",
        "authors": [
            "Svenja Jedhoff",
            "Elizaveta Semenova",
            "Aura Raulo",
            "Anne Meyer",
            "Paul-Christian Bürkner"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02246",
        "abs_url": "https://arxiv.org/abs/2601.02246",
        "pdf_url": "https://arxiv.org/pdf/2601.02246",
        "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets",
        "authors": [
            "Annoor Sharara Akhand"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02257",
        "abs_url": "https://arxiv.org/abs/2601.02257",
        "pdf_url": "https://arxiv.org/pdf/2601.02257",
        "title": "Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization",
        "authors": [
            "Joel Daniel Andersson",
            "Palak Jain",
            "Satchit Sivakumar"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)",
        "abstract": "We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties. We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02265",
        "abs_url": "https://arxiv.org/abs/2601.02265",
        "pdf_url": "https://arxiv.org/pdf/2601.02265",
        "title": "Predicting Early and Complete Drug Release from Long-Acting Injectables Using Explainable Machine Learning",
        "authors": [
            "Karla N. Robles",
            "Manar D. Samad"
        ],
        "comments": "",
        "subjects": "Biomolecules (q-bio.BM); Machine Learning (cs.LG)",
        "abstract": "Polymer-based long-acting injectables (LAIs) have transformed the treatment of chronic diseases by enabling controlled drug delivery, thus reducing dosing frequency and extending therapeutic duration. Achieving controlled drug release from LAIs requires extensive optimization of the complex underlying physicochemical properties. Machine learning (ML) can accelerate LAI development by modeling the complex relationships between LAI properties and drug release. However, recent ML studies have provided limited information on key properties that modulate drug release, due to the lack of custom modeling and analysis tailored to LAI data. This paper presents a novel data transformation and explainable ML approach to synthesize actionable information from 321 LAI formulations by predicting early drug release at 24, 48, and 72 hours, classification of release profile types, and prediction of complete release profiles. These three experiments investigate the contribution and control of LAI material characteristics in early and complete drug release profiles. A strong correlation (>0.65) is observed between the true and predicted drug release in 72 hours, while a 0.87 F1-score is obtained in classifying release profile types. A time-independent ML framework predicts delayed biphasic and triphasic curves with better performance than current time-dependent approaches. Shapley additive explanations reveal the relative influence of material characteristics during early and for complete release which fill several gaps in previous in-vitro and ML-based studies. The novel approach and findings can provide a quantitative strategy and recommendations for scientists to optimize the drug-release dynamics of LAI. The source code for the model implementation is publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02322",
        "abs_url": "https://arxiv.org/abs/2601.02322",
        "pdf_url": "https://arxiv.org/pdf/2601.02322",
        "title": "Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction",
        "authors": [
            "Shuozhi Zuo",
            "Yixin Wang"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG)",
        "abstract": "Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2026-01-06",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-06?abs=True",
        "arxiv_id": "2601.02324",
        "abs_url": "https://arxiv.org/abs/2601.02324",
        "pdf_url": "https://arxiv.org/pdf/2601.02324",
        "title": "Hunting for \"Oddballs\" with Machine Learning: Detecting Anomalous Exoplanets Using a Deep-Learned Low-Dimensional Representation of Transit Spectra with Autoencoders",
        "authors": [
            "Alexander Roman",
            "Emilie Panek",
            "Roy T. Forestano",
            "Eyup B. Unlu",
            "Katia Matcheva",
            "Konstantin T. Matchev"
        ],
        "comments": "14 pages, 12 figures",
        "subjects": "Earth and Planetary Astrophysics (astro-ph.EP); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)",
        "abstract": "This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation. We use the Atmospheric Big Challenge (ABC) database, a publicly available dataset with over 100,000 simulated exoplanet spectra, to construct an anomaly detection scenario by defining CO2-rich atmospheres as anomalies and CO2-poor atmospheres as the normal class. We benchmarked four different anomaly detection strategies: Autoencoder Reconstruction Loss, One-Class Support Vector Machine (1 class-SVM), K-means Clustering, and Local Outlier Factor (LOF). Each method was evaluated in both the original spectral space and the autoencoder's latent space using Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) metrics. To test the performance of the different methods under realistic conditions, we introduced Gaussian noise levels ranging from 10 to 50 ppm. Our results indicate that anomaly detection is consistently more effective when performed within the latent space across all noise levels. Specifically, K-means clustering in the latent space emerged as a stable and high-performing method. We demonstrate that this anomaly detection approach is robust to noise levels up to 30 ppm (consistent with realistic space-based observations) and remains viable even at 50 ppm when leveraging latent space representations. On the other hand, the performance of the anomaly detection methods applied directly in the raw spectral space degrades significantly with increasing the level of noise. This suggests that autoencoder-driven dimensionality reduction offers a robust methodology for flagging chemically anomalous targets in large-scale surveys where exhaustive retrievals are computationally prohibitive.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]