[
    {
        "order": 1,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03315",
        "abs_url": "https://arxiv.org/abs/2601.03315",
        "pdf_url": "https://arxiv.org/pdf/2601.03315",
        "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
        "authors": [
            "Dhruv Trehan",
            "Paras Chopra"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03321",
        "abs_url": "https://arxiv.org/abs/2601.03321",
        "pdf_url": "https://arxiv.org/pdf/2601.03321",
        "title": "Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting",
        "authors": [
            "Kun Zhao",
            "Siyuan Dai",
            "Pan Wang",
            "Jifeng Song",
            "Hui Ji",
            "Chenghua Lin",
            "Liang Zhan",
            "Haoteng Tang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel \"Reason-then-Summarize\" architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03327",
        "abs_url": "https://arxiv.org/abs/2601.03327",
        "pdf_url": "https://arxiv.org/pdf/2601.03327",
        "title": "Extreme-value forest fire prediction A study of the Loss Function in an Ordinality Scheme",
        "authors": [
            "Nicolas Caron",
            "Christophe Guyeux",
            "Hassan Noura",
            "Benjamin Aynes"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Wildfires are highly imbalanced natural hazards in both space and severity, making the prediction of extreme events particularly challenging. In this work, we introduce the first ordinal classification framework for forecasting wildfire severity levels directly aligned with operational decision-making in France. Our study investigates the influence of loss-function design on the ability of neural models to predict rare yet critical high-severity fire occurrences. We compare standard cross-entropy with several ordinal-aware objectives, including the proposed probabilistic TDeGPD loss derived from a truncated discrete exponentiated Generalized Pareto Distribution. Through extensive benchmarking over multiple architectures and real operational data, we show that ordinal supervision substantially improves model performance over conventional approaches. In particular, the Weighted Kappa Loss (WKLoss) achieves the best overall results, with more than +0.1 IoU gain on the most extreme severity classes while maintaining competitive calibration quality. However, performance remains limited for the rarest events due to their extremely low representation in the dataset. These findings highlight the importance of integrating both severity ordering, data imbalance considerations, and seasonality risk into wildfire forecasting systems. Future work will focus on incorporating seasonal dynamics and uncertainty information into training to further improve the reliability of extreme-event prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03332",
        "abs_url": "https://arxiv.org/abs/2601.03332",
        "pdf_url": "https://arxiv.org/pdf/2601.03332",
        "title": "LUT-KAN: Segment-wise LUT Quantization for Fast KAN Inference",
        "authors": [
            "Oleksandr Kuznetsov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Kolmogorov--Arnold Networks (KAN) replace scalar weights by learnable univariate functions, often implemented with B-splines. This design can be accurate and interpretable, but it makes inference expensive on CPU because each layer requires many spline evaluations. Standard quantization toolchains are also hard to apply because the main computation is not a matrix multiply but repeated spline basis evaluation. This paper introduces LUT-KAN, a segment-wise lookup-table (LUT) compilation and quantization method for PyKAN-style KAN layers. LUT-KAN converts each edge function into a per-segment LUT with affine int8/uint8 quantization and linear interpolation. The method provides an explicit and reproducible inference contract, including boundary conventions and out-of-bounds (OOB) policies. We propose an ``honest baseline'' methodology for speed evaluation: B-spline evaluation and LUT evaluation are compared under the same backend optimization (NumPy vs NumPy and Numba vs Numba), which separates representation gains from vectorization and JIT effects. Experiments include controlled sweeps over LUT resolution L in 16, 32, 64, 128 and two quantization schemes (symmetric int8 and asymmetric uint8). We report accuracy, speed, and memory metrics with mean and standard deviation across multiple seeds. A two-by-two OOB robustness matrix evaluates behavior under different boundary modes and OOB policies. In a case study, we compile a trained KAN model for DoS attack detection (CICIDS2017 pipeline) into LUT artifacts. The compiled model preserves classification quality (F1 drop below 0.0002) while reducing steady-state CPU inference latency by 12x under NumPy and 10x under Numba backends (honest baseline). The memory overhead is approximately 10x at L=64. All code and artifacts are publicly available with fixed release tags for reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03367",
        "abs_url": "https://arxiv.org/abs/2601.03367",
        "pdf_url": "https://arxiv.org/pdf/2601.03367",
        "title": "Physics-Informed Gaussian Process Regression for the Constitutive Modeling of Concrete: A Data-Driven Improvement to Phenomenological Models",
        "authors": [
            "Chenyang Li",
            "Himanshu Sharma",
            "Youcai Wu",
            "Joseph Magallanes",
            "K.T. Ramesh",
            "Michael D. Shields"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "Understanding and modeling the constitutive behavior of concrete is crucial for civil and defense applications, yet widely used phenomenological models such as Karagozian \\& Case concrete (KCC) model depend on empirically calibrated failure surfaces that lack flexibility in model form and associated uncertainty quantification. This work develops a physics-informed framework that retains the modular elastoplastic structure of KCC model while replacing its empirical failure surface with a constrained Gaussian Process Regression (GPR) surrogate that can be learned directly from experimentally accessible observables. Triaxial compression data under varying confinement levels are used for training, and the surrogate is then evaluated at confinement levels not included in the training set to assess its generalization capability. Results show that an unconstrained GPR interpolates well near training conditions but deteriorates and violates essential physical constraints under extrapolation, even when augmented with simulated data. In contrast, a physics-informed GPR that incorporates derivative-based constraints aligned with known material behavior yields markedly better accuracy and reliability, including at higher confinement levels beyond the training range. Probabilistic enforcement of these constraints also reduces predictive variance, producing tighter confidence intervals in data-scarce regimes. Overall, the proposed approach delivers a robust, uncertainty-aware surrogate that improves generalization and streamlines calibration without sacrificing the interpretability and numerical efficiency of the KCC model, offering a practical path toward an improved constitutive models for concrete.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03375",
        "abs_url": "https://arxiv.org/abs/2601.03375",
        "pdf_url": "https://arxiv.org/pdf/2601.03375",
        "title": "Enhancing Small Dataset Classification Using Projected Quantum Kernels with Convolutional Neural Networks",
        "authors": [
            "A.M.A.S.D. Alagiyawanna",
            "Asoka Karunananda",
            "A. Mahasinghe",
            "Thushari Silva"
        ],
        "comments": "Accepted and published in IEEE 2024. This is the authors manuscript version; final version available at IEEE Xplore: this https URL",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Convolutional Neural Networks (CNNs) have shown promising results in efficiency and accuracy in image classification. However, their efficacy often relies on large, labeled datasets, posing challenges for applications with limited data availability. Our research addresses these challenges by introducing an innovative approach that leverages projected quantum kernels (PQK) to enhance feature extraction for CNNs, specifically tailored for small datasets. Projected quantum kernels, derived from quantum computing principles, offer a promising avenue for capturing complex patterns and intricate data structures that traditional CNNs might miss. By incorporating these kernels into the feature extraction process, we improved the representational ability of CNNs. Our experiments demonstrated that, with 1000 training samples, the PQK-enhanced CNN achieved 95% accuracy on the MNIST dataset and 90% on the CIFAR-10 dataset, significantly outperforming the classical CNN, which achieved only 60% and 12% accuracy on the respective datasets. This research reveals the potential of quantum computing in overcoming data scarcity issues in machine learning and paves the way for future exploration of quantum-assisted neural networks, suggesting that projected quantum kernels can serve as a powerful approach for enhancing CNN-based classification in data-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03376",
        "abs_url": "https://arxiv.org/abs/2601.03376",
        "pdf_url": "https://arxiv.org/pdf/2601.03376",
        "title": "Weather-Aware Transformer for Real-Time Route Optimization in Drone-as-a-Service Operations",
        "authors": [
            "Kamal Mohamed",
            "Lillian Wassim",
            "Ali Hamdi",
            "Khaled Shaban"
        ],
        "comments": "2025 IEEE/ACS 22nd International Conference on Computer Systems and Applications (AICCSA)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper presents a novel framework to accelerate route prediction in Drone-as-a-Service operations through weather-aware deep learning models. While classical path-planning algorithms, such as A* and Dijkstra, provide optimal solutions, their computational complexity limits real-time applicability in dynamic environments. We address this limitation by training machine learning and deep learning models on synthetic datasets generated from classical algorithm simulations. Our approach incorporates transformer-based and attention-based architectures that utilize weather heuristics to predict optimal next-node selections while accounting for meteorological conditions affecting drone operations. The attention mechanisms dynamically weight environmental factors including wind patterns, wind bearing, and temperature to enhance routing decisions under adverse weather conditions. Experimental results demonstrate that our weather-aware models achieve significant computational speedup over traditional algorithms while maintaining route optimization performance, with transformer-based architectures showing superior adaptation to dynamic environmental constraints. The proposed framework enables real-time, weather-responsive route optimization for large-scale DaaS operations, representing a substantial advancement in the efficiency and safety of autonomous drone systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03385",
        "abs_url": "https://arxiv.org/abs/2601.03385",
        "pdf_url": "https://arxiv.org/pdf/2601.03385",
        "title": "SIGMA: Scalable Spectral Insights for LLM Collapse",
        "authors": [
            "Yi Gu",
            "Lingyou Pang",
            "Xiangkun Ye",
            "Tianyu Wang",
            "Jianyu Lin",
            "Carey E. Priebe",
            "Alexander Aue"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Probability (math.PR)",
        "abstract": "The rapid adoption of synthetic data for training Large Language Models (LLMs) has introduced the technical challenge of \"model collapse\"-a degenerative process where recursive training on model-generated content leads to a contraction of distributional variance and representational quality. While the phenomenology of collapse is increasingly evident, rigorous methods to quantify and predict its onset in high-dimensional spaces remain elusive. In this paper, we introduce SIGMA (Spectral Inequalities for Gram Matrix Analysis), a unified framework that benchmarks model collapse through the spectral lens of the embedding Gram matrix. By deriving and utilizing deterministic and stochastic bounds on the matrix's spectrum, SIGMA provides a mathematically grounded metric to track the contraction of the representation space. Crucially, our stochastic formulation enables scalable estimation of these bounds, making the framework applicable to large-scale foundation models where full eigendecomposition is intractable. We demonstrate that SIGMA effectively captures the transition towards degenerate states, offering both theoretical insights into the mechanics of collapse and a practical, scalable tool for monitoring the health of recursive training pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03413",
        "abs_url": "https://arxiv.org/abs/2601.03413",
        "pdf_url": "https://arxiv.org/pdf/2601.03413",
        "title": "Sensor to Pixels: Decentralized Swarm Gathering via Image-Based Reinforcement Learning",
        "authors": [
            "Yigal Koifman",
            "Eran Iceland",
            "Erez Koifman",
            "Ariel Barel",
            "Alfred M. Bruckstein"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
        "abstract": "This study highlights the potential of image-based reinforcement learning methods for addressing swarm-related tasks. In multi-agent reinforcement learning, effective policy learning depends on how agents sense, interpret, and process inputs. Traditional approaches often rely on handcrafted feature extraction or raw vector-based representations, which limit the scalability and efficiency of learned policies concerning input order and size. In this work we propose an image-based reinforcement learning method for decentralized control of a multi-agent system, where observations are encoded as structured visual inputs that can be processed by Neural Networks, extracting its spatial features and producing novel decentralized motion control rules. We evaluate our approach on a multi-agent convergence task of agents with limited-range and bearing-only sensing that aim to keep the swarm cohesive during the aggregation. The algorithm's performance is evaluated against two benchmarks: an analytical solution proposed by Bellaiche and Bruckstein, which ensures convergence but progresses slowly, and VariAntNet, a neural network-based framework that converges much faster but shows medium success rates in hard constellations. Our method achieves high convergence, with a pace nearly matching that of VariAntNet. In some scenarios, it serves as the only practical alternative.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03424",
        "abs_url": "https://arxiv.org/abs/2601.03424",
        "pdf_url": "https://arxiv.org/pdf/2601.03424",
        "title": "Spectral Archaeology: The Causal Topology of Model Evolution",
        "authors": [
            "Valentin NoÃ«l"
        ],
        "comments": "45 pages, 15 figures, Under Review",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Behavioral benchmarks tell us \\textit{what} a model does, but not \\textit{how}. We introduce a training-free mechanistic probe using attention-graph spectra. Treating each layer as a token graph, we compute algebraic connectivity ($\\lambda_2$), smoothness, and spectral entropy. Across 12 models and 10 languages, these measures yield stable ``spectral fingerprints'' that expose discontinuities missed by standard evaluation. We report four results. (1) Models undergoing specific curriculum transitions (e.g., code-to-chat) show an English-only, syntax-triggered connectivity failure on non-canonical constructions, reaching $\\Delta\\lambda_2 \\approx -0.76$. We term this scar \\textit{Passive-Triggered Connectivity Collapse} (PTCC). Analysis of the Phi lineage reveals that PTCC appears and resolves across developmental stages, implicating brittle curriculum shifts rather than synthetic data per se. (2) PTCC reflects a specialization trade-off: strengthened formal routing at the expense of stylistic flexibility. (3) We identify four recurrent processing strategies; simple frozen-threshold rules enable perfect forensic identification across lineages. (4) Mechanistically, PTCC localizes to a sparse Layer 2 ``compensatory patch'' of heads that fails under syntactic stress; activation steering can partially restore connectivity, recovering $\\approx 38\\%$ of lost information flow. Finally, dominant topological regimes track tokenization density more than language identity, suggesting ``healthy'' geometry varies systematically across scripts. Overall, attention-graph spectra provide a practical tool for auditing and training-regime verification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03434",
        "abs_url": "https://arxiv.org/abs/2601.03434",
        "pdf_url": "https://arxiv.org/pdf/2601.03434",
        "title": "VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding",
        "authors": [
            "Zibo Liu",
            "Muyang Li",
            "Zhe Jiang",
            "Shigang Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "News videos are carefully edited multimodal narratives that combine narration, visuals, and external quotations into coherent storylines. In recent years, there have been significant advances in evaluating multimodal large language models (MLLMs) for news video understanding. However, existing benchmarks largely focus on single-source, intra-video reasoning, where each report is processed in isolation. In contrast, real-world news consumption is inherently multi-sourced: the same event is reported by different outlets with complementary details, distinct narrative choices, and sometimes conflicting claims that unfold over time. Robust news understanding, therefore, requires models to compare perspectives from different sources, align multimodal evidence across sources, and synthesize multi-source information. To fill this gap, we introduce VNU-Bench, the first benchmark for multi-source, cross-video understanding in the news domain. We design a set of new question types that are unique in testing models' ability of understanding multi-source multimodal news from a variety of different angles. We design a novel hybrid human-model QA generation process that addresses the issues of scalability and quality control in building a large dataset for cross-source news understanding. The dataset comprises 429 news groups, 1,405 videos, and 2,501 high-quality questions. Comprehensive evaluation of both closed- and open-source multimodal models shows that VNU-Bench poses substantial challenges for current MLLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03459",
        "abs_url": "https://arxiv.org/abs/2601.03459",
        "pdf_url": "https://arxiv.org/pdf/2601.03459",
        "title": "An Expectation-Maximization Algorithm for Domain Adaptation in Gaussian Causal Models",
        "authors": [
            "Mohammad Ali Javidian"
        ],
        "comments": "An earlier version of this work was accepted for the Proceedings of the 2025 IEEE International Conference on Data Mining (ICDM)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We study the problem of imputing a designated target variable that is systematically missing in a shifted deployment domain, when a Gaussian causal DAG is available from a fully observed source domain. We propose a unified EM-based framework that combines source and target data through the DAG structure to transfer information from observed variables to the missing target. On the methodological side, we formulate a population EM operator in the DAG parameter space and introduce a first-order (gradient) EM update that replaces the costly generalized least-squares M-step with a single projected gradient step. Under standard local strong-concavity and smoothness assumptions and a BWY-style \\cite{Balakrishnan2017EM} gradient-stability (bounded missing-information) condition, we show that this first-order EM operator is locally contractive around the true target parameters, yielding geometric convergence and finite-sample guarantees on parameter error and the induced target-imputation error in Gaussian SEMs under covariate shift and local mechanism shifts. Algorithmically, we exploit the known causal DAG to freeze source-invariant mechanisms and re-estimate only those conditional distributions directly affected by the shift, making the procedure scalable to higher-dimensional models. In experiments on a synthetic seven-node SEM, the 64-node MAGIC-IRRI genetic network, and the Sachs protein-signaling data, the proposed DAG-aware first-order EM algorithm improves target imputation accuracy over a fit-on-source Bayesian network and a Kiiveri-style EM baseline, with the largest gains under pronounced domain shift.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03477",
        "abs_url": "https://arxiv.org/abs/2601.03477",
        "pdf_url": "https://arxiv.org/pdf/2601.03477",
        "title": "Hybrid Approach for Driver Behavior Analysis with Machine Learning, Feature Optimization, and Explainable AI",
        "authors": [
            "Mehedi Hasan Shuvo",
            "Md. Raihan Tapader",
            "Nur Mohammad Tamjid",
            "Sajjadul Islam",
            "Ahnaf Atef Choudhury",
            "Jia Uddin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Progressive driver behavior analytics is crucial for improving road safety and mitigating the issues caused by aggressive or inattentive driving. Previous studies have employed machine learning and deep learning techniques, which often result in low feature optimization, thereby compromising both high performance and interpretability. To fill these voids, this paper proposes a hybrid approach to driver behavior analysis that uses a 12,857-row and 18-column data set taken from Kaggle. After applying preprocessing techniques such as label encoding, random oversampling, and standard scaling, 13 machine learning algorithms were tested. The Random Forest Classifier achieved an accuracy of 95%. After deploying the LIME technique in XAI, the top 10 features with the most significant positive and negative influence on accuracy were identified, and the same algorithms were retrained. The accuracy of the Random Forest Classifier decreased slightly to 94.2%, confirming that the efficiency of the model can be improved without sacrificing performance. This hybrid model can provide a return on investment in terms of the predictive power and explainability of the driver behavior process.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03484",
        "abs_url": "https://arxiv.org/abs/2601.03484",
        "pdf_url": "https://arxiv.org/pdf/2601.03484",
        "title": "From Bits to Chips: An LLM-based Hardware-Aware Quantization Agent for Streamlined Deployment of LLMs",
        "authors": [
            "Kaiyuan Deng",
            "Hangyu Zheng",
            "Minghai Qing",
            "Kunxiong Zhu",
            "Gen Li",
            "Yang Xiao",
            "Lan Emily Zhang",
            "Linke Guo",
            "Bo Hui",
            "Yanzhi Wang",
            "Geng Yuan",
            "Gagan Agrawal",
            "Wei Niu",
            "Xiaolong Ma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deploying models, especially large language models (LLMs), is becoming increasingly attractive to a broader user base, including those without specialized expertise. However, due to the resource constraints of certain hardware, maintaining high accuracy with larger model while meeting the hardware requirements remains a significant challenge. Model quantization technique helps mitigate memory and compute bottlenecks, yet the added complexities of tuning and deploying quantized models further exacerbates these challenges, making the process unfriendly to most of the users. We introduce the Hardware-Aware Quantization Agent (HAQA), an automated framework that leverages LLMs to streamline the entire quantization and deployment process by enabling efficient hyperparameter tuning and hardware configuration, thereby simultaneously improving deployment quality and ease of use for a broad range of users. Our results demonstrate up to a 2.3x speedup in inference, along with increased throughput and improved accuracy compared to unoptimized models on Llama. Additionally, HAQA is designed to implement adaptive quantization strategies across diverse hardware platforms, as it automatically finds optimal settings even when they appear counterintuitive, thereby reducing extensive manual effort and demonstrating superior adaptability. Code will be released.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03561",
        "abs_url": "https://arxiv.org/abs/2601.03561",
        "pdf_url": "https://arxiv.org/pdf/2601.03561",
        "title": "Green's-Function Spherical Neural Operators for Biological Heterogeneity",
        "authors": [
            "Hao Tang",
            "Hao Chen",
            "Hao Li",
            "Chao Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Spherical deep learning has been widely applied to a broad range of real-world problems. Existing approaches often face challenges in balancing strong spherical geometric inductive biases with the need to model real-world heterogeneity. To solve this while retaining spherical geometry, we first introduce a designable Green's function framework (DGF) to provide new spherical operator solution strategy: Design systematic Green's functions under rotational group. Based on DGF, to model biological heterogeneity, we propose Green's-Function Spherical Neural Operator (GSNO) fusing 3 operator solutions: (1) Equivariant Solution derived from Equivariant Green's Function for symmetry-consistent modeling; (2) Invariant Solution derived from Invariant Green's Function to eliminate nuisance heterogeneity, e.g., consistent background field; (3) Anisotropic Solution derived from Anisotropic Green's Function to model anisotropic systems, especially fibers with preferred direction. Therefore, the resulting model, GSNO can adapt to real-world heterogeneous systems with nuisance variability and anisotropy while retaining spectral efficiency. Evaluations on spherical MNIST, Shallow Water Equation, diffusion MRI fiber prediction, cortical parcellation and molecule structure modeling demonstrate the superiority of GSNO.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03569",
        "abs_url": "https://arxiv.org/abs/2601.03569",
        "pdf_url": "https://arxiv.org/pdf/2601.03569",
        "title": "Local Intrinsic Dimensionality of Ground Motion Data for Early Detection of Complex Catastrophic Slope Failure",
        "authors": [
            "Yuansan Liu",
            "Antoinette Tordesillas",
            "James Bailey"
        ],
        "comments": "9 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Local Intrinsic Dimensionality (LID) has shown strong potential for identifying anomalies and outliers in high-dimensional data across a wide range of real-world applications, including landslide failure detection in granular media. Early and accurate identification of failure zones in landslide-prone areas is crucial for effective geohazard mitigation. While existing approaches typically rely on surface displacement data analyzed through statistical or machine learning techniques, they often fall short in capturing both the spatial correlations and temporal dynamics that are inherent in such data. To address this gap, we focus on ground-monitored landslides and introduce a novel approach that jointly incorporates spatial and temporal information, enabling the detection of complex landslides and including multiple successive failures occurring in distinct areas of the same slope. To be specific, our method builds upon an existing LID-based technique, known as sLID. We extend its capabilities in three key ways. (1) Kinematic enhancement: we incorporate velocity into the sLID computation to better capture short-term temporal dependencies and deformation rate relationships. (2) Spatial fusion: we apply Bayesian estimation to aggregate sLID values across spatial neighborhoods, effectively embedding spatial correlations into the LID scores. (3) Temporal modeling: we introduce a temporal variant, tLID, that learns long-term dynamics from time series data, providing a robust temporal representation of displacement behavior. Finally, we integrate both components into a unified framework, referred to as spatiotemporal LID (stLID), to identify samples that are anomalous in either or both dimensions. Extensive experiments show that stLID consistently outperforms existing methods in failure detection precision and lead-time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03577",
        "abs_url": "https://arxiv.org/abs/2601.03577",
        "pdf_url": "https://arxiv.org/pdf/2601.03577",
        "title": "Variational Inference, Entropy, and Orthogonality: A Unified Theory of Mixture-of-Experts",
        "authors": [
            "Ye Su",
            "Yong Liu"
        ],
        "comments": "27 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Mixture-of-Experts models enable large language models to scale efficiently, as they only activate a subset of experts for each input. Their core mechanisms, Top-k routing and auxiliary load balancing, remain heuristic, however, lacking a cohesive theoretical underpinning to support them. To this end, we build the first unified theoretical framework that rigorously derives these practices as optimal sparse posterior approximation and prior regularization from a Bayesian perspective, while simultaneously framing them as mechanisms to minimize routing ambiguity and maximize channel capacity from an information-theoretic perspective. We also pinpoint the inherent combinatorial hardness of routing, defining it as the NP-hard sparse subset selection problem. We rigorously prove the existence of a \"Coherence Barrier\"; when expert representations exhibit high mutual coherence, greedy routing strategies theoretically fail to recover the optimal expert subset. Importantly, we formally verify that imposing geometric orthogonality in the expert feature space is sufficient to narrow the divide between the NP-hard global optimum and polynomial-time greedy approximation. Our comparative analyses confirm orthogonality regularization as the optimal engineering relaxation for large-scale models. Our work offers essential theoretical support and technical assurance for a deeper understanding and novel designs of MoE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03584",
        "abs_url": "https://arxiv.org/abs/2601.03584",
        "pdf_url": "https://arxiv.org/pdf/2601.03584",
        "title": "Local Gradient Regulation Stabilizes Federated Learning under Client Heterogeneity",
        "authors": [
            "Ping Luo",
            "Jiahuan Wang",
            "Ziqing Wen",
            "Tao Sun",
            "Dongsheng Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, yet its stability is fundamentally challenged by statistical heterogeneity in realistic deployments. Here, we show that client heterogeneity destabilizes FL primarily by distorting local gradient dynamics during client-side optimization, causing systematic drift that accumulates across communication rounds and impedes global convergence. This observation highlights local gradients as a key regulatory lever for stabilizing heterogeneous FL systems. Building on this insight, we develop a general client-side perspective that regulates local gradient contributions without incurring additional communication overhead. Inspired by swarm intelligence, we instantiate this perspective through Exploratory--Convergent Gradient Re-aggregation (ECGR), which balances well-aligned and misaligned gradient components to preserve informative updates while suppressing destabilizing effects. Theoretical analysis and extensive experiments, including evaluations on the LC25000 medical imaging dataset, demonstrate that regulating local gradient dynamics consistently stabilizes federated learning across state-of-the-art methods under heterogeneous data distributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03603",
        "abs_url": "https://arxiv.org/abs/2601.03603",
        "pdf_url": "https://arxiv.org/pdf/2601.03603",
        "title": "A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data",
        "authors": [
            "Kaidong Feng",
            "Zhu Sun",
            "Roy Ka-Wei Lee",
            "Xun Jiang",
            "Yin-Leng Theng",
            "Yi Ding"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Smartphone sensing offers an unobtrusive and scalable way to track daily behaviors linked to mental health, capturing changes in sleep, mobility, and phone use that often precede symptoms of stress, anxiety, or depression. While most prior studies focus on detection that responds to existing conditions, forecasting mental health enables proactive support through Just-in-Time Adaptive Interventions. In this paper, we present the first comprehensive benchmarking study comparing traditional machine learning (ML), deep learning (DL), and large language model (LLM) approaches for mental health forecasting using the College Experience Sensing (CES) dataset, the most extensive longitudinal dataset of college student mental health to date. We systematically evaluate models across temporal windows, feature granularities, personalization strategies, and class imbalance handling. Our results show that DL models, particularly Transformer (Macro-F1 = 0.58), achieve the best overall performance, while LLMs show strength in contextual reasoning but weaker temporal modeling. Personalization substantially improves forecasts of severe mental health states. By revealing how different modeling approaches interpret phone sensing behavioral data over time, this work lays the groundwork for next-generation, adaptive, and human-centered mental health technologies that can advance both research and real-world well-being.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03612",
        "abs_url": "https://arxiv.org/abs/2601.03612",
        "pdf_url": "https://arxiv.org/pdf/2601.03612",
        "title": "Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias",
        "authors": [
            "Joonwon Seo"
        ],
        "comments": "Monograph. Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
        "abstract": "This monograph introduces a novel approach to polyphonic music generation by addressing the \"Missing Middle\" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03629",
        "abs_url": "https://arxiv.org/abs/2601.03629",
        "pdf_url": "https://arxiv.org/pdf/2601.03629",
        "title": "Learning Shortest Paths When Data is Scarce",
        "authors": [
            "Dmytro Matsypura",
            "Yu Pan",
            "Hanzhao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Digital twins and other simulators are increasingly used to support routing decisions in large-scale networks. However, simulator outputs often exhibit systematic bias, while ground-truth measurements are costly and scarce. We study a stochastic shortest-path problem in which a planner has access to abundant synthetic samples, limited real-world observations, and an edge-similarity structure capturing expected behavioral similarity across links. We model the simulator-to-reality discrepancy as an unknown, edge-specific bias that varies smoothly over the similarity graph, and estimate it using Laplacian-regularized least squares. This approach yields calibrated edge cost estimates even in data-scarce regimes. We establish finite-sample error bounds, translate estimation error into path-level suboptimality guarantees, and propose a computable, data-driven certificate that verifies near-optimality of a candidate route. For cold-start settings without initial real data, we develop a bias-aware active learning algorithm that leverages the simulator and adaptively selects edges to measure until a prescribed accuracy is met. Numerical experiments on multiple road networks and traffic graphs further demonstrate the effectiveness of our methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03634",
        "abs_url": "https://arxiv.org/abs/2601.03634",
        "pdf_url": "https://arxiv.org/pdf/2601.03634",
        "title": "Kantorovich-Type Stochastic Neural Network Operators for the Mean-Square Approximation of Certain Second-Order Stochastic Processes",
        "authors": [
            "Sachin Saini",
            "Uaday Singh"
        ],
        "comments": "18 Pages, 7 Figures",
        "subjects": "Machine Learning (cs.LG); Probability (math.PR)",
        "abstract": "Artificial neural network operators (ANNOs) have been widely used for approximating deterministic input-output functions; however, their extension to random dynamics remains comparatively unexplored. In this paper, we construct a new class of \\textbf{Kantorovich-type Stochastic Neural Network Operators (K-SNNOs)} in which randomness is incorporated not at the coefficient level, but through \\textbf{stochastic neurons} driven by stochastic integrators. This framework enables the operator to inherit the probabilistic structure of the underlying process, making it suitable for modeling and approximating stochastic signals. We establish mean-square convergence of K-SNNOs to the target stochastic process and derive quantitative error estimates expressing the rate of approximation in terms of the modulus of continuity. Numerical simulations further validate the theoretical results by demonstrating accurate reconstruction of sample paths and rapid decay of the mean square error (MSE). Graphical results, including sample-wise approximations and empirical MSE behaviour, illustrate the robustness and effectiveness of the proposed stochastic-neuron-based operator.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03646",
        "abs_url": "https://arxiv.org/abs/2601.03646",
        "pdf_url": "https://arxiv.org/pdf/2601.03646",
        "title": "ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning",
        "authors": [
            "Zhengyi Kwan",
            "Zhang Wei",
            "Aik Beng Ng",
            "Zhengkui Wang",
            "Simon See"
        ],
        "comments": "15 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Job scheduling is widely used in real-world manufacturing systems to assign ordered job operations to machines under various constraints. Existing solutions remain limited by long running time or insufficient schedule quality, especially when problem scale increases. In this paper, we propose ReLA, a reinforcement-learning (RL) scheduler built on structured representation learning and aggregation. ReLA first learns diverse representations from scheduling entities, including job operations and machines, using two intra-entity learning modules with self-attention and convolution and one inter-entity learning module with cross-attention. These modules are applied in a multi-scale architecture, and their outputs are aggregated to support RL decision-making. Across experiments on small, medium, and large job instances, ReLA achieves the best makespan in most tested settings over the latest solutions. On non-large instances, ReLA reduces the optimality gap of the SOTA baseline by 13.0%, while on large-scale instances it reduces the gap by 78.6%, with the average optimality gaps lowered to 7.3% and 2.1%, respectively. These results confirm that ReLA's learned representations and aggregation provide strong decision support for RL scheduling, and enable fast job completion and decision-making for real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03654",
        "abs_url": "https://arxiv.org/abs/2601.03654",
        "pdf_url": "https://arxiv.org/pdf/2601.03654",
        "title": "Quantum Classical Ridgelet Neural Network For Time Series Model",
        "authors": [
            "Bahadur Yadav",
            "Sanjay Kumar Mohanty"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC); Quantum Algebra (math.QA)",
        "abstract": "In this study, we present a quantum computing method that incorporates ridglet transforms into the quantum processing pipelines for time series data. Here, the Ridgelet neural network is integrated with a single-qubit quantum computing method, which improves feature extraction and forecasting capabilities. Furthermore, experimental results using financial time series data demonstrate the superior performance of our model compared to existing models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03658",
        "abs_url": "https://arxiv.org/abs/2601.03658",
        "pdf_url": "https://arxiv.org/pdf/2601.03658",
        "title": "Group and Exclusive Sparse Regularization-based Continual Learning of CNNs",
        "authors": [
            "Basile Tousside",
            "Janis Mohr",
            "JÃ¶rg Frochte"
        ],
        "comments": "12 pages, Canadian Artificial Intelligence Association (CAIAC)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We present a regularization-based approach for continual learning (CL) of fixed capacity convolutional neural networks (CNN) that does not suffer from the problem of catastrophic forgetting when learning multiple tasks sequentially. This method referred to as Group and Exclusive Sparsity based Continual Learning (GESCL) avoids forgetting of previous tasks by ensuring the stability of the CNN via a stability regularization term, which prevents filters detected as important for past tasks to deviate too much when learning a new task. On top of that, GESCL makes the network plastic via a plasticity regularization term that leverage the over-parameterization of CNNs to efficiently sparsify the network and tunes unimportant filters making them relevant for future tasks. Doing so, GESCL deals with significantly less parameters and computation compared to CL approaches that either dynamically expand the network or memorize past tasks' data. Experiments on popular CL vision benchmarks show that GESCL leads to significant improvements over state-of-the-art method in terms of overall CL performance, as measured by classification accuracy as well as in terms of avoiding catastrophic forgetting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03664",
        "abs_url": "https://arxiv.org/abs/2601.03664",
        "pdf_url": "https://arxiv.org/pdf/2601.03664",
        "title": "Stochastic Voronoi Ensembles for Anomaly Detection",
        "authors": [
            "Yang Cao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Anomaly detection aims to identify data instances that deviate significantly from majority of data, which has been widely used in fraud detection, network security, and industrial quality control. Existing methods struggle with datasets exhibiting varying local densities: distance-based methods miss local anomalies, while density-based approaches require careful parameter selection and incur quadratic time complexity. We observe that local anomalies, though indistinguishable under global analysis, become conspicuous when the data space is decomposed into restricted regions and each region is examined independently. Leveraging this geometric insight, we propose SVEAD (Stochastic Voronoi Ensembles Anomaly Detector), which constructs ensemble random Voronoi diagrams and scores points by normalized cell-relative distances weighted by local scale. The proposed method achieves linear time complexity and constant space complexity. Experiments on 45 datasets demonstrate that SVEAD outperforms 12 state-of-the-art approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03683",
        "abs_url": "https://arxiv.org/abs/2601.03683",
        "pdf_url": "https://arxiv.org/pdf/2601.03683",
        "title": "Rethinking Recurrent Neural Networks for Time Series Forecasting: A Reinforced Recurrent Encoder with Prediction-Oriented Proximal Policy Optimization",
        "authors": [
            "Xin Lai",
            "Shiming Deng",
            "Lu Yu",
            "Yumin Lai",
            "Shenghao Qiao",
            "Xinze Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Time series forecasting plays a crucial role in contemporary engineering information systems for supporting decision-making across various industries, where Recurrent Neural Networks (RNNs) have been widely adopted due to their capability in modeling sequential data. Conventional RNN-based predictors adopt an encoder-only strategy with sliding historical windows as inputs to forecast future values. However, this approach treats all time steps and hidden states equally without considering their distinct contributions to forecasting, leading to suboptimal performance. To address this limitation, we propose a novel Reinforced Recurrent Encoder with Prediction-oriented Proximal Policy Optimization, RRE-PPO4Pred, which significantly improves time series modeling capacity and forecasting accuracy of the RNN models. The core innovations of this method are: (1) A novel Reinforced Recurrent Encoder (RRE) framework that enhances RNNs by formulating their internal adaptation as a Markov Decision Process, creating a unified decision environment capable of learning input feature selection, hidden skip connection, and output target selection; (2) An improved Prediction-oriented Proximal Policy Optimization algorithm, termed PPO4Pred, which is equipped with a Transformer-based agent for temporal reasoning and develops a dynamic transition sampling strategy to enhance sampling efficiency; (3) A co-evolutionary optimization paradigm to facilitate the learning of the RNN predictor and the policy agent, providing adaptive and interactive time series modeling. Comprehensive evaluations on five real-world datasets indicate that our method consistently outperforms existing baselines, and attains accuracy better than state-of-the-art Transformer models, thus providing an advanced time series predictor in engineering informatics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03701",
        "abs_url": "https://arxiv.org/abs/2601.03701",
        "pdf_url": "https://arxiv.org/pdf/2601.03701",
        "title": "Inference Attacks Against Graph Generative Diffusion Models",
        "authors": [
            "Xiuling Wang",
            "Xin Huang",
            "Guibo Luo",
            "Jianliang Xu"
        ],
        "comments": "This work has been accepted by USENIX Security 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph generative diffusion models have recently emerged as a powerful paradigm for generating complex graph structures, effectively capturing intricate dependencies and relationships within graph data. However, the privacy risks associated with these models remain largely unexplored. In this paper, we investigate information leakage in such models through three types of black-box inference attacks. First, we design a graph reconstruction attack, which can reconstruct graphs structurally similar to those training graphs from the generated graphs. Second, we propose a property inference attack to infer the properties of the training graphs, such as the average graph density and the distribution of densities, from the generated graphs. Third, we develop two membership inference attacks to determine whether a given graph is present in the training set. Extensive experiments on three different types of graph generative diffusion models and six real-world graphs demonstrate the effectiveness of these attacks, significantly outperforming the baseline approaches. Finally, we propose two defense mechanisms that mitigate these inference attacks and achieve a better trade-off between defense strength and target model utility than existing methods. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03704",
        "abs_url": "https://arxiv.org/abs/2601.03704",
        "pdf_url": "https://arxiv.org/pdf/2601.03704",
        "title": "Investigating Knowledge Distillation Through Neural Networks for Protein Binding Affinity Prediction",
        "authors": [
            "Wajid Arshad Abbasi",
            "Syed Ali Abbas",
            "Maryum Bibi",
            "Saiqa Andleeb",
            "Muhammad Naveed Akhtar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM); Molecular Networks (q-bio.MN); Quantitative Methods (q-bio.QM)",
        "abstract": "The trade-off between predictive accuracy and data availability makes it difficult to predict protein--protein binding affinity accurately. The lack of experimentally resolved protein structures limits the performance of structure-based machine learning models, which generally outperform sequence-based methods. In order to overcome this constraint, we suggest a regression framework based on knowledge distillation that uses protein structural data during training and only needs sequence data during inference. The suggested method uses binding affinity labels and intermediate feature representations to jointly supervise the training of a sequence-based student network under the guidance of a structure-informed teacher network. Leave-One-Complex-Out (LOCO) cross-validation was used to assess the framework on a non-redundant protein--protein binding affinity benchmark dataset. A maximum Pearson correlation coefficient (P_r) of 0.375 and an RMSE of 2.712 kcal/mol were obtained by sequence-only baseline models, whereas a P_r of 0.512 and an RMSE of 2.445 kcal/mol were obtained by structure-based models. With a P_r of 0.481 and an RMSE of 2.488 kcal/mol, the distillation-based student model greatly enhanced sequence-only performance. Improved agreement and decreased bias were further confirmed by thorough error analyses. With the potential to close the performance gap between sequence-based and structure-based models as larger datasets become available, these findings show that knowledge distillation is an efficient method for transferring structural knowledge to sequence-based predictors. The source code for running inference with the proposed distillation-based binding affinity predictor can be accessed at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03706",
        "abs_url": "https://arxiv.org/abs/2601.03706",
        "pdf_url": "https://arxiv.org/pdf/2601.03706",
        "title": "The Geometry of the Pivot: A Note on Lazy Pivoted Cholesky and Farthest Point Sampling",
        "authors": [
            "Gil Shabat"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "Low-rank approximations of large kernel matrices are ubiquitous in machine learning, particularly for scaling Gaussian Processes to massive datasets. The Pivoted Cholesky decomposition is a standard tool for this task, offering a computationally efficient, greedy low-rank approximation. While its algebraic properties are well-documented in numerical linear algebra, its geometric intuition within the context of kernel methods often remains obscure. In this note, we elucidate the geometric interpretation of the algorithm within the Reproducing Kernel Hilbert Space (RKHS). We demonstrate that the pivotal selection step is mathematically equivalent to Farthest Point Sampling (FPS) using the kernel metric, and that the Cholesky factor construction is an implicit Gram-Schmidt orthogonalization. We provide a concise derivation and a minimalist Python implementation to bridge the gap between theory and practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03723",
        "abs_url": "https://arxiv.org/abs/2601.03723",
        "pdf_url": "https://arxiv.org/pdf/2601.03723",
        "title": "ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization",
        "authors": [
            "Shijie Zhang",
            "Kevin Zhang",
            "Zheyuan Gu",
            "Xiang Guo",
            "Rujun Guo",
            "Shaoyu Liu",
            "Guanjun Jiang",
            "Xiaozhao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \\textbf{E}lastic \\textbf{T}rust \\textbf{R}egions (\\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03725",
        "abs_url": "https://arxiv.org/abs/2601.03725",
        "pdf_url": "https://arxiv.org/pdf/2601.03725",
        "title": "EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning",
        "authors": [
            "Jing-Cheng Pang",
            "Liu Sun",
            "Chang Zhou",
            "Xian Tang",
            "Haichuan Ma",
            "Kun Jiang",
            "Jianlong Wang",
            "Kai Zhang",
            "Sijie Wu",
            "Haoran Cai",
            "Chenwei Wu",
            "Xubin Li",
            "Xin Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model's evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03753",
        "abs_url": "https://arxiv.org/abs/2601.03753",
        "pdf_url": "https://arxiv.org/pdf/2601.03753",
        "title": "Probabilistic Transformers for Joint Modeling of Global Weather Dynamics and Decision-Centric Variables",
        "authors": [
            "Paulius Rauba",
            "Viktor Cikojevic",
            "Fran Bartolic",
            "Sam Levang",
            "Ty Dickinson",
            "Chase Dwelle"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Weather forecasts sit upstream of high-stakes decisions in domains such as grid operations, aviation, agriculture, and emergency response. Yet forecast users often face a difficult trade-off. Many decision-relevant targets are functionals of the atmospheric state variables, such as extrema, accumulations, and threshold exceedances, rather than state variables themselves. As a result, users must estimate these targets via post-processing, which can be suboptimal and can introduce structural bias. The core issue is that decisions depend on distributions over these functionals that the model is not trained to learn directly. In this work, we introduce GEM-2, a probabilistic transformer that jointly learns global atmospheric dynamics alongside a suite of variables that users directly act upon. Using this training recipe, we show that a lightweight (~275M params) and computationally efficient (~20-100x training speedup relative to state-of-the-art) transformer trained on the CRPS objective can directly outperform operational numerical weather prediction (NWP) models and be competitive with ML models that rely on expensive multi-step diffusion processes or require bespoke multi-stage fine-tuning strategies. We further demonstrate state-of-the-art economic value metrics under decision-theoretic evaluation, stable convergence to climatology at S2S and seasonal timescales, and a surprising insensitivity to many commonly assumed architectural and training design choices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03776",
        "abs_url": "https://arxiv.org/abs/2601.03776",
        "pdf_url": "https://arxiv.org/pdf/2601.03776",
        "title": "Improving Compactness and Reducing Ambiguity of CFIRE Rule-Based Explanations",
        "authors": [
            "Sebastian MÃ¼ller",
            "Tobias Schneider",
            "Ruben Kemna",
            "Vanessa Toborek"
        ],
        "comments": "Prepared for ESANN 2026 submission",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Models trained on tabular data are widely used in sensitive domains, increasing the demand for explanation methods to meet transparency needs. CFIRE is a recent algorithm in this domain that constructs compact surrogate rule models from local explanations. While effective, CFIRE may assign rules associated with different classes to the same sample, introducing ambiguity. We investigate this ambiguity and propose a post-hoc pruning strategy that removes rules with low contribution or conflicting coverage, yielding smaller and less ambiguous models while preserving fidelity. Experiments across multiple datasets confirm these improvements with minimal impact on predictive performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03793",
        "abs_url": "https://arxiv.org/abs/2601.03793",
        "pdf_url": "https://arxiv.org/pdf/2601.03793",
        "title": "Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs",
        "authors": [
            "Sethupathy Parameswaran",
            "Suresh Sundaram",
            "Yuan Fang"
        ],
        "comments": "Accepted by WSDM 2026",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "Node classification is a fundamental problem in information retrieval with many real-world applications, such as community detection in social networks, grouping articles published online and product categorization in e-commerce. Zero-shot node classification in text-attributed graphs (TAGs) presents a significant challenge, particularly due to the absence of labeled data. In this paper, we propose a novel Zero-shot Prompt Tuning (ZPT) framework to address this problem by leveraging a Universal Bimodal Conditional Generator (UBCG). Our approach begins with pre-training a graph-language model to capture both the graph structure and the associated textual descriptions of each node. Following this, a conditional generative model is trained to learn the joint distribution of nodes in both graph and text modalities, enabling the generation of synthetic samples for each class based solely on the class name. These synthetic node and text embeddings are subsequently used to perform continuous prompt tuning, facilitating effective node classification in a zero-shot setting. Furthermore, we conduct extensive experiments on multiple benchmark datasets, demonstrating that our framework performs better than existing state-of-the-art baselines. We also provide ablation studies to validate the contribution of the bimodal generator. The code is provided at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03802",
        "abs_url": "https://arxiv.org/abs/2601.03802",
        "pdf_url": "https://arxiv.org/pdf/2601.03802",
        "title": "Quantum vs. Classical Machine Learning: A Benchmark Study for Financial Prediction",
        "authors": [
            "Rehan Ahmad",
            "Muhammad Kashif",
            "Nouhaila Innan",
            "Muhammad Shafique"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "In this paper, we present a reproducible benchmarking framework that systematically compares QML models with architecture-matched classical counterparts across three financial tasks: (i) directional return prediction on U.S. and Turkish equities, (ii) live-trading simulation with Quantum LSTMs versus classical LSTMs on the S\\&P 500, and (iii) realized volatility forecasting using Quantum Support Vector Regression. By standardizing data splits, features, and evaluation metrics, our study provides a fair assessment of when current-generation QML models can match or exceed classical methods. Our results reveal that quantum approaches show performance gains when data structure and circuit design are well aligned. In directional classification, hybrid quantum neural networks surpass the parameter-matched ANN by \\textbf{+3.8 AUC} and \\textbf{+3.4 accuracy points} on \\texttt{AAPL} stock and by \\textbf{+4.9 AUC} and \\textbf{+3.6 accuracy points} on Turkish stock \\texttt{KCHOL}. In live trading, the QLSTM achieves higher risk-adjusted returns in \\textbf{two of four} S\\&P~500 regimes. For volatility forecasting, an angle-encoded QSVR attains the \\textbf{lowest QLIKE} on \\texttt{KCHOL} and remains within $\\sim$0.02-0.04 QLIKE of the best classical kernels on \\texttt{S\\&P~500} and \\texttt{AAPL}. Our benchmarking framework clearly identifies the scenarios where current QML architectures offer tangible improvements and where established classical methods continue to dominate.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03805",
        "abs_url": "https://arxiv.org/abs/2601.03805",
        "pdf_url": "https://arxiv.org/pdf/2601.03805",
        "title": "Detecting Semantic Backdoors in a Mystery Shopping Scenario",
        "authors": [
            "Arpad Berta",
            "Gabor Danner",
            "Istvan Hegedus",
            "Mark Jelasity"
        ],
        "comments": "Source code available at this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Detecting semantic backdoors in classification models--where some classes can be activated by certain natural, but out-of-distribution inputs--is an important problem that has received relatively little attention. Semantic backdoors are significantly harder to detect than backdoors that are based on trigger patterns due to the lack of such clearly identifiable patterns. We tackle this problem under the assumption that the clean training dataset and the training recipe of the model are both known. These assumptions are motivated by a consumer protection scenario, in which the responsible authority performs mystery shopping to test a machine learning service provider. In this scenario, the authority uses the provider's resources and tools to train a model on a given dataset and tests whether the provider included a backdoor. In our proposed approach, the authority creates a reference model pool by training a small number of clean and poisoned models using trusted infrastructure, and calibrates a model distance threshold to identify clean models. We propose and experimentally analyze a number of approaches to compute model distances and we also test a scenario where the provider performs an adaptive attack to avoid detection. The most reliable method is based on requesting adversarial training from the provider. The model distance is best measured using a set of input samples generated by inverting the models in such a way as to maximize the distance from clean samples. With these settings, our method can often completely separate clean and poisoned models, and it proves to be superior to state-of-the-art backdoor detectors as well.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03839",
        "abs_url": "https://arxiv.org/abs/2601.03839",
        "pdf_url": "https://arxiv.org/pdf/2601.03839",
        "title": "Logic Tensor Network-Enhanced Generative Adversarial Network",
        "authors": [
            "Nijesh Upreti",
            "Vaishak Belle"
        ],
        "comments": "In Proceedings ICLP 2025, arXiv:2601.00047",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)",
        "abstract": "In this paper, we introduce Logic Tensor Network-Enhanced Generative Adversarial Network (LTN-GAN), a novel framework that enhances Generative Adversarial Networks (GANs) by incorporating Logic Tensor Networks (LTNs) to enforce domain-specific logical constraints during the sample generation process. Although GANs have shown remarkable success in generating realistic data, they often lack mechanisms to incorporate prior knowledge or enforce logical consistency, limiting their applicability in domains requiring rule adherence. LTNs provide a principled way to integrate first-order logic with neural networks, enabling models to reason over and satisfy logical constraints. By combining the strengths of GANs for realistic data synthesis with LTNs for logical reasoning, we gain valuable insights into how logical constraints influence the generative process while improving both the diversity and logical consistency of the generated samples. We evaluate LTN-GAN across multiple datasets, including synthetic datasets (gaussian, grid, rings) and the MNIST dataset, demonstrating that our model significantly outperforms traditional GANs in terms of adherence to predefined logical constraints while maintaining the quality and diversity of generated samples. This work highlights the potential of neuro-symbolic approaches to enhance generative modeling in knowledge-intensive domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03882",
        "abs_url": "https://arxiv.org/abs/2601.03882",
        "pdf_url": "https://arxiv.org/pdf/2601.03882",
        "title": "Feature-Aware One-Shot Federated Learning via Hierarchical Token Sequences",
        "authors": [
            "Shudong Liu",
            "Hanwen Zhang",
            "Xiuling Wang",
            "Yuesheng Zhu",
            "Guibo Luo"
        ],
        "comments": "9 pages; 6 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "One-shot federated learning (OSFL) reduces the communication cost and privacy risks of iterative federated learning by constructing a global model with a single round of communication. However, most existing methods struggle to achieve robust performance on real-world domains such as medical imaging, or are inefficient when handling non-IID (Independent and Identically Distributed) data. To address these limitations, we introduce FALCON, a framework that enhances the effectiveness of OSFL over non-IID image data. The core idea of FALCON is to leverage the feature-aware hierarchical token sequences generation and knowledge distillation into OSFL. First, each client leverages a pretrained visual encoder with hierarchical scale encoding to compress images into hierarchical token sequences, which capture multi-scale semantics. Second, a multi-scale autoregressive transformer generator is used to model the distribution of these token sequences and generate the synthetic sequences. Third, clients upload the synthetic sequences along with the local classifier trained on the real token sequences to the server. Finally, the server incorporates knowledge distillation into global training to reduce reliance on precise distribution modeling. Experiments on medical and natural image datasets validate the effectiveness of FALCON in diverse non-IID scenarios, outperforming the best OSFL baselines by 9.58% in average accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03895",
        "abs_url": "https://arxiv.org/abs/2601.03895",
        "pdf_url": "https://arxiv.org/pdf/2601.03895",
        "title": "Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training",
        "authors": [
            "Chi Liu",
            "Xin Chen"
        ],
        "comments": "10 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03938",
        "abs_url": "https://arxiv.org/abs/2601.03938",
        "pdf_url": "https://arxiv.org/pdf/2601.03938",
        "title": "FOREVER: Forgetting Curve-Inspired Memory Replay for Language Model Continual Learning",
        "authors": [
            "Yujie Feng",
            "Hao Wang",
            "Jian Li",
            "Xu Chu",
            "Zhaolu Kang",
            "Yiran Liu",
            "Yasha Wang",
            "Philip S. Yu",
            "Xiao-Ming Wu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Continual learning (CL) for large language models (LLMs) aims to enable sequential knowledge acquisition without catastrophic forgetting. Memory replay methods are widely used for their practicality and effectiveness, but most rely on fixed, step-based heuristics that often misalign with the model's actual learning progress, since identical training steps can result in varying degrees of parameter change. Motivated by recent findings that LLM forgetting mirrors the Ebbinghaus human forgetting curve, we propose FOREVER (FORgEtting curVe-inspired mEmory Replay), a novel CL framework that aligns replay schedules with a model-centric notion of time. FOREVER defines model time using the magnitude of optimizer updates, allowing forgetting curve-inspired replay intervals to align with the model's internal evolution rather than raw training steps. Building on this approach, FOREVER incorporates a forgetting curve-based replay scheduler to determine when to replay and an intensity-aware regularization mechanism to adaptively control how to replay. Extensive experiments on three CL benchmarks and models ranging from 0.6B to 13B parameters demonstrate that FOREVER consistently mitigates catastrophic forgetting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03977",
        "abs_url": "https://arxiv.org/abs/2601.03977",
        "pdf_url": "https://arxiv.org/pdf/2601.03977",
        "title": "Stage-specific cancer survival prediction enriched by explainable machine learning",
        "authors": [
            "Parisa Poorhasani",
            "Bogdan Iancu"
        ],
        "comments": "12 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Despite the fact that cancer survivability rates vary greatly between stages, traditional survival prediction models have frequently been trained and assessed using examples from all combined phases of the disease. This method may result in an overestimation of performance and ignore the stage-specific variations. Using the SEER dataset, we created and verified explainable machine learning (ML) models to predict stage-specific cancer survivability in colorectal, stomach, and liver cancers. ML-based cancer survival analysis has been a long-standing topic in the literature; however, studies involving the explainability and transparency of ML survivability models are limited. Our use of explainability techniques, including SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), enabled us to illustrate significant feature-cancer stage interactions that would have remained hidden in traditional black-box models. We identified how certain demographic and clinical variables influenced survival differently across cancer stages and types. These insights provide not only transparency but also clinical relevance, supporting personalized treatment planning. By focusing on stage-specific models, this study provides new insights into the most important factors at each stage of cancer, offering transparency and potential clinical relevance to support personalized treatment planning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04019",
        "abs_url": "https://arxiv.org/abs/2601.04019",
        "pdf_url": "https://arxiv.org/pdf/2601.04019",
        "title": "Modeling Behavioral Patterns in News Recommendations Using Fuzzy Neural Networks",
        "authors": [
            "Kevin Innerebner",
            "Stephan Bartl",
            "Markus Reiter-Haas",
            "Elisabeth Lex"
        ],
        "comments": "Accepted for the IR for Good track at ECIR'26",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "News recommender systems are increasingly driven by black-box models, offering little transparency for editorial decision-making. In this work, we introduce a transparent recommender system that uses fuzzy neural networks to learn human-readable rules from behavioral data for predicting article clicks. By extracting the rules at configurable thresholds, we can control rule complexity and thus, the level of interpretability. We evaluate our approach on two publicly available news datasets (i.e., MIND and EB-NeRD) and show that we can accurately predict click behavior compared to several established baselines, while learning human-readable rules. Furthermore, we show that the learned rules reveal news consumption patterns, enabling editors to align content curation goals with target audience behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04051",
        "abs_url": "https://arxiv.org/abs/2601.04051",
        "pdf_url": "https://arxiv.org/pdf/2601.04051",
        "title": "Symbolic Regression for Shared Expressions: Introducing Partial Parameter Sharing",
        "authors": [
            "Viktor Martinek",
            "Roland Herzog"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Symbolic Regression aims to find symbolic expressions that describe datasets. Due to better interpretability, it is a machine learning paradigm particularly powerful for scientific discovery. In recent years, several works have expanded the concept to allow the description of similar phenomena using a single expression with varying sets of parameters, thereby introducing categorical variables. Some previous works allow only \"non-shared\" (category-value-specific) parameters, and others also incorporate \"shared\" (category-value-agnostic) parameters. We expand upon those efforts by considering multiple categorical variables, and introducing intermediate levels of parameter sharing. With two categorical variables, an intermediate level of parameter sharing emerges, i.e., parameters which are shared across either category but change across the other. The new approach potentially decreases the number of parameters, while revealing additional information about the problem. Using a synthetic, fitting-only example, we test the limits of this setup in terms of data requirement reduction and transfer learning. As a real-world symbolic regression example, we demonstrate the benefits of the proposed approach on an astrophysics dataset used in a previous study, which considered only one categorical variable. We achieve a similar fit quality but require significantly fewer individual parameters, and extract additional information about the problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04054",
        "abs_url": "https://arxiv.org/abs/2601.04054",
        "pdf_url": "https://arxiv.org/pdf/2601.04054",
        "title": "LinkD: AutoRegressive Diffusion Model for Mechanical Linkage Synthesis",
        "authors": [
            "Yayati Jadhav",
            "Amir Barati Farimani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Designing mechanical linkages to achieve target end-effector trajectories presents a fundamental challenge due to the intricate coupling between continuous node placements, discrete topological configurations, and nonlinear kinematic constraints. The highly nonlinear motion-to-configuration relationship means small perturbations in joint positions drastically alter trajectories, while the combinatorially expanding design space renders conventional optimization and heuristic methods computationally intractable. We introduce an autoregressive diffusion framework that exploits the dyadic nature of linkage assembly by representing mechanisms as sequentially constructed graphs, where nodes correspond to joints and edges to rigid links. Our approach combines a causal transformer with a Denoising Diffusion Probabilistic Model (DDPM), both conditioned on target trajectories encoded via a transformer encoder. The causal transformer autoregressively predicts discrete topology node-by-node, while the DDPM refines each node's spatial coordinates and edge connectivity to previously generated nodes. This sequential generation enables adaptive trial-and-error synthesis where problematic nodes exhibiting kinematic locking or collisions can be selectively regenerated, allowing autonomous correction of degenerate configurations during design. Our graph-based, data-driven methodology surpasses traditional optimization approaches, enabling scalable inverse design that generalizes to mechanisms with arbitrary node counts. We demonstrate successful synthesis of linkage systems containing up to 20 nodes with extensibility to N-node architectures. This work advances autoregressive graph generation methodologies and computational kinematic synthesis, establishing new paradigms for scalable inverse design of complex mechanical systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04057",
        "abs_url": "https://arxiv.org/abs/2601.04057",
        "pdf_url": "https://arxiv.org/pdf/2601.04057",
        "title": "Using Legacy Polysomnography Data to Train a Radar System to Quantify Sleep in Older Adults and People living with Dementia",
        "authors": [
            "M. Yin",
            "K. G. Ravindran",
            "C. Hadjipanayi",
            "A. Bannon",
            "A. Rapeaux",
            "C. Della Monica",
            "T. S. Lande",
            "Derk-Jan Dijk",
            "T. G. Constandinou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Objective: Ultra-wideband radar technology offers a promising solution for unobtrusive and cost-effective in-home sleep monitoring. However, the limited availability of radar sleep data poses challenges in building robust models that generalize across diverse cohorts and environments. This study proposes a novel deep transfer learning framework to enhance sleep stage classification using radar data. Methods: An end-to-end neural network was developed to classify sleep stages based on nocturnal respiratory and motion signals. The network was trained using a combination of large-scale polysomnography (PSG) datasets and radar data. A domain adaptation approach employing adversarial learning was utilized to bridge the knowledge gap between PSG and radar signals. Validation was performed on a radar dataset of 47 older adults (mean age: 71.2), including 18 participants with prodromal or mild Alzheimer disease. Results: The proposed network structure achieves an accuracy of 79.5% with a Kappa value of 0.65 when classifying wakefulness, rapid eye movement, light sleep and deep sleep. Experimental results confirm that our deep transfer learning approach significantly enhances automatic sleep staging performance in the target domain. Conclusion: This method effectively addresses challenges associated with data variability and limited sample size, substantially improving the reliability of automatic sleep staging models, especially in contexts where radar data is limited. Significance: The findings underscore the viability of UWB radar as a nonintrusive, forward-looking sleep assessment tool that could significantly benefit care for older people and people with neurodegenerative disorders.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04058",
        "abs_url": "https://arxiv.org/abs/2601.04058",
        "pdf_url": "https://arxiv.org/pdf/2601.04058",
        "title": "Minimum distance classification for nonlinear dynamical systems",
        "authors": [
            "Dominique Martinez"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We address the problem of classifying trajectory data generated by some nonlinear dynamics, where each class corresponds to a distinct dynamical system. We propose Dynafit, a kernel-based method for learning a distance metric between training trajectories and the underlying dynamics. New observations are assigned to the class with the most similar dynamics according to the learned metric. The learning algorithm approximates the Koopman operator which globally linearizes the dynamics in a (potentially infinite) feature space associated with a kernel function. The distance metric is computed in feature space independently of its dimensionality by using the kernel trick common in machine learning. We also show that the kernel function can be tailored to incorporate partial knowledge of the dynamics when available. Dynafit is applicable to various classification tasks involving nonlinear dynamical systems and sensors. We illustrate its effectiveness on three examples: chaos detection with the logistic map, recognition of handwritten dynamics and of visual dynamic textures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04110",
        "abs_url": "https://arxiv.org/abs/2601.04110",
        "pdf_url": "https://arxiv.org/pdf/2601.04110",
        "title": "Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models",
        "authors": [
            "Magnus BÃ¼hler",
            "Lennart Purucker",
            "Frank Hutter"
        ],
        "comments": "Accepted for oral presentation at the EurIPS 2025 Workshop on AI for Tabular Data (Copenhagen)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Fine-tuning tabular foundation models (TFMs) under data scarcity is challenging, as early stopping on even scarcer validation data often fails to capture true generalization performance. We propose CausalMixFT, a method that enhances fine-tuning robustness and downstream performance by generating structurally consistent synthetic samples using Structural Causal Models (SCMs) fitted on the target dataset. This approach augments limited real data with causally informed synthetic examples, preserving feature dependencies while expanding training diversity. Evaluated across 33 classification datasets from TabArena and over 2300 fine-tuning runs, our CausalMixFT method consistently improves median normalized ROC-AUC from 0.10 (standard fine-tuning) to 0.12, outperforming purely statistical generators such as CTGAN (-0.01), TabEBM (-0.04), and TableAugment (-0.09). Moreover, it narrows the median validation-test performance correlation gap from 0.67 to 0.30, enabling more reliable validation-based early stopping, a key step toward improving fine-tuning stability under data scarcity. These results demonstrate that incorporating causal structure into data augmentation provides an effective and principled route to fine-tuning tabular foundation models in low-data regimes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04164",
        "abs_url": "https://arxiv.org/abs/2601.04164",
        "pdf_url": "https://arxiv.org/pdf/2601.04164",
        "title": "Clinical Data Goes MEDS? Let's OWL make sense of it",
        "authors": [
            "Alberto Marfoglia",
            "Jong Ho Jhee",
            "Adrien Coulet"
        ],
        "comments": "12 pages, 5 tables, 4 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to enable representing MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We demonstrate the approach on a synthetic clinical dataset that describes patient care pathways for ruptured intracranial aneurysms and validate the resulting graph using SHACL constraints. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04171",
        "abs_url": "https://arxiv.org/abs/2601.04171",
        "pdf_url": "https://arxiv.org/pdf/2601.04171",
        "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
        "authors": [
            "Mohit Raghavendra",
            "Anisha Gunjal",
            "Bing Liu",
            "Yunzhong He"
        ],
        "comments": "31 pages, 11 Figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04176",
        "abs_url": "https://arxiv.org/abs/2601.04176",
        "pdf_url": "https://arxiv.org/pdf/2601.04176",
        "title": "Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear SchrÃ¶dinger Equation",
        "authors": [
            "Pietro de Oliveira Esteves"
        ],
        "comments": "9 pages, 4 figures, 2 tables. Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04181",
        "abs_url": "https://arxiv.org/abs/2601.04181",
        "pdf_url": "https://arxiv.org/pdf/2601.04181",
        "title": "Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition",
        "authors": [
            "Nia Touko",
            "Matthew O A Ellis",
            "Cristiano Capone",
            "Alessio Burrello",
            "Elisa Donati",
            "Luca Manneschi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Human-Computer Interaction (cs.HC)",
        "abstract": "Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We propose a lightweight framework for Test-Time Adaptation (TTA) using a Temporal Convolutional Network (TCN) backbone. We introduce three deployment-ready strategies: (i) causal adaptive batch normalization for real-time statistical alignment; (ii) a Gaussian Mixture Model (GMM) alignment with experience replay to prevent forgetting; and (iii) meta-learning for rapid, few-shot calibration. Evaluated on the NinaPro DB6 multi-session dataset, our framework significantly bridges the inter-session accuracy gap with minimal overhead. Our results show that experience-replay updates yield superior stability under limited data, while meta-learning achieves competitive performance in one- and two-shot regimes using only a fraction of the data required by current benchmarks. This work establishes a path toward robust, \"plug-and-play\" myoelectric control for long-term prosthetic use.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03265",
        "abs_url": "https://arxiv.org/abs/2601.03265",
        "pdf_url": "https://arxiv.org/pdf/2601.03265",
        "title": "Jailbreak-Zero: A Path to Pareto Optimal Red Teaming for Large Language Models",
        "authors": [
            "Kai Hu",
            "Abhinav Aggarwal",
            "Mehran Khodabandeh",
            "David Zhang",
            "Eric Hsin",
            "Li Chen",
            "Ankit Jain",
            "Matt Fredrikson",
            "Akash Bharadwaj"
        ],
        "comments": "Socially Responsible and Trustworthy Foundation Models at NeurIPS 2025",
        "subjects": "Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "This paper introduces Jailbreak-Zero, a novel red teaming methodology that shifts the paradigm of Large Language Model (LLM) safety evaluation from a constrained example-based approach to a more expansive and effective policy-based framework. By leveraging an attack LLM to generate a high volume of diverse adversarial prompts and then fine-tuning this attack model with a preference dataset, Jailbreak-Zero achieves Pareto optimality across the crucial objectives of policy coverage, attack strategy diversity, and prompt fidelity to real user inputs. The empirical evidence demonstrates the superiority of this method, showcasing significantly higher attack success rates against both open-source and proprietary models like GPT-40 and Claude 3.5 when compared to existing state-of-the-art techniques. Crucially, Jailbreak-Zero accomplishes this while producing human-readable and effective adversarial prompts with minimal need for human intervention, thereby presenting a more scalable and comprehensive solution for identifying and mitigating the safety vulnerabilities of LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03268",
        "abs_url": "https://arxiv.org/abs/2601.03268",
        "pdf_url": "https://arxiv.org/pdf/2601.03268",
        "title": "WRAVAL -- WRiting Assist eVALuation",
        "authors": [
            "Gabriel Benedict",
            "Matthew Butler",
            "Naved Merchant",
            "Eetu Salama-Laine"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The emergence of Large Language Models (LLMs) has shifted language model evaluation toward reasoning and problem-solving tasks as measures of general intelligence. Small Language Models (SLMs) -- defined here as models under 10B parameters -- typically score 3-4 times lower than LLMs on these metrics. However, we demonstrate that these evaluations fail to capture SLMs' effectiveness in common industrial applications, such as tone modification tasks (e.g., funny, serious, professional). We propose an evaluation framework specifically designed to highlight SLMs' capabilities in non-reasoning tasks where predefined evaluation datasets don't exist. Our framework combines novel approaches in data generation, prompt-tuning, and LLM-based evaluation to demonstrate the potential of task-specific finetuning. This work provides practitioners with tools to effectively benchmark both SLMs and LLMs for practical applications, particularly in edge and private computing scenarios. Our implementation is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03277",
        "abs_url": "https://arxiv.org/abs/2601.03277",
        "pdf_url": "https://arxiv.org/pdf/2601.03277",
        "title": "MixRx: Predicting Drug Combination Interactions with LLMs",
        "authors": [
            "Risha Surana",
            "Cameron Saidock",
            "Hugo Chacon"
        ],
        "comments": "",
        "subjects": "Other Quantitative Biology (q-bio.OT); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "MixRx uses Large Language Models (LLMs) to classify drug combination interactions as Additive, Synergistic, or Antagonistic, given a multi-drug patient history. We evaluate the performance of 4 models, GPT-2, Mistral Instruct 2.0, and the fine-tuned counterparts. Our results showed a potential for such an application, with the Mistral Instruct 2.0 Fine-Tuned model providing an average accuracy score on standard and perturbed datasets of 81.5%. This paper aims to further develop an upcoming area of research that evaluates if LLMs can be used for biological prediction tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03295",
        "abs_url": "https://arxiv.org/abs/2601.03295",
        "pdf_url": "https://arxiv.org/pdf/2601.03295",
        "title": "MetagenBERT: a Transformer-based Architecture using Foundational genomic Large Language Models for novel Metagenome Representation",
        "authors": [
            "Gaspar Roy",
            "Eugeni Belda",
            "Baptiste Hennecart",
            "Yann Chevaleyre",
            "Edi Prifti",
            "Jean-Daniel Zucker"
        ],
        "comments": "",
        "subjects": "Genomics (q-bio.GN); Machine Learning (cs.LG)",
        "abstract": "Metagenomic disease prediction commonly relies on species abundance tables derived from large, incomplete reference catalogs, constraining resolution and discarding valuable information contained in DNA reads. To overcome these limitations, we introduce MetagenBERT, a Transformer based framework that produces end to end metagenome embeddings directly from raw DNA sequences, without taxonomic or functional annotations. Reads are embedded using foundational genomic language models (DNABERT2 and the microbiome specialized DNABERTMS), then aggregated through a scalable clustering strategy based on FAISS accelerated KMeans. Each metagenome is represented as a cluster abundance vector summarizing the distribution of its embedded reads. We evaluate this approach on five benchmark gut microbiome datasets (Cirrhosis, T2D, Obesity, IBD, CRC). MetagenBERT achieves competitive or superior AUC performance relative to species abundance baselines across most tasks. Concatenating both representations further improves prediction, demonstrating complementarity between taxonomic and embedding derived signals. Clustering remains robust when applied to as little as 10% of reads, highlighting substantial redundancy in metagenomes and enabling major computational gains. We additionally introduce MetagenBERT Glob Mcardis, a cross cohort variant trained on the large, phenotypically diverse MetaCardis cohort and transferred to other datasets, retaining predictive signal including for unseen phenotypes, indicating the feasibility of a foundation model for metagenome representation. Robustness analyses (PERMANOVA, PERMDISP, entropy) show consistent separation of different states across subsamples. Overall, MetagenBERT provides a scalable, annotation free representation of metagenomes pointing toward future phenotype aware generalization across heterogeneous cohorts and sequencing technologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03300",
        "abs_url": "https://arxiv.org/abs/2601.03300",
        "pdf_url": "https://arxiv.org/pdf/2601.03300",
        "title": "TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering",
        "authors": [
            "Scott Thornton"
        ],
        "comments": "14 pages, 4 figures. Code and datasets at this https URL",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Large language models remain vulnerable to jailbreak attacks, and single-layer defenses often trade security for usability. We present TRYLOCK, the first defense-in-depth architecture that combines four heterogeneous mechanisms across the inference stack: weight-level safety alignment via DPO, activation-level control via Representation Engineering (RepE) steering, adaptive steering strength selected by a lightweight sidecar classifier, and input canonicalization to neutralize encoding-based bypasses. On Mistral-7B-Instruct evaluated against a 249-prompt attack set spanning five attack families, TRYLOCK achieves 88.0% relative ASR reduction (46.5% to 5.6%), with each layer contributing unique coverage: RepE blocks 36% of attacks that bypass DPO alone, while canonicalization catches 14% of encoding attacks that evade both. We discover a non-monotonic steering phenomenon -- intermediate strength (alpha=1.0) degrades safety below baseline -- and provide mechanistic hypotheses explaining RepE-DPO interference. The adaptive sidecar reduces over-refusal from 60% to 48% while maintaining identical attack defense, demonstrating that security and usability need not be mutually exclusive. We release all components -- trained adapters, steering vectors, sidecar classifier, preference pairs, and complete evaluation methodology -- enabling full reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03319",
        "abs_url": "https://arxiv.org/abs/2601.03319",
        "pdf_url": "https://arxiv.org/pdf/2601.03319",
        "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
        "authors": [
            "Eldad Matmon",
            "Amit Bracha",
            "Noam Rotstein",
            "Ron Kimmel"
        ],
        "comments": "",
        "subjects": "Graphics (cs.GR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03324",
        "abs_url": "https://arxiv.org/abs/2601.03324",
        "pdf_url": "https://arxiv.org/pdf/2601.03324",
        "title": "Bare-Metal Tensor Virtualization: Overcoming the Memory Wall in Edge-AI Inference on ARM64",
        "authors": [
            "Bugra Kilictas",
            "Faruk Alpay"
        ],
        "comments": "14 pages, 2 figures. Code and data available at this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the \"Memory Wall\" the bottleneck where data movement latency outstrips arithmetic throughput. Standard inference runtimes often incur significant overhead through high-level abstractions, dynamic dispatch, and unaligned memory access patterns. In this work, we present a novel \"Virtual Tensor Core\" architecture implemented in software, optimized specifically for ARM64 microarchitectures (Apple Silicon). By bypassing standard library containers in favor of direct memory mapping (mmap) and implementing hand-tuned NEON SIMD kernels, we achieve a form of \"Software-Defined Direct Memory Access (DMA).\" Our proposed Tensor Virtualization Layout (TVL) guarantees 100% cache line utilization for weight matrices, while our zero-copy loader eliminates initialization latency. Experimental results on a 110M parameter model demonstrate a stable throughput of >60 tokens/second on M2 hardware. While proprietary hardware accelerators (e.g., Apple AMX) can achieve higher peak throughput, our architecture provides a fully open, portable, and deterministic reference implementation for studying the memory bottleneck on general-purpose ARM silicon, meeting the 200ms psycholinguistic latency threshold without opaque dependencies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03325",
        "abs_url": "https://arxiv.org/abs/2601.03325",
        "pdf_url": "https://arxiv.org/pdf/2601.03325",
        "title": "On the Identifiability of Regime-Switching Models with Multi-Lag Dependencies",
        "authors": [
            "Carles Balsells-Rodas",
            "Toshiko Matsui",
            "Pedro A.M. Mediano",
            "Yixin Wang",
            "Yingzhen Li"
        ],
        "comments": "See this https URL for code",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Identifiability is central to the interpretability of deep latent variable models, ensuring parameterisations are uniquely determined by the data-generating distribution. However, it remains underexplored for deep regime-switching time series. We develop a general theoretical framework for multi-lag Regime-Switching Models (RSMs), encompassing Markov Switching Models (MSMs) and Switching Dynamical Systems (SDSs). For MSMs, we formulate the model as a temporally structured finite mixture and prove identifiability of both the number of regimes and the multi-lag transitions in a nonlinear-Gaussian setting. For SDSs, we establish identifiability of the latent variables up to permutation and scaling via temporal structure, which in turn yields conditions for identifiability of regime-dependent latent causal graphs (up to regime/node permutations). Our results hold in a fully unsupervised setting through architectural and noise assumptions that are directly enforceable via neural network design. We complement the theory with a flexible variational estimator that satisfies the assumptions and validate the results on synthetic benchmarks. Across real-world datasets from neuroscience, finance, and climate, identifiability leads to more trustworthy interpretability analysis, which is crucial for scientific discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03331",
        "abs_url": "https://arxiv.org/abs/2601.03331",
        "pdf_url": "https://arxiv.org/pdf/2601.03331",
        "title": "MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models",
        "authors": [
            "Yang Shi",
            "Yifeng Xie",
            "Minzhe Guo",
            "Liangsi Lu",
            "Mingxuan Huang",
            "Jingchao Wang",
            "Zhihong Zhu",
            "Boyan Xu",
            "Zhiqi Huang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03368",
        "abs_url": "https://arxiv.org/abs/2601.03368",
        "pdf_url": "https://arxiv.org/pdf/2601.03368",
        "title": "A path to natural language through tokenisation and transformers",
        "authors": [
            "David S. Berman",
            "Alexander G. Stapleton"
        ],
        "comments": "19 pages, 7 figures, 2 tables",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Natural languages exhibit striking regularities in their statistical structure, including notably the emergence of Zipf's and Heaps' laws. Despite this, it remains broadly unclear how these properties relate to the modern tokenisation schemes used in contemporary transformer models. In this note, we analyse the information content (as measured by the Shannon entropy) of various corpora under the assumption of a Zipfian frequency distribution, and derive a closed-form expression for the slot entropy expectation value. We then empirically investigate how byte--pair encoding (BPE) transforms corpus statistics, showing that recursive applications of BPE drive token frequencies toward a Zipfian power law while inducing a characteristic growth pattern in empirical entropy. Utilizing the ability of transformers to learn context dependent token probability distributions, we train language models on corpora tokenised at varying BPE depths, revealing that the model predictive entropies increasingly agree with Zipf-derived predictions as the BPE depth increases. Attention-based diagnostics further indicate that deeper tokenisation reduces local token dependencies, bringing the empirical distribution closer to the weakly dependent (near IID) regime. Together, these results clarify how BPE acts not only as a compression mechanism but also as a statistical transform that reconstructs key informational properties of natural language.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03388",
        "abs_url": "https://arxiv.org/abs/2601.03388",
        "pdf_url": "https://arxiv.org/pdf/2601.03388",
        "title": "Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models",
        "authors": [
            "Zhibo Hu",
            "Chen Wang",
            "Yanfeng Shu",
            "Hye-young Paik",
            "Liming Zhu"
        ],
        "comments": "17 pages, 7 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Earlier research has shown that metaphors influence human's decision making, which raises the question of whether metaphors also influence large language models (LLMs)' reasoning pathways, considering their training data contain a large number of metaphors. In this work, we investigate the problem in the scope of the emergent misalignment problem where LLMs can generalize patterns learned from misaligned content in one domain to another domain. We discover a strong causal relationship between metaphors in training data and the misalignment degree of LLMs' reasoning contents. With interventions using metaphors in pre-training, fine-tuning and re-alignment phases, models' cross-domain misalignment degrees change significantly. As we delve deeper into the causes behind this phenomenon, we observe that there is a connection between metaphors and the activation of global and local latent features of large reasoning models. By monitoring these latent features, we design a detector that predict misaligned content with high accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03397",
        "abs_url": "https://arxiv.org/abs/2601.03397",
        "pdf_url": "https://arxiv.org/pdf/2601.03397",
        "title": "PIVONet: A Physically-Informed Variational Neuro ODE Model for Efficient Advection-Diffusion Fluid Simulation",
        "authors": [
            "Hei Shing Cheung",
            "Qicheng Long",
            "Zhiyue Lin"
        ],
        "comments": "13 pages, 14 figures",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "We present PIVONet (Physically-Informed Variational ODE Neural Network), a unified framework that integrates Neural Ordinary Differential Equations (Neuro-ODEs) with Continuous Normalizing Flows (CNFs) for stochastic fluid simulation and visualization. First, we demonstrate that a physically informed model, parameterized by CNF parameters {\\theta}, can be trained offline to yield an efficient surrogate simulator for a specific fluid system, eliminating the need to simulate the full dynamics explicitly. Second, by introducing a variational model with parameters {\\phi} that captures latent stochasticity in observed fluid trajectories, we model the network output as a variational distribution and optimize a pathwise Evidence Lower Bound (ELBO), enabling stochastic ODE integration that captures turbulence and random fluctuations in fluid motion (advection-diffusion behaviors).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03429",
        "abs_url": "https://arxiv.org/abs/2601.03429",
        "pdf_url": "https://arxiv.org/pdf/2601.03429",
        "title": "DeepLeak: Privacy Enhancing Hardening of Model Explanations Against Membership Leakage",
        "authors": [
            "Firas Ben Hmida",
            "Zain Sbeih",
            "Philemon Hailemariam",
            "Birhanu Eshete"
        ],
        "comments": "17 pages, 6 figures, 8 tables. This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (IEEE SaTML 2026)",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Machine learning (ML) explainability is central to algorithmic transparency in high-stakes settings such as predictive diagnostics and loan approval. However, these same domains require rigorous privacy guaranties, creating tension between interpretability and privacy. Although prior work has shown that explanation methods can leak membership information, practitioners still lack systematic guidance on selecting or deploying explanation techniques that balance transparency with privacy. We present DeepLeak, a system to audit and mitigate privacy risks in post-hoc explanation methods. DeepLeak advances the state-of-the-art in three ways: (1) comprehensive leakage profiling: we develop a stronger explanation-aware membership inference attack (MIA) to quantify how much representative explanation methods leak membership information under default configurations; (2) lightweight hardening strategies: we introduce practical, model-agnostic mitigations, including sensitivity-calibrated noise, attribution clipping, and masking, that substantially reduce membership leakage while preserving explanation utility; and (3) root-cause analysis: through controlled experiments, we pinpoint algorithmic properties (e.g., attribution sparsity and sensitivity) that drive leakage. Evaluating 15 explanation techniques across four families on image benchmarks, DeepLeak shows that default settings can leak up to 74.9% more membership information than previously reported. Our mitigations cut leakage by up to 95% (minimum 46.5%) with only <=3.3% utility loss on average. DeepLeak offers a systematic, reproducible path to safer explainability in privacy-sensitive ML.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03442",
        "abs_url": "https://arxiv.org/abs/2601.03442",
        "pdf_url": "https://arxiv.org/pdf/2601.03442",
        "title": "Provable Acceleration of Distributed Optimization with Local Updates",
        "authors": [
            "Zuang Wang",
            "Yongqiang Wang"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "In conventional distributed optimization, each agent performs a single local update between two communication rounds with its neighbors to synchronize solutions. Inspired by the success of using multiple local updates in federated learning, incorporating local updates into distributed optimization has recently attracted increasing attention. However, unlike federated learning, where multiple local updates can accelerate learning by improving gradient estimation under mini-batch settings, it remains unclear whether similar benefits hold in distributed optimization when gradients are exact. Moreover, existing theoretical results typically require reducing the step size when multiple local updates are employed, which can entirely offset any potential benefit of these additional local updates and obscure their true impact on convergence. In this paper, we focus on the classic DIGing algorithm and leverage the tight performance bounds provided by Performance Estimation Problems (PEP) to show that incorporating local updates can indeed accelerate distributed optimization. To the best of our knowledge, this is the first rigorous demonstration of such acceleration for a broad class of objective functions. Our analysis further reveals that, under an appropriate step size, performing only two local updates is sufficient to achieve the maximal possible improvement, and that additional local updates provide no further gains. Because more updates increase computational cost, these findings offer practical guidance for efficient implementation. Extensive experiments on both synthetic and real-world datasets corroborate the theoretical findings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03453",
        "abs_url": "https://arxiv.org/abs/2601.03453",
        "pdf_url": "https://arxiv.org/pdf/2601.03453",
        "title": "Measures of classification bias derived from sample size analysis",
        "authors": [
            "Ioannis Ivrissimtzis",
            "Shauna Concannon",
            "Matthew Houliston",
            "Graham Roberts"
        ],
        "comments": "9 pages, 3 figures",
        "subjects": "Methodology (stat.ME); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "We propose the use of a simple intuitive principle for measuring algorithmic classification bias: the significance of the differences in a classifier's error rates across the various demographics is inversely commensurate with the sample size required to statistically detect them. That is, if large sample sizes are required to statistically establish biased behavior, the algorithm is less biased, and vice versa. In a simple setting, we assume two distinct demographics, and non-parametric estimates of the error rates on them, e1 and e2, respectively. We use a well-known approximate formula for the sample size of the chi-squared test, and verify some basic desirable properties of the proposed measure. Next, we compare the proposed measure with two other commonly used statistics, the difference e2-e1 and the ratio e2/e1 of the error rates. We establish that the proposed measure is essentially different in that it can rank algorithms for bias differently, and we discuss some of its advantages over the other two measures. Finally, we briefly discuss how some of the desirable properties of the proposed measure emanate from fundamental characteristics of the method, rather than the approximate sample size formula we used, and thus, are expected to hold in more complex settings with more than two demographics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03466",
        "abs_url": "https://arxiv.org/abs/2601.03466",
        "pdf_url": "https://arxiv.org/pdf/2601.03466",
        "title": "Latent Geometry of Taste: Scalable Low-Rank Matrix Factorization",
        "authors": [
            "Joshua Salako"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Scalability and data sparsity remain critical bottlenecks for collaborative filtering on massive interaction datasets. This work investigates the latent geometry of user preferences using the MovieLens 32M dataset, implementing a high-performance, parallelized Alternating Least Squares (ALS) framework. Through extensive hyperparameter optimization, we demonstrate that constrained low-rank models significantly outperform higher dimensional counterparts in generalization, achieving an optimal balance between Root Mean Square Error (RMSE) and ranking precision. We visualize the learned embedding space to reveal the unsupervised emergence of semantic genre clusters, confirming that the model captures deep structural relationships solely from interaction data. Finally, we validate the system's practical utility in a cold-start scenario, introducing a tunable scoring parameter to manage the trade-off between popularity bias and personalized affinity effectively. The codebase for this research can be found here: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03476",
        "abs_url": "https://arxiv.org/abs/2601.03476",
        "pdf_url": "https://arxiv.org/pdf/2601.03476",
        "title": "Online Decision-Making Under Uncertainty for Vehicle-to-Building Systems",
        "authors": [
            "Rishav Sen",
            "Yunuo Zhang",
            "Fangqi Liu",
            "Jose Paolo Talusan",
            "Ava Pettet",
            "Yoshinori Suzue",
            "Ayan Mukhopadhyay",
            "Abhishek Dubey"
        ],
        "comments": "17 pages, 2 figures, 10 tables. Published in the Proceedings of the 16th ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS '25), May 06--09, 2025, Irvine, CA, USA",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Vehicle-to-building (V2B) systems integrate physical infrastructures, such as smart buildings and electric vehicles (EVs) connected to chargers at the building, with digital control mechanisms to manage energy use. By utilizing EVs as flexible energy reservoirs, buildings can dynamically charge and discharge them to optimize energy use and cut costs under time-variable pricing and demand charge policies. This setup leads to the V2B optimization problem, where buildings coordinate EV charging and discharging to minimize total electricity costs while meeting users' charging requirements. However, the V2B optimization problem is challenging because of: (1) fluctuating electricity pricing, which includes both energy charges ($/kWh) and demand charges ($/kW); (2) long planning horizons (typically over 30 days); (3) heterogeneous chargers with varying charging rates, controllability, and directionality (i.e., unidirectional or bidirectional); and (4) user-specific battery levels at departure to ensure user requirements are met. In contrast to existing approaches that often model this setting as a single-shot combinatorial optimization problem, we highlight critical limitations in prior work and instead model the V2B optimization problem as a Markov decision process (MDP), i.e., a stochastic control process. Solving the resulting MDP is challenging due to the large state and action spaces. To address the challenges of the large state space, we leverage online search, and we counter the action space by using domain-specific heuristics to prune unpromising actions. We validate our approach in collaboration with Nissan Advanced Technology Center - Silicon Valley. Using data from their EV testbed, we show that the proposed framework significantly outperforms state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03483",
        "abs_url": "https://arxiv.org/abs/2601.03483",
        "pdf_url": "https://arxiv.org/pdf/2601.03483",
        "title": "CALM: Culturally Self-Aware Language Models",
        "authors": [
            "Lingzhi Shen",
            "Xiaohao Cai",
            "Yunfei Long",
            "Imran Razzak",
            "Guanming Chen",
            "Shoaib Jameel"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "Cultural awareness in language models is the capacity to understand and adapt to diverse cultural contexts. However, most existing approaches treat culture as static background knowledge, overlooking its dynamic and evolving nature. This limitation reduces their reliability in downstream tasks that demand genuine cultural sensitivity. In this work, we introduce CALM, a novel framework designed to endow language models with cultural self-awareness. CALM disentangles task semantics from explicit cultural concepts and latent cultural signals, shaping them into structured cultural clusters through contrastive learning. These clusters are then aligned via cross-attention to establish fine-grained interactions among related cultural features and are adaptively integrated through a Mixture-of-Experts mechanism along culture-specific dimensions. The resulting unified representation is fused with the model's original knowledge to construct a culturally grounded internal identity state, which is further enhanced through self-prompted reflective learning, enabling continual adaptation and self-correction. Extensive experiments conducted on multiple cross-cultural benchmark datasets demonstrate that CALM consistently outperforms state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03533",
        "abs_url": "https://arxiv.org/abs/2601.03533",
        "pdf_url": "https://arxiv.org/pdf/2601.03533",
        "title": "Online Learning with Limited Information in the Sliding Window Model",
        "authors": [
            "Vladimir Braverman",
            "Sumegha Garg",
            "Chen Wang",
            "David P. Woodruff",
            "Samson Zhou"
        ],
        "comments": "SODA 2026",
        "subjects": "Machine Learning (stat.ML); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)",
        "abstract": "Motivated by recent work on the experts problem in the streaming model, we consider the experts problem in the sliding window model. The sliding window model is a well-studied model that captures applications such as traffic monitoring, epidemic tracking, and automated trading, where recent information is more valuable than older data. Formally, we have $n$ experts, $T$ days, the ability to query the predictions of $q$ experts on each day, a limited amount of memory, and should achieve the (near-)optimal regret $\\sqrt{nW}\\text{polylog}(nT)$ regret over any window of the last $W$ days. While it is impossible to achieve such regret with $1$ query, we show that with $2$ queries we can achieve such regret and with only $\\text{polylog}(nT)$ bits of memory. Not only are our algorithms optimal for sliding windows, but we also show for every interval $\\mathcal{I}$ of days that we achieve $\\sqrt{n|\\mathcal{I}|}\\text{polylog}(nT)$ regret with $2$ queries and only $\\text{polylog}(nT)$ bits of memory, providing an exponential improvement on the memory of previous interval regret algorithms. Building upon these techniques, we address the bandit problem in data streams, where $q=1$, achieving $n T^{2/3}\\text{polylog}(T)$ regret with $\\text{polylog}(nT)$ memory, which is the first sublinear regret in the streaming model in the bandit setting with polylogarithmic memory; this can be further improved to the optimal $\\mathcal{O}(\\sqrt{nT})$ regret if the best expert's losses are in a random order.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03546",
        "abs_url": "https://arxiv.org/abs/2601.03546",
        "pdf_url": "https://arxiv.org/pdf/2601.03546",
        "title": "Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict",
        "authors": [
            "Guanyu Chen",
            "Chenxiao Yu",
            "Xiyang Hu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) are increasingly used to simulate decision-making tasks involving personal data sharing, where privacy concerns and prosocial motivations can push choices in opposite directions. Existing evaluations often measure privacy-related attitudes or sharing intentions in isolation, which makes it difficult to determine whether a model's expressed values jointly predict its downstream data-sharing actions as in real human behaviors. We introduce a context-based assessment protocol that sequentially administers standardized questionnaires for privacy attitudes, prosocialness, and acceptance of data sharing within a bounded, history-carrying session. To evaluate value-action alignments under competing attitudes, we use multi-group structural equation modeling (MGSEM) to identify relations from privacy concerns and prosocialness to data sharing. We propose Value-Action Alignment Rate (VAAR), a human-referenced directional agreement metric that aggregates path-level evidence for expected signs. Across multiple LLMs, we observe stable but model-specific Privacy-PSA-AoDS profiles, and substantial heterogeneity in value-action alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03566",
        "abs_url": "https://arxiv.org/abs/2601.03566",
        "pdf_url": "https://arxiv.org/pdf/2601.03566",
        "title": "Provably Convergent Decentralized Optimization over Directed Graphs under Generalized Smoothness",
        "authors": [
            "Yanan Bo",
            "Yongqiang Wang"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Decentralized optimization has become a fundamental tool for large-scale learning systems; however, most existing methods rely on the classical Lipschitz smoothness assumption, which is often violated in problems with rapidly varying gradients. Motivated by this limitation, we study decentralized optimization under the generalized $(L_0, L_1)$-smoothness framework, in which the Hessian norm is allowed to grow linearly with the gradient norm, thereby accommodating rapidly varying gradients beyond classical Lipschitz smoothness. We integrate gradient-tracking techniques with gradient clipping and carefully design the clipping threshold to ensure accurate convergence over directed communication graphs under generalized smoothness. In contrast to existing distributed optimization results under generalized smoothness that require a bounded gradient dissimilarity assumption, our results remain valid even when the gradient dissimilarity is unbounded, making the proposed framework more applicable to realistic heterogeneous data environments. We validate our approach via numerical experiments on standard benchmark datasets, including LIBSVM and CIFAR-10, using regularized logistic regression and convolutional neural networks, demonstrating superior stability and faster convergence over existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03608",
        "abs_url": "https://arxiv.org/abs/2601.03608",
        "pdf_url": "https://arxiv.org/pdf/2601.03608",
        "title": "Shielded RecRL: Explanation Generation for Recommender Systems without Ranking Degradation",
        "authors": [
            "Ansh Tiwari",
            "Ayush Chauhan"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "We introduce Shielded RecRL, a reinforcement learning approach to generate personalized explanations for recommender systems without sacrificing the system's original ranking performance. Unlike prior RLHF-based recommender methods that directly optimize item rankings, our two-tower architecture keeps the recommender's ranking model intact while a language model learns to produce helpful explanations. We design a composite reward signal combining explanation length, content relevance, and coherence, and apply proximal policy optimization (PPO) with a KL-divergence constraint to fine-tune a large language model with only 0.4% of its parameters trainable via LoRA adapters. In experiments on an Amazon Books dataset (approximately 50K interactions in the fantasy and romance genres), Shielded RecRL improved the relative click-through rate (CTR) by 22.5% (1.225x over baseline) while keeping the recommender's item-ranking behavior virtually unchanged. An extensive ablation study confirms that our gradient shielding strategy and reward design effectively balance explanation quality and policy drift. Our results demonstrate that Shielded RecRL enhances user-facing aspects of recommendations through rich, personalized explanations without degrading core recommendation accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03626",
        "abs_url": "https://arxiv.org/abs/2601.03626",
        "pdf_url": "https://arxiv.org/pdf/2601.03626",
        "title": "Learning from Limited Labels: Transductive Graph Label Propagation for Indian Music Analysis",
        "authors": [
            "Parampreet Singh",
            "Akshay Raina",
            "Sayeedul Islam Sheikh",
            "Vipul Arora"
        ],
        "comments": "Published at Journal of Acoustical Society of India, 2025",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG)",
        "abstract": "Supervised machine learning frameworks rely on extensive labeled datasets for robust performance on real-world tasks. However, there is a lack of large annotated datasets in audio and music domains, as annotating such recordings is resource-intensive, laborious, and often require expert domain knowledge. In this work, we explore the use of label propagation (LP), a graph-based semi-supervised learning technique, for automatically labeling the unlabeled set in an unsupervised manner. By constructing a similarity graph over audio embeddings, we propagate limited label information from a small annotated subset to a larger unlabeled corpus in a transductive, semi-supervised setting. We apply this method to two tasks in Indian Art Music (IAM): Raga identification and Instrument classification. For both these tasks, we integrate multiple public datasets along with additional recordings we acquire from Prasar Bharati Archives to perform LP. Our experiments demonstrate that LP significantly reduces labeling overhead and produces higher-quality annotations compared to conventional baseline methods, including those based on pretrained inductive models. These results highlight the potential of graph-based semi-supervised learning to democratize data annotation and accelerate progress in music information retrieval.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03668",
        "abs_url": "https://arxiv.org/abs/2601.03668",
        "pdf_url": "https://arxiv.org/pdf/2601.03668",
        "title": "Discontinuous Galerkin finite element operator network for solving non-smooth PDEs",
        "authors": [
            "Kapil Chawla",
            "Youngjoon Hong",
            "Jae Yong Lee",
            "Sanghyun Lee"
        ],
        "comments": "24 pages, 11 figures",
        "subjects": "Numerical Analysis (math.NA); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We introduce Discontinuous Galerkin Finite Element Operator Network (DG--FEONet), a data-free operator learning framework that combines the strengths of the discontinuous Galerkin (DG) method with neural networks to solve parametric partial differential equations (PDEs) with discontinuous coefficients and non-smooth solutions. Unlike traditional operator learning models such as DeepONet and Fourier Neural Operator, which require large paired datasets and often struggle near sharp features, our approach minimizes the residual of a DG-based weak formulation using the Symmetric Interior Penalty Galerkin (SIPG) scheme. DG-FEONet predicts element-wise solution coefficients via a neural network, enabling data-free training without the need for precomputed input-output pairs. We provide theoretical justification through convergence analysis and validate the model's performance on a series of one- and two-dimensional PDE problems, demonstrating accurate recovery of discontinuities, strong generalization across parameter space, and reliable convergence rates. Our results highlight the potential of combining local discretization schemes with machine learning to achieve robust, singularity-aware operator approximation in challenging PDE settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03671",
        "abs_url": "https://arxiv.org/abs/2601.03671",
        "pdf_url": "https://arxiv.org/pdf/2601.03671",
        "title": "NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models",
        "authors": [
            "Weiqi Liu",
            "Yongliang Miao",
            "Haiyan Zhao",
            "Yanguang Liu",
            "Mengnan Du"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations into atomic semantic components, clusters them into distinct semantic modes, and iteratively refines each explanation using neuron activation feedback. Experiments demonstrate that NeuronScope uncovers hidden polysemanticity and produces explanations with significantly higher activation correlation compared to single-pass baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03679",
        "abs_url": "https://arxiv.org/abs/2601.03679",
        "pdf_url": "https://arxiv.org/pdf/2601.03679",
        "title": "Accounting for Optimal Control in the Sizing of Isolated Hybrid Renewable Energy Systems Using Imitation Learning",
        "authors": [
            "Simon Halvdansson",
            "Lucas Ferreira Bernardino",
            "Brage Rugstad Knudsen"
        ],
        "comments": "11 pages, 9 figures",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Decarbonization of isolated or off-grid energy systems through phase-in of large shares of intermittent solar or wind generation requires co-installation of energy storage or continued use of existing fossil dispatchable power sources to balance supply and demand. The effective CO2 emission reduction depends on the relative capacity of the energy storage and renewable sources, the stochasticity of the renewable generation, and the optimal control or dispatch of the isolated energy system. While the operations of the energy storage and dispatchable sources may impact the optimal sizing of the system, it is challenging to account for the effect of finite horizon, optimal control at the stage of system sizing. Here, we present a flexible and computationally efficient sizing framework for energy storage and renewable capacity in isolated energy systems, accounting for uncertainty in the renewable generation and the optimal feedback control. To this end, we implement an imitation learning approach to stochastic neural model predictive control (MPC) which allows us to relate the battery storage and wind peak capacities to the emissions reduction and investment costs while accounting for finite horizon, optimal control. Through this approach, decision makers can evaluate the effective emission reduction and costs of different storage and wind capacities at any price point while accounting for uncertainty in the renewable generation with limited foresight. We evaluate the proposed sizing framework on a case study of an offshore energy system with a gas turbine, a wind farm and a battery energy storage system (BESS). In this case, we find a nonlinear, nontrivial relationship between the investment costs and reduction in gas usage relative to the wind and BESS capacities, emphasizing the complexity and importance of accounting for optimal control in the design of isolated energy systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03786",
        "abs_url": "https://arxiv.org/abs/2601.03786",
        "pdf_url": "https://arxiv.org/pdf/2601.03786",
        "title": "Compact Example-Based Explanations for Language Models",
        "authors": [
            "Loris Schoenegger",
            "Benjamin Roth"
        ],
        "comments": "8 pages",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Training data influence estimation methods quantify the contribution of training documents to a model's output, making them a promising source of information for example-based explanations. As humans cannot interpret thousands of documents, only a small subset of the training data can be presented as an explanation. Although the choice of which documents to include directly affects explanation quality, previous evaluations of such systems have largely ignored any selection strategies. To address this, we propose a novel selection relevance score, a retraining-free metric that quantifies how useful a set of examples is for explaining a model's output. We validate this score through fine-tuning experiments, confirming that it can predict whether a set of examples supports or undermines the model's predictions. Using this metric, we further show that common selection strategies often underperform random selection. Motivated by this finding, we propose a strategy that balances influence and representativeness, enabling better use of selection budgets than naively selecting the highest-ranking examples.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03801",
        "abs_url": "https://arxiv.org/abs/2601.03801",
        "pdf_url": "https://arxiv.org/pdf/2601.03801",
        "title": "Physically Consistent Machine Learning for Melting Temperature Prediction of Refractory High-Entropy Alloys",
        "authors": [
            "Mohd Hasnain"
        ],
        "comments": "6 Pages, 3 figures, code available at Github",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Predicting the melting temperature (Tm) of multi-component and high-entropy alloys (HEAs) is critical for high-temperature applications but computationally expensive using traditional CALPHAD or DFT methods. In this work, we develop a gradient-boosted decision tree (XGBoost) model to predict Tm for complex alloys based on elemental properties. To ensure physical consistency, we address the issue of data leakage by excluding temperature-dependent thermodynamic descriptors (such as Gibbs free energy of mixing) and instead rely on physically motivated elemental features. The optimized model achieves a coefficient of determination (R2) of 0.948 and a Mean Squared Error (MSE) of 9928 which is about 5% relative error for HEAs on a validation set of approximately 1300 compositions. Crucially, we validate the model using the Valence Electron Concentration (VEC) rule. Without explicit constraints during training, the model successfully captures the known stability transition between BCC and FCC phases at a VEC of approximately 6.87. These results demonstrate that data-driven models, when properly feature-engineered, can capture fundamental metallurgical principles for rapid alloy screening.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03811",
        "abs_url": "https://arxiv.org/abs/2601.03811",
        "pdf_url": "https://arxiv.org/pdf/2601.03811",
        "title": "EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging",
        "authors": [
            "Jan Tagscherer",
            "Sarah de Boer",
            "Lena Philipp",
            "Fennie van der Graaf",
            "DrÃ© Peeters",
            "Joeran Bosma",
            "Lars Leijten",
            "Bogdan Obreja",
            "Ewoud Smit",
            "Alessa Hering"
        ],
        "comments": "Accepted at BVM 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03825",
        "abs_url": "https://arxiv.org/abs/2601.03825",
        "pdf_url": "https://arxiv.org/pdf/2601.03825",
        "title": "Beyond Physical Labels: Redefining Domains for Robust WiFi-based Gesture Recognition",
        "authors": [
            "Xiang Zhang",
            "Huan Yan",
            "Jinyang Huang",
            "Bin Liu",
            "Yuanhao Feng",
            "Jianchun Liu",
            "Meng Li",
            "Fusang Zhang",
            "Zhi Liu"
        ],
        "comments": "Accepted by IMWUT/Ubicomp 2026",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "In this paper, we propose GesFi, a novel WiFi-based gesture recognition system that introduces WiFi latent domain mining to redefine domains directly from the data itself. GesFi first processes raw sensing data collected from WiFi receivers using CSI-ratio denoising, Short-Time Fast Fourier Transform, and visualization techniques to generate standardized input representations. It then employs class-wise adversarial learning to suppress gesture semantic and leverages unsupervised clustering to automatically uncover latent domain factors responsible for distributional shifts. These latent domains are then aligned through adversarial learning to support robust cross-domain generalization. Finally, the system is applied to the target environment for robust gesture inference. We deployed GesFi under both single-pair and multi-pair settings using commodity WiFi transceivers, and evaluated it across multiple public datasets and real-world environments. Compared to state-of-the-art baselines, GesFi achieves up to 78% and 50% performance improvements over existing adversarial methods, and consistently outperforms prior generalization approaches across most cross-domain tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03853",
        "abs_url": "https://arxiv.org/abs/2601.03853",
        "pdf_url": "https://arxiv.org/pdf/2601.03853",
        "title": "From No-Regret to Strategically Robust Learning in Repeated Auctions",
        "authors": [
            "Junyao Zhao"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Theoretical Economics (econ.TH)",
        "abstract": "In Bayesian single-item auctions, a monotone bidding strategy--one that prescribes a higher bid for a higher value type--can be equivalently represented as a partition of the quantile space into consecutive intervals corresponding to increasing bids. Kumar et al. (2024) prove that agile online gradient descent (OGD), when used to update a monotone bidding strategy through its quantile representation, is strategically robust in repeated first-price auctions: when all bidders employ agile OGD in this way, the auctioneer's average revenue per round is at most the revenue of Myerson's optimal auction, regardless of how she adjusts the reserve price over time. In this work, we show that this strategic robustness guarantee is not unique to agile OGD or to the first-price auction: any no-regret learning algorithm, when fed gradient feedback with respect to the quantile representation, is strategically robust, even if the auction format changes every round, provided the format satisfies allocation monotonicity and voluntary participation. In particular, the multiplicative weights update (MWU) algorithm simultaneously achieves the optimal regret guarantee and the best-known strategic robustness guarantee. At a technical level, our results are established via a simple relation that bridges Myerson's auction theory and standard no-regret learning theory. This showcases the potential of translating standard regret guarantees into strategic robustness guarantees for specific games, without explicitly minimizing any form of swap regret.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03892",
        "abs_url": "https://arxiv.org/abs/2601.03892",
        "pdf_url": "https://arxiv.org/pdf/2601.03892",
        "title": "Lightweight and perceptually-guided voice conversion for electro-laryngeal speech",
        "authors": [
            "Benedikt Mayrhofer",
            "Franz Pernkopf",
            "Philipp Aichinger",
            "Martin HagmÃ¼ller"
        ],
        "comments": "5 pages, 5 figures. Audio samples available at this https URL Preprint submitted to ICASSP",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG)",
        "abstract": "Electro-laryngeal (EL) speech is characterized by constant pitch, limited prosody, and mechanical noise, reducing naturalness and intelligibility. We propose a lightweight adaptation of the state-of-the-art StreamVC framework to this setting by removing pitch and energy modules and combining self-supervised pretraining with supervised fine-tuning on parallel EL and healthy (HE) speech data, guided by perceptual and intelligibility losses. Objective and subjective evaluations across different loss configurations confirm their influence: the best model variant, based on WavLM features and human-feedback predictions (+WavLM+HF), drastically reduces character error rate (CER) of EL inputs, raises naturalness mean opinion score (nMOS) from 1.1 to 3.3, and consistently narrows the gap to HE ground-truth speech in all evaluated metrics. These findings demonstrate the feasibility of adapting lightweight voice conversion architectures to EL voice rehabilitation while also identifying prosody generation and intelligibility improvements as the main remaining bottlenecks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03910",
        "abs_url": "https://arxiv.org/abs/2601.03910",
        "pdf_url": "https://arxiv.org/pdf/2601.03910",
        "title": "An Algebraic Representation Theorem for Linear GENEOs in Geometric Machine Learning",
        "authors": [
            "Francesco Conti",
            "Patrizio Frosini",
            "Nicola Quercioli"
        ],
        "comments": "",
        "subjects": "Representation Theory (math.RT); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Geometric and Topological Deep Learning are rapidly growing research areas that enhance machine learning through the use of geometric and topological structures. Within this framework, Group Equivariant Non-Expansive Operators (GENEOs) have emerged as a powerful class of operators for encoding symmetries and designing efficient, interpretable neural architectures. Originally introduced in Topological Data Analysis, GENEOs have since found applications in Deep Learning as tools for constructing equivariant models with reduced parameter complexity. GENEOs provide a unifying framework bridging Geometric and Topological Deep Learning and include the operator computing persistence diagrams as a special case. Their theoretical foundations rely on group actions, equivariance, and compactness properties of operator spaces, grounding them in algebra and geometry while enabling both mathematical rigor and practical relevance. While a previous representation theorem characterized linear GENEOs acting on data of the same type, many real-world applications require operators between heterogeneous data spaces. In this work, we address this limitation by introducing a new representation theorem for linear GENEOs acting between different perception pairs, based on generalized T-permutant measures. Under mild assumptions on the data domains and group actions, our result provides a complete characterization of such operators. We also prove the compactness and convexity of the space of linear GENEOs. We further demonstrate the practical impact of this theory by applying the proposed framework to improve the performance of autoencoders, highlighting the relevance of GENEOs in modern machine learning applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03930",
        "abs_url": "https://arxiv.org/abs/2601.03930",
        "pdf_url": "https://arxiv.org/pdf/2601.03930",
        "title": "Bayes-PD: Exploring a Sequence to Binding Bayesian Neural Network model trained on Phage Display data",
        "authors": [
            "Ilann Amiaud-Plachy",
            "Michael Blank",
            "Oliver Bent",
            "Sebastien Boyer"
        ],
        "comments": "",
        "subjects": "Populations and Evolution (q-bio.PE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Phage display is a powerful laboratory technique used to study the interactions between proteins and other molecules, whether other proteins, peptides, DNA or RNA. The under-utilisation of this data in conjunction with deep learning models for protein design may be attributed to; high experimental noise levels; the complex nature of data pre-processing; and difficulty interpreting these experimental results. In this work, we propose a novel approach utilising a Bayesian Neural Network within a training loop, in order to simulate the phage display experiment and its associated noise. Our goal is to investigate how understanding the experimental noise and model uncertainty can enable the reliable application of such models to reliably interpret phage display experiments. We validate our approach using actual binding affinity measurements instead of relying solely on proxy values derived from 'held-out' phage display rounds.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03946",
        "abs_url": "https://arxiv.org/abs/2601.03946",
        "pdf_url": "https://arxiv.org/pdf/2601.03946",
        "title": "Provably Finding a Hidden Dense Submatrix among Many Planted Dense Submatrices via Convex Programming",
        "authors": [
            "Valentine Olanubi",
            "Phineas Agar",
            "Brendan Ames"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "We consider the densest submatrix problem, which seeks the submatrix of fixed size of a given binary matrix that contains the most nonzero entries. This problem is a natural generalization of fundamental problems in combinatorial optimization, e.g., the densest subgraph, maximum clique, and maximum edge biclique problems, and has wide application the study of complex networks. Much recent research has focused on the development of sufficient conditions for exact solution of the densest submatrix problem via convex relaxation. The vast majority of these sufficient conditions establish identification of the densest submatrix within a graph containing exactly one large dense submatrix hidden by noise. The assumptions of these underlying models are not observed in real-world networks, where the data may correspond to a matrix containing many dense submatrices of varying sizes. We extend and generalize these results to the more realistic setting where the input matrix may contain \\emph{many} large dense subgraphs. Specifically, we establish sufficient conditions under which we can expect to solve the densest submatrix problem in polynomial time for random input matrices sampled from a generalization of the stochastic block model. Moreover, we also provide sufficient conditions for perfect recovery under a deterministic adversarial. Numerical experiments involving randomly generated problem instances and real-world collaboration and communication networks are used empirically to verify the theoretical phase-transitions to perfect recovery given by these sufficient conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.03988",
        "abs_url": "https://arxiv.org/abs/2601.03988",
        "pdf_url": "https://arxiv.org/pdf/2601.03988",
        "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures",
        "authors": [
            "Nicolas Lacroix",
            "Mireille Blay-Fornarino",
            "SÃ©bastien Mosser",
            "Frederic Precioso"
        ],
        "comments": "SANER 2026 Registered Report",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions. Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices. Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04083",
        "abs_url": "https://arxiv.org/abs/2601.04083",
        "pdf_url": "https://arxiv.org/pdf/2601.04083",
        "title": "Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning",
        "authors": [
            "Marvin Illian",
            "Ramin Khalili",
            "Antonio A. de A. Rocha",
            "Lin Wang"
        ],
        "comments": "11 pages, 12 figures",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "The widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based on operator experience and rarely adapted to dynamic network conditions. In this work, we ask: Can an agent automatically learn and adapt cell (re)selection parameters to consistently improve network performance? We present a reinforcement learning (RL)-based framework called CellPilot that adaptively tunes cell (re)selection parameters by learning spatiotemporal patterns of mobile network dynamics. Our study with real-world data demonstrates that even a lightweight RL agent can outperform conventional heuristic reconfigurations by up to 167%, while generalizing effectively across different network scenarios. These results indicate that data-driven approaches can significantly improve cell (re)selection configurations and enhance mobile network performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04104",
        "abs_url": "https://arxiv.org/abs/2601.04104",
        "pdf_url": "https://arxiv.org/pdf/2601.04104",
        "title": "Equivariant Neural Networks for Force-Field Models of Lattice Systems",
        "authors": [
            "Yunhao Fan",
            "Gia-Wei Chern"
        ],
        "comments": "13 pages, 6 figures",
        "subjects": "Strongly Correlated Electrons (cond-mat.str-el); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Machine-learning (ML) force fields enable large-scale simulations with near-first-principles accuracy at substantially reduced computational cost. Recent work has extended ML force-field approaches to adiabatic dynamical simulations of condensed-matter lattice models with coupled electronic and structural or magnetic degrees of freedom. However, most existing formulations rely on hand-crafted, symmetry-aware descriptors, whose construction is often system-specific and can hinder generality and transferability across different lattice Hamiltonians. Here we introduce a symmetry-preserving framework based on equivariant neural networks (ENNs) that provides a general, data-driven mapping from local configurations of dynamical variables to the associated on-site forces in a lattice Hamiltonian. In contrast to ENN architectures developed for molecular systems -- where continuous Euclidean symmetries dominate -- our approach aims to embed the discrete point-group and internal symmetries intrinsic to lattice models directly into the neural-network representation of the force field. As a proof of principle, we construct an ENN-based force-field model for the adiabatic dynamics of the Holstein Hamiltonian on a square lattice, a canonical system for electron-lattice physics. The resulting ML-enabled large-scale dynamical simulations faithfully capture mesoscale evolution of the symmetry-breaking phase, illustrating the utility of lattice-equivariant architectures for linking microscopic electronic processes to emergent dynamical behavior in condensed-matter lattice systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04120",
        "abs_url": "https://arxiv.org/abs/2601.04120",
        "pdf_url": "https://arxiv.org/pdf/2601.04120",
        "title": "A Single-Loop Bilevel Deep Learning Method for Optimal Control of Obstacle Problems",
        "authors": [
            "Yongcun Song",
            "Shangzhi Zeng",
            "Jin Zhang",
            "Lvgang Zhang"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Optimal control of obstacle problems arises in a wide range of applications and is computationally challenging due to its nonsmoothness, nonlinearity, and bilevel structure. Classical numerical approaches rely on mesh-based discretization and typically require solving a sequence of costly subproblems. In this work, we propose a single-loop bilevel deep learning method, which is mesh-free, scalable to high-dimensional and complex domains, and avoids repeated solution of discretized subproblems. The method employs constraint-embedding neural networks to approximate the state and control and preserves the bilevel structure. To train the neural networks efficiently, we propose a Single-Loop Stochastic First-Order Bilevel Algorithm (S2-FOBA), which eliminates nested optimization and does not rely on restrictive lower-level uniqueness assumptions. We analyze the convergence behavior of S2-FOBA under mild assumptions. Numerical experiments on benchmark examples, including distributed and obstacle control problems with regular and irregular obstacles on complex domains, demonstrate that the proposed method achieves satisfactory accuracy while reducing computational cost compared to classical numerical methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04149",
        "abs_url": "https://arxiv.org/abs/2601.04149",
        "pdf_url": "https://arxiv.org/pdf/2601.04149",
        "title": "A Theoretical and Empirical Taxonomy of Imbalance in Binary Classification",
        "authors": [
            "Rose Yvette Bandolo Essomba",
            "Ernest FokouÃ©"
        ],
        "comments": "24 pages, 10 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Class imbalance significantly degrades classification performance, yet its effects are rarely analyzed from a unified theoretical perspective. We propose a principled framework based on three fundamental scales: the imbalance coefficient $\\eta$, the sample--dimension ratio $\\kappa$, and the intrinsic separability $\\Delta$. Starting from the Gaussian Bayes classifier, we derive closed-form Bayes errors and show how imbalance shifts the discriminant boundary, yielding a deterioration slope that predicts four regimes: Normal, Mild, Extreme, and Catastrophic. Using a balanced high-dimensional genomic dataset, we vary only $\\eta$ while keeping $\\kappa$ and $\\Delta$ fixed. Across parametric and non-parametric models, empirical degradation closely follows theoretical predictions: minority Recall collapses once $\\log(\\eta)$ exceeds $\\Delta\\sqrt{\\kappa}$, Precision increases asymmetrically, and F1-score and PR-AUC decline in line with the predicted regimes. These results show that the triplet $(\\eta,\\kappa,\\Delta)$ provides a model-agnostic, geometrically grounded explanation of imbalance-induced deterioration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2026-01-08",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-08?abs=True",
        "arxiv_id": "2601.04157",
        "abs_url": "https://arxiv.org/abs/2601.04157",
        "pdf_url": "https://arxiv.org/pdf/2601.04157",
        "title": "FLEx: Language Modeling with Few-shot Language Explanations",
        "authors": [
            "Adar Avsian",
            "Christopher Richardson",
            "Anirudh Sundar",
            "Larry Heck"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\\textbf{F}$ew-shot $\\textbf{L}$anguage $\\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\\% of CoT's remaining errors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]