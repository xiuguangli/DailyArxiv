[
    {
        "order": 1,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02923",
        "abs_url": "https://arxiv.org/abs/2511.02923",
        "pdf_url": "https://arxiv.org/pdf/2511.02923",
        "title": "Cropland Mapping using Geospatial Embeddings",
        "authors": [
            "Ivan Zvonkov",
            "Gabriel Tseng",
            "Inbal Becker-Reshef",
            "Hannah Kerner"
        ],
        "comments": "8 pages, 11 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate and up-to-date land cover maps are essential for understanding land use change, a key driver of climate change. Geospatial embeddings offer a more efficient and accessible way to map landscape features, yet their use in real-world mapping applications remains underexplored. In this work, we evaluated the utility of geospatial embeddings for cropland mapping in Togo. We produced cropland maps using embeddings from Presto and AlphaEarth. Our findings show that geospatial embeddings can simplify workflows, achieve high-accuracy cropland classification and ultimately support better assessments of land use change and its climate impacts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02933",
        "abs_url": "https://arxiv.org/abs/2511.02933",
        "pdf_url": "https://arxiv.org/pdf/2511.02933",
        "title": "Generative Hints",
        "authors": [
            "Andy Dimnaku",
            "Abdullah Yusuf KavranoÄŸlu",
            "Yaser Abu-Mostafa"
        ],
        "comments": "13 pages, 9 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02946",
        "abs_url": "https://arxiv.org/abs/2511.02946",
        "pdf_url": "https://arxiv.org/pdf/2511.02946",
        "title": "ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology",
        "authors": [
            "Srikumar Sastry",
            "Subash Khanal",
            "Aayush Dhakal",
            "Jiayu Lin",
            "Dan Cher",
            "Phoenix Jarosz",
            "Nathan Jacobs"
        ],
        "comments": "21 pages, 16 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce ProM3E, a probabilistic masked multimodal embedding model for any-to-any generation of multimodal representations for ecology. ProM3E is based on masked modality reconstruction in the embedding space, learning to infer missing modalities given a few context modalities. By design, our model supports modality inversion in the embedding space. The probabilistic nature of our model allows us to analyse the feasibility of fusing various modalities for given downstream tasks, essentially learning what to fuse. Using these features of our model, we propose a novel cross-modal retrieval approach that mixes inter-modal and intra-modal similarities to achieve superior performance across all retrieval tasks. We further leverage the hidden representation from our model to perform linear probing tasks and demonstrate the superior representation learning capability of our model. All our code, datasets and model will be released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02953",
        "abs_url": "https://arxiv.org/abs/2511.02953",
        "pdf_url": "https://arxiv.org/pdf/2511.02953",
        "title": "EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation",
        "authors": [
            "Sadiq Layi Macaulay",
            "Nimet Kaygusuz",
            "Simon Hadfield"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02992",
        "abs_url": "https://arxiv.org/abs/2511.02992",
        "pdf_url": "https://arxiv.org/pdf/2511.02992",
        "title": "Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification",
        "authors": [
            "Mikhael Djajapermana",
            "Moritz Reiber",
            "Daniel Mueller-Gritschneder",
            "Ulf Schlichtmann"
        ],
        "comments": "Presented at ITEM workshop co-located with ECML PKDD 2024, Vilnius LT",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02996",
        "abs_url": "https://arxiv.org/abs/2511.02996",
        "pdf_url": "https://arxiv.org/pdf/2511.02996",
        "title": "SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics",
        "authors": [
            "Ailar Mahdizadeh",
            "Puria Azadi Moghadam",
            "Xiangteng He",
            "Shahriar Mirabbasi",
            "Panos Nasiopoulos",
            "Leonid Sigal"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-language models (VLMs) have demonstrated strong cross-modal capabilities, yet most work remains limited to 2D data and assumes binary supervision (i.e., positive vs. negative pairs), overlooking the continuous and structured dependencies present in volumetric data such as CT. Existing approaches often treat volumetric scans as independent 2D slices, compromising spatial coherence and underutilizing rich clinical semantics. We propose SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework that integrates (i) volumetric spatial semantics to preserve anatomical structure and (ii) domain-aware, knowledge-infused semantics (e.g., radiological ontologies) to guide alignment. This yields structurally consistent and semantically grounded representations under limited supervision, demonstrating strong cross-task transferability (retrieval, report generation, and classification), and cross-domain generalizability with consistent gains without further fine-tuning. In particular, compared to the previous state of the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval, improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an out-of-domain external dataset, we observe consistent gains, indicating the cross-task and cross-domain generalization ability of SCALE-VLP.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03004",
        "abs_url": "https://arxiv.org/abs/2511.03004",
        "pdf_url": "https://arxiv.org/pdf/2511.03004",
        "title": "Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning",
        "authors": [
            "Dakota Hester",
            "Vitor S. Martins",
            "Lucas B. Ferreira",
            "Thainara M. A. Lima"
        ],
        "comments": "25 pages, 11 figures. Submitted in Science of Remote Sensing",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep learning semantic segmentation methods have shown promising performance for very high 1-m resolution land cover classification, but the challenge of collecting large volumes of representative training data creates a significant barrier to widespread adoption of such models for meter-scale land cover mapping over large areas. In this study, we present a novel label-efficient approach for statewide 1-m land cover classification using only 1,000 annotated reference image patches with self-supervised deep learning. We use the \"Bootstrap Your Own Latent\" pre-training strategy with a large amount of unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to pre-train a ResNet-101 convolutional encoder. The learned encoder weights were subsequently transferred into multiple deep semantic segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then fine-tuned using very small training dataset sizes with cross-validation (250, 500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall accuracy and 75.58% macro F1 score using an ensemble of the best performing U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more than 123 billion pixels over the state of Mississippi, USA. Detailed qualitative and quantitative analysis revealed accurate mapping of open water and forested areas, while highlighting challenges in accurate delineation between cropland, herbaceous, and barren land cover types. These results show that self-supervised learning is an effective strategy for reducing the need for large volumes of manually annotated data, directly addressing a major limitation to high spatial resolution land cover mapping at scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03014",
        "abs_url": "https://arxiv.org/abs/2511.03014",
        "pdf_url": "https://arxiv.org/pdf/2511.03014",
        "title": "A Foundation Model for Brain MRI with Dynamic Modality Integration",
        "authors": [
            "Minh Sao Khue Luu",
            "Bair N. Tuchinov"
        ],
        "comments": "Preliminary work; results ongoing",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present a foundation model for brain MRI that can work with different combinations of imaging sequences. The model uses one encoder with learnable modality embeddings, conditional layer normalization, and a masked autoencoding objective that accounts for missing modalities. A variance-covariance regularizer is applied to stabilize feature learning and improve representation diversity. This design removes the need for separate models for each modality and allows the network to adapt when some sequences are missing or unseen. It is trained on about 60,000 multi-center MRIs using self-supervised reconstruction and modality imputation to learn flexible representations. A learnable modality embedding guides feature extraction so the encoder can adjust to different inputs. We describe our planned evaluation on brain tumor and multiple sclerosis segmentation, as well as lesion classification, under various modality settings. Preliminary results show that the method works feasibly, and further experiments are planned to study its performance in more detail. All code and pretrained models are available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03019",
        "abs_url": "https://arxiv.org/abs/2511.03019",
        "pdf_url": "https://arxiv.org/pdf/2511.03019",
        "title": "SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment",
        "authors": [
            "Wenbo Lu"
        ],
        "comments": "Capstone Paper",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03053",
        "abs_url": "https://arxiv.org/abs/2511.03053",
        "pdf_url": "https://arxiv.org/pdf/2511.03053",
        "title": "From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth",
        "authors": [
            "Ziyang Xu",
            "Olaf Wysocki",
            "Christoph Holst"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning (MLS) point clouds in many high-precision applications such as Scan-to-BIM, deformation analysis, and 3D modeling. However, obtaining the ground truth (GT) for evaluation is often costly and infeasible in many real-world applications. To reduce this long-standing reliance on GT in uncertainty evaluation research, this study presents a learning-based framework for MLS point clouds that integrates optimal neighborhood estimation with geometric feature extraction. Experiments on a real-world dataset show that the proposed framework is feasible and the XGBoost model delivers fully comparable accuracy to Random Forest while achieving substantially higher efficiency (about 3 times faster), providing initial evidence that geometric features can be used to predict point-level uncertainty quantified by the C2C distance. In summary, this study shows that MLS point clouds' uncertainty is learnable, offering a novel learning-based viewpoint towards uncertainty evaluation research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03093",
        "abs_url": "https://arxiv.org/abs/2511.03093",
        "pdf_url": "https://arxiv.org/pdf/2511.03093",
        "title": "A Plug-and-Play Framework for Volumetric Light-Sheet Image Reconstruction",
        "authors": [
            "Yi Gong",
            "Xinyuan Zhang",
            "Jichen Chai",
            "Yichen Ding",
            "Yifei Lou"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Numerical Analysis (math.NA)",
        "abstract": "Cardiac contraction is a rapid, coordinated process that unfolds across three-dimensional tissue on millisecond timescales. Traditional optical imaging is often inadequate for capturing dynamic cellular structure in the beating heart because of a fundamental trade-off between spatial and temporal resolution. To overcome these limitations, we propose a high-performance computational imaging framework that integrates Compressive Sensing (CS) with Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The system performs compressed acquisition of fluorescence signals via random binary mask coding using a Digital Micromirror Device (DMD). We propose a Plug-and-Play (PnP) framework, solved using the alternating direction method of multipliers (ADMM), which flexibly incorporates advanced denoisers, including Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in dynamic imaging, we further introduce temporal regularization enforcing smoothness between adjacent z-slices. Experimental results on zebrafish heart imaging under high compression ratios demonstrate that the proposed method successfully reconstructs cellular structures with excellent denoising performance and image clarity, validating the effectiveness and robustness of our algorithm in real-world high-speed, low-light biological imaging scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03098",
        "abs_url": "https://arxiv.org/abs/2511.03098",
        "pdf_url": "https://arxiv.org/pdf/2511.03098",
        "title": "ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly",
        "authors": [
            "Miftahur Rahman",
            "Samuel Adebayo",
            "Dorian A. Acevedo-Mejia",
            "David Hester",
            "Daniel McPolin",
            "Karen Rafferty",
            "Debra F. Laefer"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "The Intermeshed Steel Connection (ISC) system, when paired with robotic manipulators, can accelerate steel-frame assembly and improve worker safety by eliminating manual assembly. Dependable perception is one of the initial stages for ISC-aware robots. However, this is hampered by the absence of a dedicated image corpus, as collecting photographs on active construction sites is logistically difficult and raises safety and privacy concerns. In response, we introduce ISC-Perception, the first hybrid dataset expressly designed for ISC component detection. It blends procedurally rendered CAD images, game-engine photorealistic scenes, and a limited, curated set of real photographs, enabling fully automatic labelling of the synthetic portion. We explicitly account for all human effort to produce the dataset, including simulation engine and scene setup, asset preparation, post-processing scripts and quality checks; our total human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for manual labelling at 60,s per image (-81.7%). A manual pilot on a representative image with five instances of ISC members took 60,s (maximum 80,s), anchoring the manual baseline. Detectors trained on ISC-Perception achieved a mean Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we report mAP@0.50/mAP@[0.50:0.95] of 0.943/0.823. By bridging the data gap for construction-robotics perception, ISC-Perception facilitates rapid development of custom object detectors and is freely available for research and industrial use upon request.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03099",
        "abs_url": "https://arxiv.org/abs/2511.03099",
        "pdf_url": "https://arxiv.org/pdf/2511.03099",
        "title": "DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs",
        "authors": [
            "Yiyi Miao",
            "Taoyu Wu",
            "Tong Chen",
            "Sihao Li",
            "Ji Jiang",
            "Youpeng Yang",
            "Angelos Stefanidis",
            "Limin Yu",
            "Jionglong Su"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In orthodontic treatment, particularly within telemedicine contexts, observing patients' dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03120",
        "abs_url": "https://arxiv.org/abs/2511.03120",
        "pdf_url": "https://arxiv.org/pdf/2511.03120",
        "title": "Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning",
        "authors": [
            "Botong.Zhao",
            "Xubin.Wang",
            "Shujing.Lyu",
            "Yue.Lu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Integrated circuit manufacturing is highly complex, comprising hundreds of process steps. Defects can arise at any stage, causing yield loss and ultimately degrading product reliability. Supervised methods require extensive human annotation and struggle with emergent categories and rare, data scarce defects. Clustering-based unsupervised methods often exhibit unstable performance due to missing priors. We propose IC DefectNCD, a support set free framework that leverages Image Intrinsic Priors in IC SEM images for defect detection and novel class discovery. We first develop Self Normal Information Guided IC Defect Detection, aggregating representative normal features via a learnable normal information extractor and using reconstruction residuals to coarsely localize defect regions. To handle saliency variations across defects, we introduce an adaptive binarization strategy that produces stable subimages focused on core defective areas. Finally, we design Self Defect Information Guided IC Defect Classification, which incorporates a soft mask guided attention mechanism to inject spatial defect priors into the teacher student model. This enhances sensitivity to defective regions, suppresses background interference, and enables recognition and classification of unseen defects. We validate the approach on a real world dataset spanning three key fabrication stages and covering 15 defect types. Experiments demonstrate robust performance on both defect detection and unseen defect classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03126",
        "abs_url": "https://arxiv.org/abs/2511.03126",
        "pdf_url": "https://arxiv.org/pdf/2511.03126",
        "title": "Accelerating Physical Property Reasoning for Augmented Visual Cognition",
        "authors": [
            "Hongbo Lan",
            "Zhenlin An",
            "Haoyu Li",
            "Vaibhav Singh",
            "Longfei Shangguan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
        "abstract": "This paper introduces \\sysname, a system that accelerates vision-guided physical property reasoning to enable augmented visual cognition. \\sysname minimizes the run-time latency of this reasoning pipeline through a combination of both algorithmic and systematic optimizations, including rapid geometric 3D reconstruction, efficient semantic feature fusion, and parallel view encoding. Through these simple yet effective optimizations, \\sysname reduces the end-to-end latency of this reasoning pipeline from 10--20 minutes to less than 6 seconds. A head-to-head comparison on the ABO dataset shows that \\sysname achieves this 62.9$\\times$--287.2$\\times$ speedup while not only reaching on-par (and sometimes slightly better) object-level physical property estimation accuracy(e.g. mass), but also demonstrating superior performance in material segmentation and voxel-level inference than two SOTA baselines. We further combine gaze-tracking with \\sysname to localize the object of interest in cluttered, real-world environments, streamlining the physical property reasoning on smart glasses. The case study with Meta Aria Glasses conducted at an IKEA furniture store demonstrates that \\sysname achives consistently high performance compared to controlled captures, providing robust property estimations even with fewer views in real-world scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03132",
        "abs_url": "https://arxiv.org/abs/2511.03132",
        "pdf_url": "https://arxiv.org/pdf/2511.03132",
        "title": "Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response",
        "authors": [
            "Thomas Manzini",
            "Priyankari Perali",
            "Robin R. Murphy"
        ],
        "comments": "6 pages, 4 figures, 1 table. Accepted - In Press, IAAI'26",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03156",
        "abs_url": "https://arxiv.org/abs/2511.03156",
        "pdf_url": "https://arxiv.org/pdf/2511.03156",
        "title": "Finetuning-Free Personalization of Text to Image Generation via Hypernetworks",
        "authors": [
            "Sagar Shrestha",
            "Gopal Sharma",
            "Luowei Zhou",
            "Suren Kumar"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Personalizing text-to-image diffusion models has traditionally relied on subject-specific fine-tuning approaches such as DreamBooth~\\cite{ruiz2023dreambooth}, which are computationally expensive and slow at inference. Recent adapter- and encoder-based methods attempt to reduce this overhead but still depend on additional fine-tuning or large backbone models for satisfactory results. In this work, we revisit an orthogonal direction: fine-tuning-free personalization via Hypernetworks that predict LoRA-adapted weights directly from subject images. Prior hypernetwork-based approaches, however, suffer from costly data generation or unstable attempts to mimic base model optimization trajectories. We address these limitations with an end-to-end training objective, stabilized by a simple output regularization, yielding reliable and effective hypernetworks. Our method removes the need for per-subject optimization at test time while preserving both subject fidelity and prompt alignment. To further enhance compositional generalization at inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG), which combines the compositional strengths of the base diffusion model with the subject fidelity of personalized models during sampling. Extensive experiments on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves strong personalization performance and highlights the promise of hypernetworks as a scalable and effective direction for open-category personalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03163",
        "abs_url": "https://arxiv.org/abs/2511.03163",
        "pdf_url": "https://arxiv.org/pdf/2511.03163",
        "title": "Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation",
        "authors": [
            "Yun-Chen Lin",
            "Jiayuan Huang",
            "Hanyuan Zhang",
            "Sergi Kavtaradze",
            "Matthew J. Clarkson",
            "Mobarak I. Hoque"
        ],
        "comments": "12 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate detection and delineation of anatomical structures in medical imaging are critical for computer-assisted interventions, particularly in laparoscopic liver surgery where 2D video streams limit depth perception and complicate landmark localization. While recent works have leveraged monocular depth cues for enhanced landmark detection, challenges remain in fusing RGB and depth features and in efficiently adapting large-scale vision models to surgical domains. We propose a depth-guided liver landmark segmentation framework integrating semantic and geometric cues via vision foundation encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB features and Depth Anything V2 (DA2) encoder to extract depth-aware features. To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient projection method that replaces the computationally expensive SVD with a Subsampled Randomized Fourier Transform (SRFT). This enables efficient fine-tuning of high-dimensional attention layers without sacrificing representational power. A cross-attention fusion module further integrates RGB and depth cues. To assess cross-dataset generalization, we also construct a new Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark. On the public L3D dataset, our method achieves a 4.85% improvement in Dice Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface Distance compared to the D2GPLand. To further assess generalization capability, we evaluate our model on LLSD dataset. Our model maintains competitive performance and significantly outperforms SAM-based baselines, demonstrating strong cross-dataset robustness and adaptability to unseen surgical environments. These results demonstrate that our SRFT-GaLore-enhanced dual-encoder framework enables scalable and precise segmentation under real-time, depth-constrained surgical settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03178",
        "abs_url": "https://arxiv.org/abs/2511.03178",
        "pdf_url": "https://arxiv.org/pdf/2511.03178",
        "title": "SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention",
        "authors": [
            "Shreyas C. Dhake",
            "Jiayuan Huang",
            "Runlong He",
            "Danyal Z. Khan",
            "Evangelos B. Mazomenos",
            "Sophia Bano",
            "Hani J. Marcus",
            "Danail Stoyanov",
            "Matthew J. Clarkson",
            "Mobarak I. Hoque"
        ],
        "comments": "12 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Anticipating forthcoming surgical events is vital for real-time assistance in endonasal transsphenoidal pituitary surgery, where visibility is limited and workflow changes rapidly. Most visual question answering (VQA) systems reason on isolated frames with static vision language alignment, providing little support for forecasting next steps or instrument needs. Existing surgical VQA datasets likewise center on the current scene rather than the near future. We introduce PitVQA-Anticipation, the first VQA dataset designed for forward looking surgical reasoning. It comprises 33.5 hours of operative video and 734,769 question answer pairs built from temporally grouped clips and expert annotations across four tasks: predicting the future phase, next step, upcoming instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video language model that adapts a large language model using a GRU Gated Temporal Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics, while an adaptive gate injects visual context into the language stream at the token level. Parameter efficient fine tuning customizes the language backbone to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and EndoVis datasets, surpassing strong image and video based baselines. Ablations show that temporal recurrence and gated fusion drive most of the gains. A frame budget study indicates a trade-off: 8 frames maximize fluency, whereas 32 frames slightly reduce BLEU but improve numeric time estimation. By pairing a temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA advances surgical VQA from retrospective description to proactive anticipation. PitVQA-Anticipation offers a comprehensive benchmark for this setting and highlights the importance of targeted temporal modeling for reliable, future aware surgical assistance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03194",
        "abs_url": "https://arxiv.org/abs/2511.03194",
        "pdf_url": "https://arxiv.org/pdf/2511.03194",
        "title": "PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research",
        "authors": [
            "Le Xue",
            "Gang Feng",
            "Wenbo Zhang",
            "Yichi Zhang",
            "Lanlan Li",
            "Shuqi Wang",
            "Liling Peng",
            "Sisi Peng",
            "Xin Gao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Publicly available, large-scale medical imaging datasets are crucial for developing and validating artificial intelligence models and conducting retrospective clinical research. However, datasets that combine functional and anatomical imaging with detailed clinical reports across multiple cancer types remain scarce. Here, we present PETWB-REP, a curated dataset comprising whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed Tomography (PET/CT) scans and corresponding radiology reports from 490 patients diagnosed with various malignancies. The dataset primarily includes common cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and ovarian cancer. This dataset includes paired PET and CT images, de-identified textual reports, and structured clinical metadata. It is designed to support research in medical imaging, radiomics, artificial intelligence, and multi-modal learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03206",
        "abs_url": "https://arxiv.org/abs/2511.03206",
        "pdf_url": "https://arxiv.org/pdf/2511.03206",
        "title": "QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models",
        "authors": [
            "Kuei-Chun Kao",
            "Hsu Tzu-Yin",
            "Yunqi Hong",
            "Ruochen Wang",
            "Cho-Jui Hsieh"
        ],
        "comments": "16 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03212",
        "abs_url": "https://arxiv.org/abs/2511.03212",
        "pdf_url": "https://arxiv.org/pdf/2511.03212",
        "title": "MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction",
        "authors": [
            "Ruting Cheng",
            "Boyuan Feng",
            "Yijiang Zheng",
            "Chuhui Qiu",
            "Aizierjiang Aiersilan",
            "Joaquin A. Calderon",
            "Wentao Zhao",
            "Qing Pan",
            "James K. Hahn"
        ],
        "comments": "19 pages, 4 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurately assessing the risk of cesarean section (CS) delivery is critical, especially in settings with limited medical resources, where access to healthcare is often restricted. Early and reliable risk prediction allows better-informed prenatal care decisions and can improve maternal and neonatal outcomes. However, most existing predictive models are tailored for in-hospital use during labor and rely on parameters that are often unavailable in resource-limited or home-based settings. In this study, we conduct a pilot investigation to examine the feasibility of using 3D body shape for CS risk assessment for future applications with more affordable general devices. We propose a novel multi-view-based Transformer network, MvBody, which predicts CS risk using only self-reported medical data and 3D optical body scans obtained between the 31st and 38th weeks of gestation. To enhance training efficiency and model generalizability in data-scarce environments, we incorporate a metric learning loss into the network. Compared to widely used machine learning models and the latest advanced 3D analysis methods, our method demonstrates superior performance, achieving an accuracy of 84.62% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set. To improve transparency and trust in the model's predictions, we apply the Integrated Gradients algorithm to provide theoretically grounded explanations of the model's decision-making process. Our results indicate that pre-pregnancy weight, maternal age, obstetric history, previous CS history, and body shape, particularly around the head and shoulders, are key contributors to CS risk prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03219",
        "abs_url": "https://arxiv.org/abs/2511.03219",
        "pdf_url": "https://arxiv.org/pdf/2511.03219",
        "title": "Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation",
        "authors": [
            "Pengyu Jie",
            "Wanquan Liu",
            "Rui He",
            "Yihui Wen",
            "Deyu Meng",
            "Chenqiang Gao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Augmentation for dense prediction typically relies on either sample mixing or generative synthesis. Mixing improves robustness but misaligned masks yield soft label ambiguity. Diffusion synthesis increases apparent diversity but, when trained as common samples, overlooks the structural benefit of mask conditioning and introduces synthetic-real domain shift. We propose a paired, diffusion-guided paradigm that fuses the strengths of both. For each real image, a synthetic counterpart is generated under the same mask and the pair is used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which mixes only image appearance while supervision always uses the original hard mask. This produces a continuous family of intermediate samples that smoothly bridges synthetic and real appearances under shared geometry, enlarging diversity without compromising pixel-level semantics. To keep learning aligned with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the mixing strength and the loss weight of mixed samples over training, gradually re-anchoring optimization to real data and mitigating distributional bias. Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC 2017, the approach achieves state-of-the-art segmentation performance and consistent gains over baselines. The results show that combining label-preserving mixing with diffusion-driven diversity, together with adaptive re-anchoring, yields robust and generalizable endoscopic segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03232",
        "abs_url": "https://arxiv.org/abs/2511.03232",
        "pdf_url": "https://arxiv.org/pdf/2511.03232",
        "title": "Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution",
        "authors": [
            "Sichen Guo",
            "Wenjie Li",
            "Yuanyang Liu",
            "Guangwei Gao",
            "Jian Yang",
            "Chia-Wen Lin"
        ],
        "comments": "12 pages, 10 figures, 7 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, Mamba-based super-resolution (SR) methods have demonstrated the ability to capture global receptive fields with linear complexity, addressing the quadratic computational cost of Transformer-based SR approaches. However, existing Mamba-based methods lack fine-grained transitions across different modeling scales, which limits the efficiency of feature representation. In this paper, we propose T-PMambaSR, a lightweight SR framework that integrates window-based self-attention with Progressive Mamba. By enabling interactions among receptive fields of different scales, our method establishes a fine-grained modeling paradigm that progressively enhances feature representation with linear complexity. Furthermore, we introduce an Adaptive High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost during Transformer and Mamba processing. Extensive experiments demonstrate that T-PMambaSR progressively enhances the model's receptive field and expressiveness, yielding better performance than recent Transformer- or Mamba-based methods while incurring lower computational cost. Our codes will be released after acceptance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03245",
        "abs_url": "https://arxiv.org/abs/2511.03245",
        "pdf_url": "https://arxiv.org/pdf/2511.03245",
        "title": "Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning",
        "authors": [
            "Liwei Luo",
            "Shuaitengyuan Li",
            "Dongwei Ren",
            "Qilong Wang",
            "Pengfei Zhu",
            "Qinghua Hu"
        ],
        "comments": "Accepted by ICCV2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, remarkable progress has been made in large-scale pre-trained model tuning, and inference efficiency is becoming more crucial for practical deployment. Early exiting in conjunction with multi-stage predictors, when cooperated with a parameter-efficient fine-tuning strategy, offers a straightforward way to achieve an inference-efficient model. However, a key challenge remains unresolved: How can early stages provide low-level fundamental features to deep stages while simultaneously supplying high-level discriminative features to early-stage predictors? To address this problem, we propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively decouple the low-level representative ability and high-level discriminative ability in early stages. First, in terms of architecture, we introduce a lightweight bypass module into multi-stage predictors for functional decomposition of shallow features from early stages, while a high-order statistics-based predictor is developed for early stages to effectively enhance their discriminative ability. To reasonably train our multi-predictor architecture, a decoupled optimization is proposed to allocate two-phase loss weights for multi-stage predictors during model tuning, where the initial training phase enables the model to prioritize the acquisition of discriminative ability of deep stages via emphasizing representative ability of early stages, and the latter training phase drives discriminative ability towards earlier stages as much as possible. As such, our DMPO can effectively decouple representative and discriminative abilities in early stages in terms of architecture design and model optimization. Experiments across various datasets and pre-trained backbones demonstrate that DMPO clearly outperforms its counterparts when reducing computational cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03255",
        "abs_url": "https://arxiv.org/abs/2511.03255",
        "pdf_url": "https://arxiv.org/pdf/2511.03255",
        "title": "Generative deep learning for foundational video translation in ultrasound",
        "authors": [
            "Nikolina Tomic Roshni Bhatnagar",
            "Sarthak Jain",
            "Connor Lau",
            "Tien-Yu Liu",
            "Laura Gambini",
            "Rima Arnaout"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Deep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03260",
        "abs_url": "https://arxiv.org/abs/2511.03260",
        "pdf_url": "https://arxiv.org/pdf/2511.03260",
        "title": "Enhancing Medical Image Segmentation via Heat Conduction Equation",
        "authors": [
            "Rong Wu",
            "Yim-Sang Yu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical image segmentation has been significantly advanced by deep learning architectures, notably U-Net variants. However, existing models struggle to achieve efficient global context modeling and long-range dependency reasoning under practical computational budgets simultaneously. In this work, we propose a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation. Our model combines Mamba-based state-space modules for efficient long-range reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers, simulating frequency-domain thermal diffusion for enhanced semantic abstraction. Experimental results on multimodal abdominal CT and MRI datasets demonstrate that the proposed model consistently outperforms strong baselines, validating its effectiveness and generalizability. It suggest that blending state-space dynamics with heat-based global diffusion offers a scalable and interpretable solution for medical segmentation tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03267",
        "abs_url": "https://arxiv.org/abs/2511.03267",
        "pdf_url": "https://arxiv.org/pdf/2511.03267",
        "title": "IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection",
        "authors": [
            "Bingyang Guo",
            "Hongjie Li",
            "Ruiyun Yu",
            "Hanzhe Liang",
            "Jinbao Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D anomaly detection (3D-AD) plays a critical role in industrial manufacturing, particularly in ensuring the reliability and safety of core equipment components. Although existing 3D datasets like Real3D-AD and MVTec 3D-AD offer broad application support, they fall short in capturing the complexities and subtle defects found in real industrial environments. This limitation hampers precise anomaly detection research, especially for industrial equipment components (IEC) such as bearings, rings, and bolts. To address this challenge, we have developed a point cloud anomaly detection dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is directly collected from actual production lines, ensuring high fidelity and relevance. Compared to existing datasets, IEC3D-AD features significantly improved point cloud resolution and defect annotation granularity, facilitating more demanding anomaly detection tasks. Furthermore, inspired by generative 2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This paradigm generates synthetic point cloud samples based on geometric morphological analysis, then reduces the margin and increases the overlap between normal and abnormal point-level features through spatial discrepancy optimization. Extensive experiments demonstrate the effectiveness of our method on both IEC3D-AD and other datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03272",
        "abs_url": "https://arxiv.org/abs/2511.03272",
        "pdf_url": "https://arxiv.org/pdf/2511.03272",
        "title": "Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising",
        "authors": [
            "Shuangquan Lyu",
            "Steven Mao",
            "Yue Ma"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Generating long videos remains a fundamental challenge, and achieving high controllability in video inpainting and outpainting is particularly demanding. To address both of these challenges simultaneously and achieve controllable video inpainting and outpainting for long video clips, we introduce a novel and unified approach for long video inpainting and outpainting that extends text-to-video diffusion models to generate arbitrarily long, spatially edited videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a large pre-trained video diffusion model like Alibaba's Wan 2.1 for masked region video synthesis, and employs an overlap-and-blend temporal co-denoising strategy with high-order solvers to maintain consistency across long sequences. In contrast to prior work that struggles with fixed-length clips or exhibits stitching artifacts, our system enables arbitrarily long video generation and editing without noticeable seams or drift. We validate our approach on challenging inpainting/outpainting tasks including editing or adding objects over hundreds of frames and demonstrate superior performance to baseline methods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and perceptual realism (LPIPS). Our method enables practical long-range video editing with minimal overhead, achieved a balance between parameter efficient and superior performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03317",
        "abs_url": "https://arxiv.org/abs/2511.03317",
        "pdf_url": "https://arxiv.org/pdf/2511.03317",
        "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models",
        "authors": [
            "Minghao Fu",
            "Guo-Hua Wang",
            "Tianyu Cui",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "comments": "The code is publicly available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03325",
        "abs_url": "https://arxiv.org/abs/2511.03325",
        "pdf_url": "https://arxiv.org/pdf/2511.03325",
        "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding",
        "authors": [
            "Mauro Orazio Drago",
            "Luca Carlini",
            "Pelinsu Celebi Balyemez",
            "Dennis Pierantozzi",
            "Chiara Lena",
            "Cesare Hassan",
            "Danail Stoyanov",
            "Elena De Momi",
            "Sophia Bano",
            "Mobarak I. Hoque"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool--tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11\\% on REAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03332",
        "abs_url": "https://arxiv.org/abs/2511.03332",
        "pdf_url": "https://arxiv.org/pdf/2511.03332",
        "title": "Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge",
        "authors": [
            "Yi Yang",
            "Yiming Xu",
            "Timo Kaiser",
            "Hao Cheng",
            "Bodo Rosenhahn",
            "Michael Ying Yang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03334",
        "abs_url": "https://arxiv.org/abs/2511.03334",
        "pdf_url": "https://arxiv.org/pdf/2511.03334",
        "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions",
        "authors": [
            "Guozhen Zhang",
            "Zixiang Zhou",
            "Teng Hu",
            "Ziqiao Peng",
            "Youliang Zhang",
            "Yi Chen",
            "Yuan Zhou",
            "Qinglin Lu",
            "Limin Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03367",
        "abs_url": "https://arxiv.org/abs/2511.03367",
        "pdf_url": "https://arxiv.org/pdf/2511.03367",
        "title": "Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models",
        "authors": [
            "Gahyeon Kim",
            "Sohee Kim",
            "Seokju Lee"
        ],
        "comments": "Accepted in Pattern Recognition",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03416",
        "abs_url": "https://arxiv.org/abs/2511.03416",
        "pdf_url": "https://arxiv.org/pdf/2511.03416",
        "title": "Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort",
        "authors": [
            "Nikolai Herrmann",
            "Marcella C. Zijta",
            "Stefan Klein",
            "RÃ©gine P.M. Steegers-Theunissen",
            "Rene M.H. Wijnen",
            "Bernadette S. de Bakker",
            "Melek Rousian",
            "Wietske A.P. Bastiaansen"
        ],
        "comments": "Submitted version of paper accepted at International Workshop on Preterm, Perinatal and Paediatric Image Analysis 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Standardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans. In this work, we propose an automated method for standardizing this alignment. Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryo's principal axes, from which four candidate orientations are derived. The candidate in standard orientation is selected using one of three strategies: a heuristic based on Pearson's correlation assessing shape, image matching to an atlas through normalized cross-correlation, and a Random Forest classifier. We tested our method on 2166 images longitudinally acquired 3D ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images, PCA correctly extracted the principal axes of the embryo. The correct candidate was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%, 95.8%, and 98.4% of images, respectively. A Majority Vote of these selection methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline enables consistent embryonic alignment in the first trimester, enabling scalable analysis in both clinical and research settings. The code is publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03459",
        "abs_url": "https://arxiv.org/abs/2511.03459",
        "pdf_url": "https://arxiv.org/pdf/2511.03459",
        "title": "Generalizing Shape-from-Template to Topological Changes",
        "authors": [
            "Kevin Manogue",
            "Tomasz M Schang",
            "Dilara KuÅŸ",
            "Jonas MÃ¼ller",
            "Stefan Zachow",
            "Agniva Sengupta"
        ],
        "comments": "Accepted for publication at Smart Tools and Applications in Graphics (STAG), Genoa, Italy (2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Reconstructing the surfaces of deformable objects from correspondences between a 3D template and a 2D image is well studied under Shape-from-Template (SfT) methods; however, existing approaches break down when topological changes accompany the deformation. We propose a principled extension of SfT that enables reconstruction in the presence of such changes. Our approach is initialized with a classical SfT solution and iteratively adapts the template by partitioning its spatial domain so as to minimize an energy functional that jointly encodes physical plausibility and reprojection consistency. We demonstrate that the method robustly captures a wide range of practically relevant topological events including tears and cuts on bounded 2D surfaces, thereby establishing the first general framework for topological-change-aware SfT. Experiments on both synthetic and real data confirm that our approach consistently outperforms baseline methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03589",
        "abs_url": "https://arxiv.org/abs/2511.03589",
        "pdf_url": "https://arxiv.org/pdf/2511.03589",
        "title": "Human Mesh Modeling for Anny Body",
        "authors": [
            "Romain BrÃ©gier",
            "GuÃ©nolÃ© Fiche",
            "Laura Bravo-SÃ¡nchez",
            "Thomas Lucas",
            "Matthieu Armando",
            "Philippe Weinzaepfel",
            "GrÃ©gory Rogez",
            "Fabien Baradel"
        ],
        "comments": "We release our model and code at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Parametric body models are central to many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms -- across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling -- supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic humans generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models, while remaining interpretable and broadly representative. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03645",
        "abs_url": "https://arxiv.org/abs/2511.03645",
        "pdf_url": "https://arxiv.org/pdf/2511.03645",
        "title": "Signal Intensity-weighted coordinate channels improve learning stability and generalisation in 1D and 2D CNNs in localisation tasks on biomedical signals",
        "authors": [
            "Vittal L. Rao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Localisation tasks in biomedical data often require models to learn meaningful spatial or temporal relationships from signals with complex intensity distributions. A common strategy, exemplified by CoordConv layers, is to append coordinate channels to convolutional inputs, enabling networks to learn absolute positions. In this work, we propose a signal intensity-weighted coordinate representation that replaces the pure coordinate channels with channels scaled by local signal intensity. This modification embeds an intensity-position coupling directly in the input representation, introducing a simple and modality-agnostic inductive bias. We evaluate the approach on two distinct localisation problems: (i) predicting the time of morphological transition in 20-second, two-lead ECG signals, and (ii) regressing the coordinates of nuclear centres in cytological images from the SiPaKMeD dataset. In both cases, the proposed representation yields faster convergence and higher generalisation performance relative to conventional coordinate-channel approaches, demonstrating its effectiveness across both one-dimensional and two-dimensional biomedical signals.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03665",
        "abs_url": "https://arxiv.org/abs/2511.03665",
        "pdf_url": "https://arxiv.org/pdf/2511.03665",
        "title": "A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential",
        "authors": [
            "Mehdi Sefidgar Dilmaghani",
            "Francis Fowley",
            "Peter Corcoran"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper presents a lightweight three-dimensional convolutional neural network (3DCNN) for human activity recognition (HAR) using event-based vision data. Privacy preservation is a key challenge in human monitoring systems, as conventional frame-based cameras capture identifiable personal information. In contrast, event cameras record only changes in pixel intensity, providing an inherently privacy-preserving sensing modality. The proposed network effectively models both spatial and temporal dynamics while maintaining a compact design suitable for edge deployment. To address class imbalance and enhance generalization, focal loss with class reweighting and targeted data augmentation strategies are employed. The model is trained and evaluated on a composite dataset derived from the Toyota Smart Home and ETRI datasets. Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D, and MC3_18 by up to 3%. These results highlight the potential of event-based deep learning for developing accurate, efficient, and privacy-aware human action recognition systems suitable for real-world edge applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03666",
        "abs_url": "https://arxiv.org/abs/2511.03666",
        "pdf_url": "https://arxiv.org/pdf/2511.03666",
        "title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection",
        "authors": [
            "Dongkeun Kim",
            "Minsu Cho",
            "Suha Kwak"
        ],
        "comments": "Accepted to NeurIPS 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03725",
        "abs_url": "https://arxiv.org/abs/2511.03725",
        "pdf_url": "https://arxiv.org/pdf/2511.03725",
        "title": "Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition",
        "authors": [
            "Jongseo Lee",
            "Wooil Lee",
            "Gyeong-Moon Park",
            "Seong Tae Kim",
            "Jinwoo Choi"
        ],
        "comments": "NeurIPS 2025 Spotlight paper. Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature -- intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101 -- demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02849",
        "abs_url": "https://arxiv.org/abs/2511.02849",
        "pdf_url": "https://arxiv.org/pdf/2511.02849",
        "title": "Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData",
        "authors": [
            "Beyza Cinar",
            "Maria Maleshkova"
        ],
        "comments": "11 pages, 5 Tables, 4 Figures, BHI 2025 conference (JBHI special issue)",
        "subjects": "Signal Processing (eess.SP); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Individualized therapy is driven forward by medical data analysis, which provides insight into the patient's context. In particular, for Type 1 Diabetes (T1D), which is an autoimmune disease, relationships between demographics, sensor data, and context can be analyzed. However, outliers, noisy data, and small data volumes cannot provide a reliable analysis. Hence, the research domain requires large volumes of high-quality data. Moreover, missing values can lead to information loss. To address this limitation, this study improves the data quality of DiaData, an integration of 15 separate datasets containing glucose values from 2510 subjects with T1D. Notably, we make the following contributions: 1) Outliers are identified with the interquartile range (IQR) approach and treated by replacing them with missing values. 2) Small gaps ($\\le$ 25 min) are imputed with linear interpolation and larger gaps ($\\ge$ 30 and $<$ 120 min) with Stineman interpolation. Based on a visual comparison, Stineman interpolation provides more realistic glucose estimates than linear interpolation for larger gaps. 3) After data cleaning, the correlation between glucose and heart rate is analyzed, yielding a moderate relation between 15 and 60 minutes before hypoglycemia ($\\le$ 70 mg/dL). 4) Finally, a benchmark for hypoglycemia classification is provided with a state-of-the-art ResNet model. The model is trained with the Maindatabase and Subdatabase II of DiaData to classify hypoglycemia onset up to 2 hours in advance. Training with more data improves performance by 7% while using quality-refined data yields a 2-3% gain compared to raw data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02880",
        "abs_url": "https://arxiv.org/abs/2511.02880",
        "pdf_url": "https://arxiv.org/pdf/2511.02880",
        "title": "NEF-NET+: Adapting Electrocardio panorama in the wild",
        "authors": [
            "Zehui Zhan",
            "Yaojun Hu",
            "Jiajing Zhan",
            "Wanchen Lian",
            "Wanqing Wu",
            "Jintai Chen"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and com- pensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02893",
        "abs_url": "https://arxiv.org/abs/2511.02893",
        "pdf_url": "https://arxiv.org/pdf/2511.02893",
        "title": "Optimizing the nnU-Net model for brain tumor (Glioma) segmentation Using a BraTS Sub-Saharan Africa (SSA) dataset",
        "authors": [
            "Chukwuemeka Arua Kalu",
            "Adaobi Chiazor Emegoakor",
            "Fortune Okafor",
            "Augustine Okoh Uchenna",
            "Chijioke Kelvin Ukpai",
            "Godsent Erere Onyeugbo"
        ],
        "comments": "10 pages, 4 figures",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical image segmentation is a critical achievement in modern medical science, developed over decades of research. It allows for the exact delineation of anatomical and pathological features in two- or three-dimensional pictures by utilizing notions like pixel intensity, texture, and anatomical context. With the advent of automated segmentation, physicians and radiologists may now concentrate on diagnosis and treatment planning while intelligent computers perform routine image processing tasks. This study used the BraTS Sub-Saharan Africa dataset, a selected subset of the BraTS dataset that included 60 multimodal MRI cases from patients with glioma. Surprisingly, the nnU Net model trained on the initial 60 instances performed better than the network trained on an offline-augmented dataset of 360 cases. Hypothetically, the offline augmentations introduced artificial anatomical variances or intensity distributions, reducing generalization. In contrast, the original dataset, when paired with nnU Net's robust online augmentation procedures, maintained realistic variability and produced better results. The study achieved a Dice score of 0.84 for whole tumor segmentation. These findings highlight the significance of data quality and proper augmentation approaches in constructing accurate, generalizable medical picture segmentation models, particularly for under-represented locations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02928",
        "abs_url": "https://arxiv.org/abs/2511.02928",
        "pdf_url": "https://arxiv.org/pdf/2511.02928",
        "title": "Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI",
        "authors": [
            "Ilerioluwakiiye Abolade",
            "Aniekan Udo",
            "Augustine Ojo",
            "Abdulbasit Oyetunji",
            "Hammed Ajigbotosho",
            "Aondana Iorumbur",
            "Confidence Raymond",
            "Maruf Adewole"
        ],
        "comments": "4 pages, 2 figures. Accepted as an abstract at the Women in Machine Learning (WiML) Workshop at NeurIPS 2025",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Glioma segmentation is critical for diagnosis and treatment planning, yet remains challenging in Sub-Saharan Africa due to limited MRI infrastructure and heterogeneous acquisition protocols that induce severe domain shift. We propose SegFormer3D-plus, a radiomics-guided transformer architecture designed for robust segmentation under domain variability. Our method combines: (1) histogram matching for intensity harmonization across scanners, (2) radiomic feature extraction with PCA-reduced k-means for domain-aware stratified sampling, (3) a dual-pathway encoder with frequency-aware feature extraction and spatial-channel attention, and (4) composite Dice-Cross-Entropy loss for boundary refinement. Pretrained on BraTS 2023 and fine-tuned on BraTS-Africa data, SegFormer3D-plus demonstrates improved tumor subregion delineation and boundary localization across heterogeneous African clinical scans, highlighting the value of radiomics-guided domain adaptation for resource-limited settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.02994",
        "abs_url": "https://arxiv.org/abs/2511.02994",
        "pdf_url": "https://arxiv.org/pdf/2511.02994",
        "title": "Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data",
        "authors": [
            "Syed Mostaquim Ali",
            "Taufiq Rahman",
            "Ghazal Farhani",
            "Mohamed H. Zaki",
            "Benoit Anctil",
            "Dominique Charlebois"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "For developing safe Autonomous Driving Systems (ADS), rigorous testing is required before they are deemed safe for road deployments. Since comprehensive conventional physical testing is impractical due to cost and safety concerns, Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing VTE-generated sensor outputs against their real-world analogues can be a strong indication that the VTE accurately represents reality. Correspondingly, this work explores a comprehensive experimental approach to finding evaluation metrics suitable for comparing real-world and simulated LiDAR scans. The metrics were tested in terms of sensitivity and accuracy with different noise, density, distortion, sensor orientation, and channel settings. From comparing the metrics, we found that Density Aware Chamfer Distance (DCD) works best across all cases. In the second step of the research, a Virtual Testing Environment was generated using real LiDAR scan data. The data was collected in a controlled environment with only static objects using an instrumented vehicle equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans were compared in terms of model perception and geometric similarity. Actual and simulated LiDAR scans have a similar semantic segmentation output with a mIoU of 21\\% with corrected intensity and an average density aware chamfer distance (DCD) of 0.63. This indicates a slight difference in the geometric properties of simulated and real LiDAR scans and a significant difference between model outputs. During the comparison, density-aware chamfer distance was found to be the most correlated among the metrics with perception methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03046",
        "abs_url": "https://arxiv.org/abs/2511.03046",
        "pdf_url": "https://arxiv.org/pdf/2511.03046",
        "title": "Data-Efficient Realized Volatility Forecasting with Vision Transformers",
        "authors": [
            "Emi Soroka",
            "Artem Arzyn"
        ],
        "comments": "NeurIPS Generative AI in Finance",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent work in financial machine learning has shown the virtue of complexity: the phenomenon by which deep learning methods capable of learning highly nonlinear relationships outperform simpler approaches in financial forecasting. While transformer architectures like Informer have shown promise for financial time series forecasting, the application of transformer models for options data remains largely unexplored. We conduct preliminary studies towards the development of a transformer model for options data by training the Vision Transformer (ViT) architecture, typically used in modern image recognition and classification systems, to predict the realized volatility of an asset over the next 30 days from its implied volatility surface (augmented with date information) for a single day. We show that the ViT can learn seasonal patterns and nonlinear features from the IV surface, suggesting a promising direction for model development.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03147",
        "abs_url": "https://arxiv.org/abs/2511.03147",
        "pdf_url": "https://arxiv.org/pdf/2511.03147",
        "title": "Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models",
        "authors": [
            "Haotian Yin",
            "Przemyslaw Musialski"
        ],
        "comments": "Lecture Notes in Computer Science (LNCS), 20th International Symposium on Visual Computing 2025, 12 pages, 4 figures, preprint",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Neural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03148",
        "abs_url": "https://arxiv.org/abs/2511.03148",
        "pdf_url": "https://arxiv.org/pdf/2511.03148",
        "title": "Test Time Adaptation Using Adaptive Quantile Recalibration",
        "authors": [
            "Paria Mehrbod",
            "Pedro Vianna",
            "Geraldin Nanfack",
            "Guy Wolf",
            "Eugene Belilovsky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Domain adaptation is a key strategy for enhancing the generalizability of deep learning models in real-world scenarios, where test distributions often diverge significantly from the training domain. However, conventional approaches typically rely on prior knowledge of the target domain or require model retraining, limiting their practicality in dynamic or resource-constrained environments. Recent test-time adaptation methods based on batch normalization statistic updates allow for unsupervised adaptation, but they often fail to capture complex activation distributions and are constrained to specific normalization layers. We propose Adaptive Quantile Recalibration (AQR), a test-time adaptation technique that modifies pre-activation distributions by aligning quantiles on a channel-wise basis. AQR captures the full shape of activation distributions and generalizes across architectures employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of estimating distribution tails under varying batch sizes, AQR incorporates a robust tail calibration strategy that improves stability and precision. Our method leverages source-domain statistics computed at training time, enabling unsupervised adaptation without retraining models. Experiments on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR achieves robust adaptation across diverse settings, outperforming existing test-time adaptation baselines. These results highlight AQR's potential for deployment in real-world scenarios with dynamic and unpredictable data distributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03197",
        "abs_url": "https://arxiv.org/abs/2511.03197",
        "pdf_url": "https://arxiv.org/pdf/2511.03197",
        "title": "A Probabilistic U-Net Approach to Downscaling Climate Simulations",
        "authors": [
            "Maryam Alipourhajiagha",
            "Pierre-Louis Lemaire",
            "Youssef Diouane",
            "Julie Carreau"
        ],
        "comments": "NeurIPS 2025 AI4Science",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Climate models are limited by heavy computational costs, often producing outputs at coarse spatial resolutions, while many climate change impact studies require finer scales. Statistical downscaling bridges this gap, and we adapt the probabilistic U-Net for this task, combining a deterministic U-Net backbone with a variational latent space to capture aleatoric uncertainty. We evaluate four training objectives, afCRPS and WMSE-MS-SSIM with three settings for downscaling precipitation and temperature from $16\\times$ coarser resolution. Our main finding is that WMSE-MS-SSIM performs well for extremes under certain settings, whereas afCRPS better captures spatial variability across scales.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03239",
        "abs_url": "https://arxiv.org/abs/2511.03239",
        "pdf_url": "https://arxiv.org/pdf/2511.03239",
        "title": "A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams",
        "authors": [
            "Philipp Reis",
            "Philipp Rigoll",
            "Christian Steinhauser",
            "Jacob Langner",
            "Eric Sax"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Modern AI systems are increasingly constrained not by model capacity but by the quality and diversity of their data. Despite growing emphasis on data-centric AI, most datasets are still gathered in an open-loop manner which accumulates redundant samples without feedback from the current coverage. This results in inefficient storage, costly labeling, and limited generalization. To address this, this paper introduces \\ac{FCDC}, a paradigm that formulates data collection as a closed-loop control problem. \\ac{FCDC} continuously approximates the state of the collected data distribution using an online probabilistic model and adaptively regulates sample retention using based on feedback signals such as likelihood and Mahalanobis distance. Through this feedback mechanism, the system dynamically balances exploration and exploitation, maintains dataset diversity, and prevents redundancy from accumulating over time. Besides showcasing the controllability of \\ac{FCDC} on a synthetic dataset, experiments on a real data stream show that \\ac{FCDC} produces more balanced datasets by $\\SI{25.9}{\\percent}$ while reducing data storage by $\\SI{39.8}{\\percent}$. These results demonstrate that data collection itself can be actively controlled, transforming collection from a passive pipeline stage into a self-regulating, feedback-driven process at the core of data-centric AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03256",
        "abs_url": "https://arxiv.org/abs/2511.03256",
        "pdf_url": "https://arxiv.org/pdf/2511.03256",
        "title": "Decoupled Entropy Minimization",
        "authors": [
            "Jing Ma",
            "Hanlin Li",
            "Xiang Xiang"
        ],
        "comments": "To appear at NeurIPS 2025 (main conference), San Diego, CA, USA. Codes available at this https URL",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)",
        "abstract": "Entropy Minimization (EM) is beneficial to reducing class overlap, bridging domain gap, and restricting uncertainty for various tasks in machine learning, yet its potential is limited. To study the internal mechanism of EM, we reformulate and decouple the classical EM into two parts with opposite effects: cluster aggregation driving factor (CADF) rewards dominant classes and prompts a peaked output distribution, while gradient mitigation calibrator (GMC) penalizes high-confidence classes based on predicted probabilities. Furthermore, we reveal the limitations of classical EM caused by its coupled formulation: 1) reward collapse impedes the contribution of high-certainty samples in the learning process, and 2) easy-class bias induces misalignment between output distribution and label distribution. To address these issues, we propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the reward brought from CADF and employs a marginal entropy calibrator (MEC) to replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM, and achieves superior performance across various imperfectly supervised learning tasks in noisy and dynamic environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03328",
        "abs_url": "https://arxiv.org/abs/2511.03328",
        "pdf_url": "https://arxiv.org/pdf/2511.03328",
        "title": "Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks",
        "authors": [
            "Jindong Hong",
            "Tianjie Chen",
            "Lingjie Luo",
            "Chuanyang Zheng",
            "Ting Xu",
            "Haibao Yu",
            "Jianing Qiu",
            "Qianzhong Chen",
            "Suning Huang",
            "Yan Xu",
            "Yong Gui",
            "Yijun He",
            "Jiankai Sun"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of \"reasoning MLLMs\" that offer explicit control over their internal thinking processes (normally referred as the \"thinking mode\") alongside the standard \"non-thinking mode\". This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these \"dual-state\" MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active \"thinking mode\" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03365",
        "abs_url": "https://arxiv.org/abs/2511.03365",
        "pdf_url": "https://arxiv.org/pdf/2511.03365",
        "title": "Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene Mutation Prediction from Histopathology",
        "authors": [
            "Gabriela Fernandes"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Quantitative Methods (q-bio.QM)",
        "abstract": "Ovarian cancer remains one of the most lethal gynecological malignancies, largely due to late diagnosis and extensive heterogeneity across subtypes. Current diagnostic methods are limited in their ability to reveal underlying genomic variations essential for precision oncology. This study introduces a novel hybrid deep learning pipeline that integrates quantitative nuclear morphometry with deep convolutional image features to perform ovarian cancer subtype classification and gene mutation inference directly from Hematoxylin and Eosin (H&E) histopathological images. Using $\\sim45,000$ image patches sourced from The Cancer Genome Atlas (TCGA) and public datasets, a fusion model combining a ResNet-50 Convolutional Neural Network (CNN) encoder and a Vision Transformer (ViT) was developed. This model successfully captured both local morphological texture and global tissue context. The pipeline achieved a robust overall subtype classification accuracy of $84.2\\%$ (Macro AUC of $0.87 \\pm 0.03$). Crucially, the model demonstrated the capacity for gene mutation inference with moderate-to-high accuracy: $AUC_{TP53} = 0.82 \\pm 0.02$, $AUC_{BRCA1} = 0.76 \\pm 0.04$, and $AUC_{ARID1A} = 0.73 \\pm 0.05$. Feature importance analysis established direct quantitative links, revealing that nuclear solidity and eccentricity were the dominant predictors for TP53 mutation. These findings validate that quantifiable histological phenotypes encode measurable genomic signals, paving the way for cost-effective, precision histopathology in ovarian cancer triage and diagnosis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03423",
        "abs_url": "https://arxiv.org/abs/2511.03423",
        "pdf_url": "https://arxiv.org/pdf/2511.03423",
        "title": "Seeing What You Say: Expressive Image Generation from Speech",
        "authors": [
            "Jiyoung Lee",
            "Song Park",
            "Sanghyuk Chun",
            "Soo-Whan Chung"
        ],
        "comments": "In progress",
        "subjects": "Audio and Speech Processing (eess.AS); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)",
        "abstract": "This paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information. At its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance. By operating directly on these tokens, VoxStudio eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired emotional speech-image dataset built via an advanced TTS engine to affordably generate richly expressive utterances. Comprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03571",
        "abs_url": "https://arxiv.org/abs/2511.03571",
        "pdf_url": "https://arxiv.org/pdf/2511.03571",
        "title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera",
        "authors": [
            "Hao Shi",
            "Ze Wang",
            "Shangwei Guo",
            "Mengfei Duan",
            "Song Wang",
            "Teng Chen",
            "Kailun Yang",
            "Lin Wang",
            "Kaiwei Wang"
        ],
        "comments": "Datasets and code will be publicly available at this https URL",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360Â° continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360Â° continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range/occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360Â°) and Human360Occ (H3O) (CARLA human-ego 360Â° with RGB, Depth, semantic occupancy; standardized within-/cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged/humanoid robots. Datasets and code will be publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-06",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-06?abs=True",
        "arxiv_id": "2511.03651",
        "abs_url": "https://arxiv.org/abs/2511.03651",
        "pdf_url": "https://arxiv.org/pdf/2511.03651",
        "title": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural",
        "authors": [
            "Andrei A. Korigodskii",
            "Oleg D. Kalachev",
            "Artem E. Vasiunik",
            "Matvei V. Urvantsev",
            "Georgii E. Bondar"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)",
        "abstract": "This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world's largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone's propellers, which also protects the drone's critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system's robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]