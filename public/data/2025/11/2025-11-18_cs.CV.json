[
    {
        "order": 1,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11633",
        "abs_url": "https://arxiv.org/abs/2511.11633",
        "pdf_url": "https://arxiv.org/pdf/2511.11633",
        "title": "Psychological stress during Examination and its estimation by handwriting in answer script",
        "authors": [
            "Abhijeet Kumar",
            "Chetan Agarwal",
            "Pronoy B. Neogi",
            "Mayank Goswami"
        ],
        "comments": "10 Pages, 6 Figures and 1 Table",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11643",
        "abs_url": "https://arxiv.org/abs/2511.11643",
        "pdf_url": "https://arxiv.org/pdf/2511.11643",
        "title": "Real-time pothole detection with onboard sensors and camera on vehicles",
        "authors": [
            "Aswath Muthuselvam",
            "Jeevak Raj S",
            "Mohanaprasad K"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11659",
        "abs_url": "https://arxiv.org/abs/2511.11659",
        "pdf_url": "https://arxiv.org/pdf/2511.11659",
        "title": "A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model",
        "authors": [
            "Kesong Zheng",
            "Zhi Song",
            "Peizhou Li",
            "Shuyi Yao",
            "Zhenxing Bian"
        ],
        "comments": "30 pages,12 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11662",
        "abs_url": "https://arxiv.org/abs/2511.11662",
        "pdf_url": "https://arxiv.org/pdf/2511.11662",
        "title": "AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation",
        "authors": [
            "Ziyuan Gao"
        ],
        "comments": "Accepted for publication in WACV 2026 (Round 2)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11700",
        "abs_url": "https://arxiv.org/abs/2511.11700",
        "pdf_url": "https://arxiv.org/pdf/2511.11700",
        "title": "EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance",
        "authors": [
            "Jiahui Wang",
            "Haiyue Zhu",
            "Haoren Guo",
            "Abdullah Al Mamun",
            "Cheng Xiang",
            "Tong Heng Lee"
        ],
        "comments": "AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11702",
        "abs_url": "https://arxiv.org/abs/2511.11702",
        "pdf_url": "https://arxiv.org/pdf/2511.11702",
        "title": "Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement",
        "authors": [
            "Lian He",
            "Meng Liu",
            "Qilang Ye",
            "Yu Zhou",
            "Xiang Deng",
            "Gangyi Ding"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11708",
        "abs_url": "https://arxiv.org/abs/2511.11708",
        "pdf_url": "https://arxiv.org/pdf/2511.11708",
        "title": "LE-CapsNet: A Light and Enhanced Capsule Network",
        "authors": [
            "Pouya Shiri",
            "Amirali Baniasadi"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11710",
        "abs_url": "https://arxiv.org/abs/2511.11710",
        "pdf_url": "https://arxiv.org/pdf/2511.11710",
        "title": "Target-Balanced Score Distillation",
        "authors": [
            "Zhou Xu",
            "Qi Wang",
            "Yuxiao Yang",
            "Luyuan Zhang",
            "Zhang Liang",
            "Yang Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11716",
        "abs_url": "https://arxiv.org/abs/2511.11716",
        "pdf_url": "https://arxiv.org/pdf/2511.11716",
        "title": "CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition",
        "authors": [
            "Sudhakar Sah",
            "Nikhil Chabbra",
            "Matthieu Durnerin"
        ],
        "comments": "11 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11720",
        "abs_url": "https://arxiv.org/abs/2511.11720",
        "pdf_url": "https://arxiv.org/pdf/2511.11720",
        "title": "AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks",
        "authors": [
            "Jiao Chen",
            "Haoyi Wang",
            "Jianhua Tang",
            "Junyi Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Multiagent Systems (cs.MA)",
        "abstract": "Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11725",
        "abs_url": "https://arxiv.org/abs/2511.11725",
        "pdf_url": "https://arxiv.org/pdf/2511.11725",
        "title": "Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video",
        "authors": [
            "Zekai Shi",
            "Zhixi Cai",
            "Kalin Stefanov"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11730",
        "abs_url": "https://arxiv.org/abs/2511.11730",
        "pdf_url": "https://arxiv.org/pdf/2511.11730",
        "title": "GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion",
        "authors": [
            "Yongjun Xiao",
            "Dian Meng",
            "Xinlei Huang",
            "Yanran Liu",
            "Shiwei Ruan",
            "Ziyue Qiao",
            "Xubin Zheng"
        ],
        "comments": "8 pages, 3 figures, Accepted to AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11732",
        "abs_url": "https://arxiv.org/abs/2511.11732",
        "pdf_url": "https://arxiv.org/pdf/2511.11732",
        "title": "Exposing DeepFakes via Hyperspectral Domain Mapping",
        "authors": [
            "Aditya Mehta",
            "Swarnim Chaudhary",
            "Pratik Narang",
            "Jagat Sesh Challa"
        ],
        "comments": "Accepted at AAAI 2026 Student Abstract",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11735",
        "abs_url": "https://arxiv.org/abs/2511.11735",
        "pdf_url": "https://arxiv.org/pdf/2511.11735",
        "title": "Toward bilipshiz geometric models",
        "authors": [
            "Yonatan Sverdlov",
            "Eitan Rosen",
            "Nadav Dym"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios. We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11751",
        "abs_url": "https://arxiv.org/abs/2511.11751",
        "pdf_url": "https://arxiv.org/pdf/2511.11751",
        "title": "Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models",
        "authors": [
            "Sanchit Sinha",
            "Guangzhi Xiong",
            "Zhenghao He",
            "Aidong Zhang"
        ],
        "comments": "AAAI 2026 (oral)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11754",
        "abs_url": "https://arxiv.org/abs/2511.11754",
        "pdf_url": "https://arxiv.org/pdf/2511.11754",
        "title": "Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition",
        "authors": [
            "Stanislav Selitskiy"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike \"traditional\" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the \"important\" dimensions (primary components) is implemented. In such a way, the \"important\" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11780",
        "abs_url": "https://arxiv.org/abs/2511.11780",
        "pdf_url": "https://arxiv.org/pdf/2511.11780",
        "title": "Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing",
        "authors": [
            "Hossein Mohebbi",
            "Mohammed Abdulrahman",
            "Yanting Miao",
            "Pascal Poupart",
            "Suraj Kothawade"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11824",
        "abs_url": "https://arxiv.org/abs/2511.11824",
        "pdf_url": "https://arxiv.org/pdf/2511.11824",
        "title": "SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction",
        "authors": [
            "Zhongping Dong",
            "Pengyang Yu",
            "Shuangjian Li",
            "Liming Chen",
            "Mohand Tahar Kechadi"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \\textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11837",
        "abs_url": "https://arxiv.org/abs/2511.11837",
        "pdf_url": "https://arxiv.org/pdf/2511.11837",
        "title": "MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning",
        "authors": [
            "Fatemeh Elhambakhsh",
            "Gaurav Ameta",
            "Aditi Roy",
            "Hyunwoong Ko"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\\% and 36\\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11851",
        "abs_url": "https://arxiv.org/abs/2511.11851",
        "pdf_url": "https://arxiv.org/pdf/2511.11851",
        "title": "Defending Unauthorized Model Merging via Dual-Stage Weight Protection",
        "authors": [
            "Wei-Jia Chen",
            "Min-Yen Tsai",
            "Cheng-Yi Lee",
            "Chia-Mu Yu"
        ],
        "comments": "10 pages, under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Cryptography and Security (cs.CR)",
        "abstract": "The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11864",
        "abs_url": "https://arxiv.org/abs/2511.11864",
        "pdf_url": "https://arxiv.org/pdf/2511.11864",
        "title": "FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision",
        "authors": [
            "Muzammal Shafique",
            "Nasir Rahim",
            "Jamil Ahmad",
            "Mohammad Siadat",
            "Khalid Malik",
            "Ghaus Malik"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11882",
        "abs_url": "https://arxiv.org/abs/2511.11882",
        "pdf_url": "https://arxiv.org/pdf/2511.11882",
        "title": "Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)",
        "authors": [
            "Simon Durand",
            "Samuel Foucher",
            "Alexandre Delplanque",
            "Joëlle Taillon",
            "Jérôme Théau"
        ],
        "comments": "34 pages, 10 figures, submitted to Remote Sensing in Ecology and Conservation",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11890",
        "abs_url": "https://arxiv.org/abs/2511.11890",
        "pdf_url": "https://arxiv.org/pdf/2511.11890",
        "title": "Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation",
        "authors": [
            "Camila Machado de Araujo",
            "Egon P. B. S. Borges",
            "Ricardo Marcelo Canteiro Grangeiro",
            "Allan Pinto"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11898",
        "abs_url": "https://arxiv.org/abs/2511.11898",
        "pdf_url": "https://arxiv.org/pdf/2511.11898",
        "title": "Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks",
        "authors": [
            "Arnav Singhvi",
            "Vasiliki Bikia",
            "Asad Aali",
            "Akshay Chaudhari",
            "Roxana Daneshjou"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11908",
        "abs_url": "https://arxiv.org/abs/2511.11908",
        "pdf_url": "https://arxiv.org/pdf/2511.11908",
        "title": "PI-NAIM: Path-Integrated Neural Adaptive Imputation Model",
        "authors": [
            "Afifa Khaled",
            "Ebrahim Hamid Sumiea"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11910",
        "abs_url": "https://arxiv.org/abs/2511.11910",
        "pdf_url": "https://arxiv.org/pdf/2511.11910",
        "title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models",
        "authors": [
            "Siyou Li",
            "Huanan Wu",
            "Juexi Shao",
            "Yinghao Ma",
            "Yujian Gan",
            "Yihao Luo",
            "Yuwei Wang",
            "Dong Nie",
            "Lu Wang",
            "Wengqing Wu",
            "Le Zhang",
            "Massimo Poesio",
            "Juntao Yu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage. Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence. We will make all code, data, and trained models' weights publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11944",
        "abs_url": "https://arxiv.org/abs/2511.11944",
        "pdf_url": "https://arxiv.org/pdf/2511.11944",
        "title": "From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing",
        "authors": [
            "Ling Wang",
            "Yunfan Lu",
            "Wenzong Ma",
            "Huizai Yao",
            "Pengteng Li",
            "Hui Xiong"
        ],
        "comments": "11 pages, 8 figures. Completed in April 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \\textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \\textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11959",
        "abs_url": "https://arxiv.org/abs/2511.11959",
        "pdf_url": "https://arxiv.org/pdf/2511.11959",
        "title": "Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs",
        "authors": [
            "Leonardi Melo",
            "Luís Gustavo",
            "Dimmy Magalhães",
            "Lucciani Vieira",
            "Mauro Araújo"
        ],
        "comments": "14 pages, 8 figures. Preprint submitted to arXiv",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11984",
        "abs_url": "https://arxiv.org/abs/2511.11984",
        "pdf_url": "https://arxiv.org/pdf/2511.11984",
        "title": "From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology",
        "authors": [
            "Zhenhao Guo",
            "Rachit Saluja",
            "Tianyuan Yao",
            "Quan Liu",
            "Junchao Zhu",
            "Haibo Wang",
            "Daniel Reisenbüchler",
            "Yuankai Huo",
            "Benjamin Liechty",
            "David J. Pisapia",
            "Kenji Ikemura",
            "Steven Salvatoree",
            "Surya Seshane",
            "Mert R. Sabuncu",
            "Yihe Yang",
            "Ruining Deng"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11989",
        "abs_url": "https://arxiv.org/abs/2511.11989",
        "pdf_url": "https://arxiv.org/pdf/2511.11989",
        "title": "BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups",
        "authors": [
            "Songsong Zhang",
            "Chuanqi Tang",
            "Hongguang Zhang",
            "Guijian Tang",
            "Minglong Li",
            "Xueqiong Li",
            "Shaowu Yang",
            "Yuanxi Peng",
            "Wenjing Yang",
            "Jing Zhao"
        ],
        "comments": "9 pages, 10 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial this http URL methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11993",
        "abs_url": "https://arxiv.org/abs/2511.11993",
        "pdf_url": "https://arxiv.org/pdf/2511.11993",
        "title": "Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks",
        "authors": [
            "Jiaming Liang",
            "Chi-Man Pun"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12005",
        "abs_url": "https://arxiv.org/abs/2511.12005",
        "pdf_url": "https://arxiv.org/pdf/2511.12005",
        "title": "LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation",
        "authors": [
            "Xinyu He",
            "Botong Zhao",
            "Bingbing Li",
            "Shujing Lyu",
            "Jiwei Shen",
            "Yue Lu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Networking and Internet Architecture (cs.NI)",
        "abstract": "Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12006",
        "abs_url": "https://arxiv.org/abs/2511.12006",
        "pdf_url": "https://arxiv.org/pdf/2511.12006",
        "title": "Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy",
        "authors": [
            "Kai-Wen K. Yang",
            "Andrew Bai",
            "Alexandra Bermudez",
            "Yunqi Hong",
            "Zoe Latham",
            "Iris Sloan",
            "Michael Liu",
            "Vishrut Goyal",
            "Cho-Jui Hsieh",
            "Neil Y.C. Lin"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12018",
        "abs_url": "https://arxiv.org/abs/2511.12018",
        "pdf_url": "https://arxiv.org/pdf/2511.12018",
        "title": "Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis",
        "authors": [
            "Shounak Ray Chaudhuri",
            "Arash Jahangiri",
            "Christopher Paolini"
        ],
        "comments": "8 pages, 10 figures, Submitted to IEEE Intelligent Vehicles Symposium 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12020",
        "abs_url": "https://arxiv.org/abs/2511.12020",
        "pdf_url": "https://arxiv.org/pdf/2511.12020",
        "title": "LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension",
        "authors": [
            "Xianglong Shi",
            "Silin Cheng",
            "Sirui Zhao",
            "Yunhan Jiang",
            "Enhong Chen",
            "Yang Liu",
            "Sebastien Ourselin"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\\%. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12024",
        "abs_url": "https://arxiv.org/abs/2511.12024",
        "pdf_url": "https://arxiv.org/pdf/2511.12024",
        "title": "Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging",
        "authors": [
            "Jose Reinaldo Cunha Santos A V Silva Neto",
            "Hodaka Kawachi",
            "Yasushi Yagi",
            "Tomoya Nakamura"
        ],
        "comments": "8 pages without reference, 6 figures, 1 table",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12026",
        "abs_url": "https://arxiv.org/abs/2511.12026",
        "pdf_url": "https://arxiv.org/pdf/2511.12026",
        "title": "Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark",
        "authors": [
            "Rulin Zhou",
            "Wenlong He",
            "An Wang",
            "Jianhang Zhang",
            "Xuanhui Zeng",
            "Xi Zhang",
            "Chaowei Zhu",
            "Haijun Hu",
            "Hongliang Ren"
        ],
        "comments": "AAAI 2026 oral",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12027",
        "abs_url": "https://arxiv.org/abs/2511.12027",
        "pdf_url": "https://arxiv.org/pdf/2511.12027",
        "title": "GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory",
        "authors": [
            "Jeong Hun Yeo",
            "Sangyun Chung",
            "Sungjune Park",
            "Dae Hoe Kim",
            "Jinyoung Moon",
            "Yong Man Ro"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\\% accuracy on the Long split and the highest overall average (71.9\\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12030",
        "abs_url": "https://arxiv.org/abs/2511.12030",
        "pdf_url": "https://arxiv.org/pdf/2511.12030",
        "title": "VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation",
        "authors": [
            "Jun Zhou",
            "Chi Xu",
            "Kaifeng Tang",
            "Yuting Ge",
            "Tingrui Guo",
            "Li Cheng"
        ],
        "comments": "14 pages, 9 figures, extended version of the AAAI 2026 paper \"VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation\"",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12032",
        "abs_url": "https://arxiv.org/abs/2511.12032",
        "pdf_url": "https://arxiv.org/pdf/2511.12032",
        "title": "Improved Masked Image Generation with Knowledge-Augmented Token Representations",
        "authors": [
            "Guotao Liang",
            "Baoquan Zhang",
            "Zhiyuan Wen",
            "Zihao Han",
            "Yunming Ye"
        ],
        "comments": "AAAI-26",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12034",
        "abs_url": "https://arxiv.org/abs/2511.12034",
        "pdf_url": "https://arxiv.org/pdf/2511.12034",
        "title": "Calibrated Multimodal Representation Learning with Missing Modalities",
        "authors": [
            "Xiaohao Liu",
            "Xiaobo Xia",
            "Jiaheng Wei",
            "Shuo Yang",
            "Xiu Su",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12040",
        "abs_url": "https://arxiv.org/abs/2511.12040",
        "pdf_url": "https://arxiv.org/pdf/2511.12040",
        "title": "SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images",
        "authors": [
            "Xinyuan Hu",
            "Changyue Shi",
            "Chuxiao Yang",
            "Minghao Chen",
            "Jiajun Ding",
            "Tao Wei",
            "Chen Wei",
            "Zhou Yu",
            "Min Tan"
        ],
        "comments": "AAAI2026-Oral. Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \\textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \\textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \\textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \\textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12044",
        "abs_url": "https://arxiv.org/abs/2511.12044",
        "pdf_url": "https://arxiv.org/pdf/2511.12044",
        "title": "FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification",
        "authors": [
            "Cheng-Chang Tsai",
            "Kai-Wen Cheng",
            "Chun-Shien Lu"
        ],
        "comments": "Extended version. 22 pages, 18 figures, 6 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12047",
        "abs_url": "https://arxiv.org/abs/2511.12047",
        "pdf_url": "https://arxiv.org/pdf/2511.12047",
        "title": "DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging",
        "authors": [
            "Huimin Cheng",
            "Xiaowei Yu",
            "Shushan Wu",
            "Luyang Fang",
            "Chao Cao",
            "Jing Zhang",
            "Tianming Liu",
            "Dajiang Zhu",
            "Wenxuan Zhong",
            "Ping Ma"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12048",
        "abs_url": "https://arxiv.org/abs/2511.12048",
        "pdf_url": "https://arxiv.org/pdf/2511.12048",
        "title": "DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training",
        "authors": [
            "Saksham Kumar",
            "Ashish Singh",
            "Srinivasarao Thota",
            "Sunil Kumar Singh",
            "Chandan Kumar"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Cryptography and Security (cs.CR)",
        "abstract": "Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\\% accuracy after stage one and 99.22\\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12054",
        "abs_url": "https://arxiv.org/abs/2511.12054",
        "pdf_url": "https://arxiv.org/pdf/2511.12054",
        "title": "UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization",
        "authors": [
            "Cuiqun Chen",
            "Qi Chen",
            "Bin Yang",
            "Xingyi Zhang"
        ],
        "comments": "Accepted as Oral Presentation at AAAI 2026. 10 pages, 9 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Cross-view geo-localization (CVGL) matches query images ($\\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\\rightarrow$ Drone AP by +10.63\\% on University-1652 and +16.73\\% on SUES-200, even surpassing supervised baselines. The source code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12056",
        "abs_url": "https://arxiv.org/abs/2511.12056",
        "pdf_url": "https://arxiv.org/pdf/2511.12056",
        "title": "PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling",
        "authors": [
            "Sijie Wang",
            "Qiang Wang",
            "Shaohuai Shi"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12061",
        "abs_url": "https://arxiv.org/abs/2511.12061",
        "pdf_url": "https://arxiv.org/pdf/2511.12061",
        "title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity",
        "authors": [
            "Zhichen Lai",
            "Hua Lu",
            "Huan Li",
            "Jialiang Li",
            "Christian S. Jensen"
        ],
        "comments": "8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12066",
        "abs_url": "https://arxiv.org/abs/2511.12066",
        "pdf_url": "https://arxiv.org/pdf/2511.12066",
        "title": "DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal",
        "authors": [
            "Jialang Lu",
            "Shuning Sun",
            "Pu Wang",
            "Chen Wu",
            "Feng Gao",
            "Lina Gong",
            "Dianjie Lu",
            "Guijuan Zhang",
            "Zhuoran Zheng"
        ],
        "comments": "11 pages, 9 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel\", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12077",
        "abs_url": "https://arxiv.org/abs/2511.12077",
        "pdf_url": "https://arxiv.org/pdf/2511.12077",
        "title": "Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound",
        "authors": [
            "Dengming Zhang",
            "Weitao You",
            "Jingxiong Li",
            "Weishen Lin",
            "Wenda Shi",
            "Xue Zhao",
            "Heda Zuo",
            "Junxian Wu",
            "Lingyun Sun"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12079",
        "abs_url": "https://arxiv.org/abs/2511.12079",
        "pdf_url": "https://arxiv.org/pdf/2511.12079",
        "title": "Point Cloud Quantization through Multimodal Prompting for 3D Understanding",
        "authors": [
            "Hongxuan Li",
            "Wencheng Zhu",
            "Huiying Xu",
            "Xinzhong Zhu",
            "Pengfei Zhu"
        ],
        "comments": "Accepted by AAAI 2026. 11 pages, 7 figures. Corresponding author: Wencheng Zhu (wenchengzhu@tju.this http URL)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12082",
        "abs_url": "https://arxiv.org/abs/2511.12082",
        "pdf_url": "https://arxiv.org/pdf/2511.12082",
        "title": "Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning",
        "authors": [
            "Lokender Singh",
            "Saksham Kumar",
            "Chandan Kumar"
        ],
        "comments": "ICCCNT 2025 Conference Proceedings, IIT Indore",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12084",
        "abs_url": "https://arxiv.org/abs/2511.12084",
        "pdf_url": "https://arxiv.org/pdf/2511.12084",
        "title": "SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving",
        "authors": [
            "Ji-Ping Jin",
            "Chen-Bin Feng",
            "Rui Fan",
            "Chi-Man Vong"
        ],
        "comments": "12pages, has been early accepted by The Visual Computer: International Journal of Computer Graphics, 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12090",
        "abs_url": "https://arxiv.org/abs/2511.12090",
        "pdf_url": "https://arxiv.org/pdf/2511.12090",
        "title": "Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning",
        "authors": [
            "Shengqin Jiang",
            "Tianqi Kong",
            "Yuankai Qi",
            "Haokui Zhang",
            "Lina Yao",
            "Quan Z. Sheng",
            "Qingshan Liu",
            "Ming-Hsuan Yang"
        ],
        "comments": "under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12095",
        "abs_url": "https://arxiv.org/abs/2511.12095",
        "pdf_url": "https://arxiv.org/pdf/2511.12095",
        "title": "Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio",
        "authors": [
            "Shuhan Ye",
            "Yi Yu",
            "Qixin Zhang",
            "Chenqi Kong",
            "Qiangqiang Wu",
            "Kun Wang",
            "Xudong Jiang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \\textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \\textbf{ST-DSM} and \\textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \\(84.4\\%\\) accuracy, about \\(85\\%\\) of the full training set performance, while reducing training time by more than \\(50\\times\\) and storage cost by \\(6000\\times\\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12097",
        "abs_url": "https://arxiv.org/abs/2511.12097",
        "pdf_url": "https://arxiv.org/pdf/2511.12097",
        "title": "Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks",
        "authors": [
            "Shuhan Ye",
            "Yi Yu",
            "Qixin Zhang",
            "Chenqi Kong",
            "Qiangqiang Wu",
            "Xudong Jiang",
            "Dacheng Tao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \\emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \\emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \\textbf{SpikeNM}, the first SNN-oriented \\emph{semi-structured} \\(N{:}M\\) pruning framework that learns sparse SNNs \\emph{from scratch}, enforcing \\emph{at most \\(N\\)} non-zeros per \\(M\\)-weight block. To avoid the combinatorial space complexity \\(\\sum_{k=1}^{N}\\binom{M}{k}\\) growing exponentially with \\(M\\), SpikeNM adopts an \\(M\\)-way basis-logit parameterization with a differentiable top-\\(k\\) sampler, \\emph{linearizing} per-block complexity to \\(\\mathcal O(M)\\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \\emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \\(2{:}4\\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12098",
        "abs_url": "https://arxiv.org/abs/2511.12098",
        "pdf_url": "https://arxiv.org/pdf/2511.12098",
        "title": "DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT",
        "authors": [
            "Xianhao Zhou",
            "Jianghao Wu",
            "Ku Zhao",
            "Jinlong He",
            "Huangxuan Zhao",
            "Lei Chen",
            "Shaoting Zhang",
            "Guotai Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\\rightarrow$CT and CBCT$\\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12099",
        "abs_url": "https://arxiv.org/abs/2511.12099",
        "pdf_url": "https://arxiv.org/pdf/2511.12099",
        "title": "Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models",
        "authors": [
            "Tianle Cheng",
            "Zeyan Zhang",
            "Kaifeng Gao",
            "Jun Xiao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12100",
        "abs_url": "https://arxiv.org/abs/2511.12100",
        "pdf_url": "https://arxiv.org/pdf/2511.12100",
        "title": "Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation",
        "authors": [
            "Yannan Chen",
            "Ruoyu Chen",
            "Bin Zeng",
            "Wei Wang",
            "Shiming Liu",
            "Qunli Zhang",
            "Zheng Hu",
            "Laiyuan Wang",
            "Yaowei Wang",
            "Xiaochun Cao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12103",
        "abs_url": "https://arxiv.org/abs/2511.12103",
        "pdf_url": "https://arxiv.org/pdf/2511.12103",
        "title": "BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation",
        "authors": [
            "Sayad Ibna Azad",
            "Md. Atiqur Rahman"
        ],
        "comments": "Accepted to 20th International Symposium on Visual Computing (ISVC 2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12104",
        "abs_url": "https://arxiv.org/abs/2511.12104",
        "pdf_url": "https://arxiv.org/pdf/2511.12104",
        "title": "TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery",
        "authors": [
            "Tammy Glazer",
            "Gilles Q. Hacheme",
            "Akram Zaytar",
            "Luana Marotti",
            "Amy Michaels",
            "Girmaw Abebe Tadesse",
            "Kevin White",
            "Rahul Dodhia",
            "Andrew Zolli",
            "Inbal Becker-Reshef",
            "Juan M. Lavista Ferres",
            "Caleb Robinson"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12107",
        "abs_url": "https://arxiv.org/abs/2511.12107",
        "pdf_url": "https://arxiv.org/pdf/2511.12107",
        "title": "Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection",
        "authors": [
            "Tianxiang Zhang",
            "Peipeng Yu",
            "Zhihua Xia",
            "Longchen Dai",
            "Xiaoyu Zhou",
            "Hui Gao"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12110",
        "abs_url": "https://arxiv.org/abs/2511.12110",
        "pdf_url": "https://arxiv.org/pdf/2511.12110",
        "title": "MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images",
        "authors": [
            "Qinyue Tong",
            "Ziqian Lu",
            "Jun Liu",
            "Rui Zuo",
            "Zheming Lu"
        ],
        "comments": "12pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12117",
        "abs_url": "https://arxiv.org/abs/2511.12117",
        "pdf_url": "https://arxiv.org/pdf/2511.12117",
        "title": "RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving",
        "authors": [
            "Ruiqi Cheng",
            "Huijun Di",
            "Jian Li",
            "Feng Liu",
            "Wei Liang"
        ],
        "comments": "12 pages, 6 figures. Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12131",
        "abs_url": "https://arxiv.org/abs/2511.12131",
        "pdf_url": "https://arxiv.org/pdf/2511.12131",
        "title": "OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description",
        "authors": [
            "Quanxing Xu",
            "Ling Zhou",
            "Feifei Zhang",
            "Jinyu Tian",
            "Rubing Huang"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12136",
        "abs_url": "https://arxiv.org/abs/2511.12136",
        "pdf_url": "https://arxiv.org/pdf/2511.12136",
        "title": "Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware",
        "authors": [
            "Karol C. Jurzec",
            "Tomasz Szydlo",
            "Maciej Wielgosz"
        ],
        "comments": "6 pages, 6 figures, 1 table; code available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12142",
        "abs_url": "https://arxiv.org/abs/2511.12142",
        "pdf_url": "https://arxiv.org/pdf/2511.12142",
        "title": "MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering",
        "authors": [
            "Seokwon Song",
            "Minsu Park",
            "Gunhee Kim"
        ],
        "comments": "Accepted for publication in the Association for the Advancement of Artificial Intelligence (AAAI), 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12150",
        "abs_url": "https://arxiv.org/abs/2511.12150",
        "pdf_url": "https://arxiv.org/pdf/2511.12150",
        "title": "Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain",
        "authors": [
            "Yuqi Xie",
            "Shuhan Ye",
            "Yi Yu",
            "Chong Wang",
            "Qixin Zhang",
            "Jiazhen Xu",
            "Le Shen",
            "Yuanbin Qian",
            "Jiangbo Qian",
            "Guoqi Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12151",
        "abs_url": "https://arxiv.org/abs/2511.12151",
        "pdf_url": "https://arxiv.org/pdf/2511.12151",
        "title": "FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing",
        "authors": [
            "Kaixiang Yang",
            "Boyang Shen",
            "Xin Li",
            "Yuchen Dai",
            "Yuxuan Luo",
            "Yueran Ma",
            "Wei Fang",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "comments": "AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12162",
        "abs_url": "https://arxiv.org/abs/2511.12162",
        "pdf_url": "https://arxiv.org/pdf/2511.12162",
        "title": "Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function",
        "authors": [
            "Shuo Yin",
            "Zhiyuan Yin",
            "Yuqing Hou",
            "Rui Liu",
            "Yong Chen",
            "Dell Zhang"
        ],
        "comments": "14 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12170",
        "abs_url": "https://arxiv.org/abs/2511.12170",
        "pdf_url": "https://arxiv.org/pdf/2511.12170",
        "title": "Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective",
        "authors": [
            "Wang Luo",
            "Di Wu",
            "Hengyuan Na",
            "Yinlin Zhu",
            "Miao Hu",
            "Guocong Quan"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12181",
        "abs_url": "https://arxiv.org/abs/2511.12181",
        "pdf_url": "https://arxiv.org/pdf/2511.12181",
        "title": "MixAR: Mixture Autoregressive Image Generation",
        "authors": [
            "Jinyuan Hu",
            "Jiayou Zhang",
            "Shaobo Cui",
            "Kun Zhang",
            "Guangyi Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12193",
        "abs_url": "https://arxiv.org/abs/2511.12193",
        "pdf_url": "https://arxiv.org/pdf/2511.12193",
        "title": "MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis",
        "authors": [
            "Abdelrahman Elsayed",
            "Ahmed Jaheen",
            "Mohammad Yaqub"
        ],
        "comments": "Under Review at The IEEE International Symposium on Biomedical Imaging (ISBI 2026)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: this http URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12196",
        "abs_url": "https://arxiv.org/abs/2511.12196",
        "pdf_url": "https://arxiv.org/pdf/2511.12196",
        "title": "Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System",
        "authors": [
            "Aditi Bhalla",
            "Christian Hellert",
            "Enkelejda Kasneci"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
        "abstract": "Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12200",
        "abs_url": "https://arxiv.org/abs/2511.12200",
        "pdf_url": "https://arxiv.org/pdf/2511.12200",
        "title": "Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation",
        "authors": [
            "Sujun Sun",
            "Haowen Gu",
            "Cheng Xie",
            "Yanxu Ren",
            "Mingwu Ren",
            "Haofeng Zhang"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12201",
        "abs_url": "https://arxiv.org/abs/2511.12201",
        "pdf_url": "https://arxiv.org/pdf/2511.12201",
        "title": "OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs",
        "authors": [
            "Feng Chen",
            "Yefei He",
            "Shaoxuan He",
            "Yuanyu He",
            "Jing Liu",
            "Lequan Lin",
            "Akide Liu",
            "Zhaoyang Li",
            "Jiyuan Zhang",
            "Zhenbang Sun",
            "Bohan Zhuang",
            "Qi Wu"
        ],
        "comments": "Accepted by AAAI2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12202",
        "abs_url": "https://arxiv.org/abs/2511.12202",
        "pdf_url": "https://arxiv.org/pdf/2511.12202",
        "title": "LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image",
        "authors": [
            "Zhuojiang Cai",
            "Yiheng Zhang",
            "Meitong Guo",
            "Mingdao Wang",
            "Yuwang Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12204",
        "abs_url": "https://arxiv.org/abs/2511.12204",
        "pdf_url": "https://arxiv.org/pdf/2511.12204",
        "title": "GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction",
        "authors": [
            "Jiaqi Wu",
            "Yaosen Chen",
            "Shuyuan Zhu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12206",
        "abs_url": "https://arxiv.org/abs/2511.12206",
        "pdf_url": "https://arxiv.org/pdf/2511.12206",
        "title": "A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR",
        "authors": [
            "Nishant Vasantkumar Hegde",
            "Aditi Agarwal",
            "Minal Moharir"
        ],
        "comments": "6 pages, 4 figures. Published in: Proceedings of the 12th International Conference on Emerging Trends in Engineering Technology Signal and Information Processing (ICETET SIP 2025) Note: The conference proceedings contain an outdated abstract due to a publisher-side error. This arXiv version includes the correct and updated abstract",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12207",
        "abs_url": "https://arxiv.org/abs/2511.12207",
        "pdf_url": "https://arxiv.org/pdf/2511.12207",
        "title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation",
        "authors": [
            "Haozhe Liu",
            "Ding Liu",
            "Mingchen Zhuge",
            "Zijian Zhou",
            "Tian Xie",
            "Sen He",
            "Yukang Yang",
            "Shuming Liu",
            "Yuren Cong",
            "Jiadong Guo",
            "Hongyu Xu",
            "Ke Xu",
            "Kam-Woh Ng",
            "Juan C. Pérez",
            "Juan-ManuelPérez-Rúa",
            "Tao Xiang",
            "Wei Liu",
            "Shikun Liu",
            "Jürgen Schmidhuber"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $\\epsilon$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12215",
        "abs_url": "https://arxiv.org/abs/2511.12215",
        "pdf_url": "https://arxiv.org/pdf/2511.12215",
        "title": "FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention",
        "authors": [
            "Peng Zhang",
            "Zhihui Lai",
            "Wenting Chen",
            "Xu Wu",
            "Heng Kong"
        ],
        "comments": "AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12220",
        "abs_url": "https://arxiv.org/abs/2511.12220",
        "pdf_url": "https://arxiv.org/pdf/2511.12220",
        "title": "Suppressing VLM Hallucinations with Spectral Representation Filtering",
        "authors": [
            "Ameen Ali",
            "Tamim Zoabi",
            "Lior Wolf"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12233",
        "abs_url": "https://arxiv.org/abs/2511.12233",
        "pdf_url": "https://arxiv.org/pdf/2511.12233",
        "title": "Model Inversion Attack Against Deep Hashing",
        "authors": [
            "Dongdong Zhao",
            "Qiben Xu",
            "Ranxin Fang",
            "Baogang Song"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12255",
        "abs_url": "https://arxiv.org/abs/2511.12255",
        "pdf_url": "https://arxiv.org/pdf/2511.12255",
        "title": "Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets",
        "authors": [
            "Huy M. Le",
            "Dat Tien Nguyen",
            "Phuc Binh Nguyen",
            "Gia-Bao Le-Tran",
            "Phu Truong Thien",
            "Cuong Dinh",
            "Minh Nguyen",
            "Nga Nguyen",
            "Thuy T. N. Nguyen",
            "Huy Gia Ngo",
            "Tan Nhat Nguyen",
            "Binh T. Nguyen",
            "Monojit Choudhury"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12256",
        "abs_url": "https://arxiv.org/abs/2511.12256",
        "pdf_url": "https://arxiv.org/pdf/2511.12256",
        "title": "Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment",
        "authors": [
            "Tolga Demiroglu",
            "Mehmet Ozan Unal",
            "Metin Ertas",
            "Isa Yildirim"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12259",
        "abs_url": "https://arxiv.org/abs/2511.12259",
        "pdf_url": "https://arxiv.org/pdf/2511.12259",
        "title": "A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation",
        "authors": [
            "Puzhen Wu",
            "Hexin Dong",
            "Yi Lin",
            "Yihao Ding",
            "Yifan Peng"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12263",
        "abs_url": "https://arxiv.org/abs/2511.12263",
        "pdf_url": "https://arxiv.org/pdf/2511.12263",
        "title": "CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models",
        "authors": [
            "Jingyao Li",
            "Jingyun Wang",
            "Molin Tan",
            "Haochen Wang",
            "Cilin Yan",
            "Likun Shi",
            "Jiayin Cai",
            "Xiaolong Jiang",
            "Yao Hu"
        ],
        "comments": "30 pages, 28 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12267",
        "abs_url": "https://arxiv.org/abs/2511.12267",
        "pdf_url": "https://arxiv.org/pdf/2511.12267",
        "title": "ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks",
        "authors": [
            "Ruixun Liu",
            "Bowen Fu",
            "Jiayi Song",
            "Kaiyu Li",
            "Wanchen Li",
            "Lanxuan Xue",
            "Hui Qiao",
            "Weizhan Zhang",
            "Deyu Meng",
            "Xiangyong Cao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12270",
        "abs_url": "https://arxiv.org/abs/2511.12270",
        "pdf_url": "https://arxiv.org/pdf/2511.12270",
        "title": "TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation",
        "authors": [
            "Yaxuan Jiao",
            "Qing Xu",
            "Yuxiang Luo",
            "Xiangjian He",
            "Zhen Chen",
            "Wenting Duan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12280",
        "abs_url": "https://arxiv.org/abs/2511.12280",
        "pdf_url": "https://arxiv.org/pdf/2511.12280",
        "title": "D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs",
        "authors": [
            "Shuochen Chang",
            "Xiaofeng Zhang",
            "Qingyang Liu",
            "Li Niu"
        ],
        "comments": "Accepted by AAAI Conference on Artificial Intelligence (AAAI) 2026. Code available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12291",
        "abs_url": "https://arxiv.org/abs/2511.12291",
        "pdf_url": "https://arxiv.org/pdf/2511.12291",
        "title": "One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving",
        "authors": [
            "Andrea Bertogalli",
            "Giacomo Boracchi",
            "Luca Magri"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12301",
        "abs_url": "https://arxiv.org/abs/2511.12301",
        "pdf_url": "https://arxiv.org/pdf/2511.12301",
        "title": "Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method",
        "authors": [
            "Chi Liu",
            "Jincheng Liu",
            "Congcong Zhu",
            "Minghao Wang",
            "Sheng Shen",
            "Jia Gu",
            "Tianqing Zhu",
            "Wanlei Zhou"
        ],
        "comments": "Accepted for AAAI 2026 (Main Track Poster)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12304",
        "abs_url": "https://arxiv.org/abs/2511.12304",
        "pdf_url": "https://arxiv.org/pdf/2511.12304",
        "title": "LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors",
        "authors": [
            "Qifeng Chen",
            "Jiarun Liu",
            "Rengan Xie",
            "Tao Tang",
            "Sicong Du",
            "Yiru Zhao",
            "Yuchi Huo",
            "Sheng Yang"
        ],
        "comments": "Accepted by AAAI-26",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12321",
        "abs_url": "https://arxiv.org/abs/2511.12321",
        "pdf_url": "https://arxiv.org/pdf/2511.12321",
        "title": "Learning Time in Static Classifiers",
        "authors": [
            "Xi Ding",
            "Lei Wang",
            "Piotr Koniusz",
            "Yongsheng Gao"
        ],
        "comments": "Accepted at the Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12331",
        "abs_url": "https://arxiv.org/abs/2511.12331",
        "pdf_url": "https://arxiv.org/pdf/2511.12331",
        "title": "SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models",
        "authors": [
            "Sepehr Kazemi Ranjbar",
            "Kumail Alhamoud",
            "Marzyeh Ghassemi"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-Language Models (VLMs) struggle with negation. Given a prompt like \"retrieve (or generate) a street scene without pedestrians,\" they often fail to respect the \"not.\" Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as \"A but not N,\" we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12342",
        "abs_url": "https://arxiv.org/abs/2511.12342",
        "pdf_url": "https://arxiv.org/pdf/2511.12342",
        "title": "Ground Plane Projection for Improved Traffic Analytics at Intersections",
        "authors": [
            "Sajjad Pakdamansavoji",
            "Kumar Vaibhav Jha",
            "Baher Abdulhai",
            "James H Elder"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12346",
        "abs_url": "https://arxiv.org/abs/2511.12346",
        "pdf_url": "https://arxiv.org/pdf/2511.12346",
        "title": "CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification",
        "authors": [
            "Asmit Bandyopadhyay",
            "Anindita Das Bhattacharjee",
            "Rakesh Das"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\\mathcal{O}(T^2D)$ to $\\mathcal{O}(T\\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12363",
        "abs_url": "https://arxiv.org/abs/2511.12363",
        "pdf_url": "https://arxiv.org/pdf/2511.12363",
        "title": "Explainable AI-Generated Image Detection RewardBench",
        "authors": [
            "Michael Yang",
            "Shijian Deng",
            "William T. Doan",
            "Kai Wang",
            "Tianyu Yang",
            "Harsh Singh",
            "Yapeng Tian"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an \"MLLM as a judge\" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \\textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\\% on this benchmark (while human inter-annotator agreement reaches 98.30\\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12365",
        "abs_url": "https://arxiv.org/abs/2511.12365",
        "pdf_url": "https://arxiv.org/pdf/2511.12365",
        "title": "Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning",
        "authors": [
            "Yiqing Shen",
            "Mathias Unberath"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12368",
        "abs_url": "https://arxiv.org/abs/2511.12368",
        "pdf_url": "https://arxiv.org/pdf/2511.12368",
        "title": "Fast Reasoning Segmentation for Images and Videos",
        "authors": [
            "Yiqing Shen",
            "Mathias Unberath"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12370",
        "abs_url": "https://arxiv.org/abs/2511.12370",
        "pdf_url": "https://arxiv.org/pdf/2511.12370",
        "title": "Changes in Real Time: Online Scene Change Detection with Multi-View Fusion",
        "authors": [
            "Chamuditha Jayanga Galappaththige",
            "Jason Lai",
            "Lloyd Windrim",
            "Donald Dansereau",
            "Niko Sünderhauf",
            "Dimity Miller"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12371",
        "abs_url": "https://arxiv.org/abs/2511.12371",
        "pdf_url": "https://arxiv.org/pdf/2511.12371",
        "title": "Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models",
        "authors": [
            "Yiqing Shen",
            "Chenxiao Fan",
            "Chenjia Li",
            "Mathias Unberath"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12382",
        "abs_url": "https://arxiv.org/abs/2511.12382",
        "pdf_url": "https://arxiv.org/pdf/2511.12382",
        "title": "AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification",
        "authors": [
            "Ansh Makwe",
            "Akansh Agrawal",
            "Prateek Jain",
            "Akshan Agrawal",
            "Priyanka Bagade"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12386",
        "abs_url": "https://arxiv.org/abs/2511.12386",
        "pdf_url": "https://arxiv.org/pdf/2511.12386",
        "title": "Leveraging Quantum-Based Architectures for Robust Diagnostics",
        "authors": [
            "Shabnam Sodagari",
            "Tommy Long"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12389",
        "abs_url": "https://arxiv.org/abs/2511.12389",
        "pdf_url": "https://arxiv.org/pdf/2511.12389",
        "title": "Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation",
        "authors": [
            "Divake Kumar",
            "Patrick Poggi",
            "Sina Tayebati",
            "Devashri Naik",
            "Nilesh Ahuja",
            "Amit Ranjan Trivedi"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)",
        "abstract": "Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12400",
        "abs_url": "https://arxiv.org/abs/2511.12400",
        "pdf_url": "https://arxiv.org/pdf/2511.12400",
        "title": "MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting",
        "authors": [
            "Xu Yang",
            "Gady Agam"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly modulates spatial and channel attention. The two components are fused through pointwise multiplication and a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained weights frozen. Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification, detection, and segmentation tasks with roughly less than 5\\% of backbone parameters. The design further enables stable optimization, fast convergence, and strong cross-architecture generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach for efficient adaptation of frozen vision backbones.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12405",
        "abs_url": "https://arxiv.org/abs/2511.12405",
        "pdf_url": "https://arxiv.org/pdf/2511.12405",
        "title": "VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving",
        "authors": [
            "Hyunki Seong",
            "Seongwoo Moon",
            "Hojin Ahn",
            "Jehun Kang",
            "David Hyunchul Shim"
        ],
        "comments": "9 pages, 9 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12410",
        "abs_url": "https://arxiv.org/abs/2511.12410",
        "pdf_url": "https://arxiv.org/pdf/2511.12410",
        "title": "Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection",
        "authors": [
            "Xi Xiao",
            "Zhuxuanzi Wang",
            "Mingqiao Mo",
            "Chen Liu",
            "Chenrui Ma",
            "Yanshu Li",
            "Smita Krishnaswamy",
            "Xiao Wang",
            "Tianyang Wang"
        ],
        "comments": "Accepted by WACV 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \\ours, a self-supervised framework that \\emph{visually probes} target domains without labels. \\ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \\ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12415",
        "abs_url": "https://arxiv.org/abs/2511.12415",
        "pdf_url": "https://arxiv.org/pdf/2511.12415",
        "title": "Towards Rotation-only Imaging Geometry: Rotation Estimation",
        "authors": [
            "Xinrui Li",
            "Qi Cai",
            "Yuanxin Wu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12419",
        "abs_url": "https://arxiv.org/abs/2511.12419",
        "pdf_url": "https://arxiv.org/pdf/2511.12419",
        "title": "Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance",
        "authors": [
            "Wenjie Li",
            "Jinglei Shi",
            "Jin Han",
            "Heng Guo",
            "Zhanyu Ma"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12422",
        "abs_url": "https://arxiv.org/abs/2511.12422",
        "pdf_url": "https://arxiv.org/pdf/2511.12422",
        "title": "MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation",
        "authors": [
            "Nuolin Sun",
            "Linyuan Wang",
            "Haonan Wei",
            "Lei Li",
            "Bin Yan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12428",
        "abs_url": "https://arxiv.org/abs/2511.12428",
        "pdf_url": "https://arxiv.org/pdf/2511.12428",
        "title": "RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning",
        "authors": [
            "Jingqi Xu",
            "Jingxi Lu",
            "Chenghao Li",
            "Sreetama Sarkar",
            "Souvik Kundu",
            "Peter A. Beerel"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12432",
        "abs_url": "https://arxiv.org/abs/2511.12432",
        "pdf_url": "https://arxiv.org/pdf/2511.12432",
        "title": "Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion",
        "authors": [
            "Xilai Li",
            "Xiaosong Li",
            "Weijun Jiang"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12438",
        "abs_url": "https://arxiv.org/abs/2511.12438",
        "pdf_url": "https://arxiv.org/pdf/2511.12438",
        "title": "Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning",
        "authors": [
            "ANK Zaman",
            "Prosenjit Chatterjee",
            "Rajat Sharma"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and this http URL proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car this http URL potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12446",
        "abs_url": "https://arxiv.org/abs/2511.12446",
        "pdf_url": "https://arxiv.org/pdf/2511.12446",
        "title": "CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training",
        "authors": [
            "Jiahe Qian",
            "Yuhao Shen",
            "Zhangtianyi Chen",
            "Juexiao Zhou",
            "Peisong Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12449",
        "abs_url": "https://arxiv.org/abs/2511.12449",
        "pdf_url": "https://arxiv.org/pdf/2511.12449",
        "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding",
        "authors": [
            "Zhanheng Nie",
            "Chenghan Fu",
            "Daoze Zhang",
            "Junxian Wu",
            "Wanxian Guan",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "comments": "11 pages, 7 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12452",
        "abs_url": "https://arxiv.org/abs/2511.12452",
        "pdf_url": "https://arxiv.org/pdf/2511.12452",
        "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions",
        "authors": [
            "Xiaoyu Lin",
            "Aniket Ghorpade",
            "Hansheng Zhu",
            "Justin Qiu",
            "Dea Rrozhani",
            "Monica Lama",
            "Mick Yang",
            "Zixuan Bian",
            "Ruohan Ren",
            "Alan B. Hong",
            "Jiatao Gu",
            "Chris Callison-Burch"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)",
        "abstract": "With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12474",
        "abs_url": "https://arxiv.org/abs/2511.12474",
        "pdf_url": "https://arxiv.org/pdf/2511.12474",
        "title": "Co-Layout: LLM-driven Co-optimization for Interior Layout",
        "authors": [
            "Chucheng Xiang",
            "Ruchao Bao",
            "Biyin Feng",
            "Wenzheng Wu",
            "Zhongyuan Liu",
            "Yirui Guan",
            "Ligang Liu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Graphics (cs.GR)",
        "abstract": "We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor\". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12480",
        "abs_url": "https://arxiv.org/abs/2511.12480",
        "pdf_url": "https://arxiv.org/pdf/2511.12480",
        "title": "MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning",
        "authors": [
            "Jingshan Hong",
            "Haigen Hu",
            "Huihuang Zhang",
            "Qianwei Zhou",
            "Zhao Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12498",
        "abs_url": "https://arxiv.org/abs/2511.12498",
        "pdf_url": "https://arxiv.org/pdf/2511.12498",
        "title": "Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion",
        "authors": [
            "Jongseong Bae",
            "Junwoo Ha",
            "Jinnyeong Heo",
            "Yeongin Lee",
            "Ha Young Kim"
        ],
        "comments": "Accepted to AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12503",
        "abs_url": "https://arxiv.org/abs/2511.12503",
        "pdf_url": "https://arxiv.org/pdf/2511.12503",
        "title": "Visible Structure Retrieval for Lightweight Image-Based Relocalisation",
        "authors": [
            "Fereidoon Zangeneh",
            "Leonard Bruns",
            "Amit Dekel",
            "Alessandro Pieropan",
            "Patric Jensfelt"
        ],
        "comments": "Accepted at BMVC 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12511",
        "abs_url": "https://arxiv.org/abs/2511.12511",
        "pdf_url": "https://arxiv.org/pdf/2511.12511",
        "title": "DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection",
        "authors": [
            "Jialiang Shen",
            "Jiyang Zheng",
            "Yunqi Xue",
            "Huajie Chen",
            "Yu Yao",
            "Hui Kang",
            "Ruiqi Liu",
            "Helin Gong",
            "Yang Yang",
            "Dadong Wang",
            "Tongliang Liu"
        ],
        "comments": "12 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12525",
        "abs_url": "https://arxiv.org/abs/2511.12525",
        "pdf_url": "https://arxiv.org/pdf/2511.12525",
        "title": "MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics",
        "authors": [
            "Jing Li",
            "Yifan Wang",
            "Jiafeng Yan",
            "Renlong Zhang",
            "Bin Yang"
        ],
        "comments": "10 pages, 7 figures. Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12528",
        "abs_url": "https://arxiv.org/abs/2511.12528",
        "pdf_url": "https://arxiv.org/pdf/2511.12528",
        "title": "D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation",
        "authors": [
            "Zheyuan Zhang",
            "Jiwei Zhang",
            "Boyu Zhou",
            "Linzhimeng Duan",
            "Hong Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12530",
        "abs_url": "https://arxiv.org/abs/2511.12530",
        "pdf_url": "https://arxiv.org/pdf/2511.12530",
        "title": "ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding",
        "authors": [
            "Yuan Zhou",
            "Litao Hua",
            "Shilong Jin",
            "Wentao Huang",
            "Haoran Duan"
        ],
        "comments": "Accepted to AAAI 2026. Code is available at: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12547",
        "abs_url": "https://arxiv.org/abs/2511.12547",
        "pdf_url": "https://arxiv.org/pdf/2511.12547",
        "title": "HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models",
        "authors": [
            "Zhiguang Lu",
            "Qianqian Xu",
            "Peisong Wen",
            "Siran Da",
            "Qingming Huang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12554",
        "abs_url": "https://arxiv.org/abs/2511.12554",
        "pdf_url": "https://arxiv.org/pdf/2511.12554",
        "title": "EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis",
        "authors": [
            "Yijie Guo",
            "Dexiang Hong",
            "Weidong Chen",
            "Zihan She",
            "Cheng Ye",
            "Xiaojun Chang",
            "Zhendong Mao"
        ],
        "comments": "11 pages, 7 figures. This is a preprint version of a paper submitted to CVPR 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12559",
        "abs_url": "https://arxiv.org/abs/2511.12559",
        "pdf_url": "https://arxiv.org/pdf/2511.12559",
        "title": "SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition",
        "authors": [
            "Qing Cai",
            "Guihao Yan",
            "Fan Zhang",
            "Cheng Zhang",
            "Zhi Liu"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12572",
        "abs_url": "https://arxiv.org/abs/2511.12572",
        "pdf_url": "https://arxiv.org/pdf/2511.12572",
        "title": "Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection",
        "authors": [
            "Mohamed Youssef",
            "Lukas Brunner",
            "Klaus Rundhammer",
            "Gerald Czech",
            "Oliver Bimber"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12575",
        "abs_url": "https://arxiv.org/abs/2511.12575",
        "pdf_url": "https://arxiv.org/pdf/2511.12575",
        "title": "Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection",
        "authors": [
            "Jiayi Zhu",
            "Yihao Huang",
            "Yue Cao",
            "Xiaojun Jia",
            "Qing Guo",
            "Felix Juefei-Xu",
            "Geguang Pu",
            "Bin Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12578",
        "abs_url": "https://arxiv.org/abs/2511.12578",
        "pdf_url": "https://arxiv.org/pdf/2511.12578",
        "title": "TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction",
        "authors": [
            "Yukuo Ma",
            "Cong Liu",
            "Junke Wang",
            "Junqi Liu",
            "Haibin Huang",
            "Zuxuan Wu",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12588",
        "abs_url": "https://arxiv.org/abs/2511.12588",
        "pdf_url": "https://arxiv.org/pdf/2511.12588",
        "title": "Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting",
        "authors": [
            "Zuqi Huang",
            "Mengxin Tian",
            "Huan Liu",
            "Wentao Li",
            "Baobao Liang",
            "Jie Wu",
            "Fang Yan",
            "Zhaoqing Tang",
            "Zhongyu Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12590",
        "abs_url": "https://arxiv.org/abs/2511.12590",
        "pdf_url": "https://arxiv.org/pdf/2511.12590",
        "title": "Fine-Grained Representation for Lane Topology Reasoning",
        "authors": [
            "Guoqing Xu",
            "Yiheng Li",
            "Yang Yang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control this http URL methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane this http URL, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology this http URL this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query this http URL constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each this http URL models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching this http URL integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology this http URL experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12594",
        "abs_url": "https://arxiv.org/abs/2511.12594",
        "pdf_url": "https://arxiv.org/pdf/2511.12594",
        "title": "Seg-VAR: Image Segmentation with Visual Autoregressive Modeling",
        "authors": [
            "Rongkun Zheng",
            "Lu Qi",
            "Xi Chen",
            "Yi Wang",
            "Kun Wang",
            "Hengshuang Zhao"
        ],
        "comments": "NeurIPS 2025, 22 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11634",
        "abs_url": "https://arxiv.org/abs/2511.11634",
        "pdf_url": "https://arxiv.org/pdf/2511.11634",
        "title": "Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding",
        "authors": [
            "Michikuni Eguchi",
            "Takekazu Kitagishi",
            "Yuichi Hiroi",
            "Takefumi Hiraki"
        ],
        "comments": "3 pages, 2 figures, 1 table. Presented at SIGGRAPH Asia 2025 Posters (SA Posters '25), December 15-18, 2025, Hong Kong, Hong Kong",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11639",
        "abs_url": "https://arxiv.org/abs/2511.11639",
        "pdf_url": "https://arxiv.org/pdf/2511.11639",
        "title": "Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature",
        "authors": [
            "Jie Fan",
            "Francesco Visentin",
            "Barbara Mazzolai",
            "Emanuela Del Dottore"
        ],
        "comments": "This manuscript is a preprint version of the article currently under peer review at International Journal of Computer Vision (IJCV)",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11644",
        "abs_url": "https://arxiv.org/abs/2511.11644",
        "pdf_url": "https://arxiv.org/pdf/2511.11644",
        "title": "Slow - Motion Video Synthesis for Basketball Using Frame Interpolation",
        "authors": [
            "Jiantang Huang"
        ],
        "comments": "3 pages, 4 figures",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Basketball broadcast footage is traditionally captured at 30-60 fps, limiting viewers' ability to appreciate rapid plays such as dunks and crossovers. We present a real-time slow-motion synthesis system that produces high-quality basketball-specific interpolated frames by fine-tuning the recent Real-Time Intermediate Flow Estimation (RIFE) network on the SportsSloMo dataset. Our pipeline isolates the basketball subset of SportsSloMo, extracts training triplets, and fine-tunes RIFE with human-aware random cropping. We compare the resulting model against Super SloMo and the baseline RIFE model using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) on held-out clips. The fine-tuned RIFE attains a mean PSNR of 34.3 dB and SSIM of 0.949, outperforming Super SloMo by 2.1 dB and the baseline RIFE by 1.3 dB. A lightweight Gradio interface demonstrates end-to-end 4x slow-motion generation on a single RTX 4070 Ti Super at approximately 30 fps. These results indicate that task-specific adaptation is crucial for sports slow-motion, and that RIFE provides an attractive accuracy-speed trade-off for consumer applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11664",
        "abs_url": "https://arxiv.org/abs/2511.11664",
        "pdf_url": "https://arxiv.org/pdf/2511.11664",
        "title": "Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks",
        "authors": [
            "Mingyu Sung",
            "Suhwan Im",
            "Vikas Palakonda",
            "Jae-Mo Kang"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11676",
        "abs_url": "https://arxiv.org/abs/2511.11676",
        "pdf_url": "https://arxiv.org/pdf/2511.11676",
        "title": "Learning with Preserving for Continual Multitask Learning",
        "authors": [
            "Hanchen David Wang",
            "Siwoo Bae",
            "Zirong Chen",
            "Meiyi Ma"
        ],
        "comments": "25 pages, 16 figures, accepted at AAAI-2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11679",
        "abs_url": "https://arxiv.org/abs/2511.11679",
        "pdf_url": "https://arxiv.org/pdf/2511.11679",
        "title": "A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications",
        "authors": [
            "Zhehao Xu",
            "Lok Ming Lui"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Complex Variables (math.CV); Differential Geometry (math.DG)",
        "abstract": "Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11680",
        "abs_url": "https://arxiv.org/abs/2511.11680",
        "pdf_url": "https://arxiv.org/pdf/2511.11680",
        "title": "Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP",
        "authors": [
            "Udaya Bhasker Cheerala",
            "Varun Teja Chirukuri",
            "Venkata Akhil Kumar Gummadi",
            "Jintu Moni Bhuyan",
            "Praveen Damacharla"
        ],
        "comments": "7 pages, 2025 IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11681",
        "abs_url": "https://arxiv.org/abs/2511.11681",
        "pdf_url": "https://arxiv.org/pdf/2511.11681",
        "title": "MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation",
        "authors": [
            "Penghui Niu",
            "Jiashuai She",
            "Taotao Cai",
            "Yajuan Zhang",
            "Ping Zhang",
            "Junhua Gu",
            "Jianxin Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11683",
        "abs_url": "https://arxiv.org/abs/2511.11683",
        "pdf_url": "https://arxiv.org/pdf/2511.11683",
        "title": "Stratified Knowledge-Density Super-Network for Scalable Vision Transformers",
        "authors": [
            "Longhua Li",
            "Lei Qi",
            "Xin Geng"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \\textbf{W}eighted \\textbf{P}CA for \\textbf{A}ttention \\textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \\textbf{P}rogressive \\textbf{I}mportance-\\textbf{A}ware \\textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11688",
        "abs_url": "https://arxiv.org/abs/2511.11688",
        "pdf_url": "https://arxiv.org/pdf/2511.11688",
        "title": "Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling",
        "authors": [
            "Aihua Zhu",
            "Rui Su",
            "Qinglin Zhao",
            "Li Feng",
            "Meng Shen",
            "Shibo He"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11690",
        "abs_url": "https://arxiv.org/abs/2511.11690",
        "pdf_url": "https://arxiv.org/pdf/2511.11690",
        "title": "Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models",
        "authors": [
            "Fei Song",
            "Yi Li",
            "Rui Wang",
            "Jiahuan Zhou",
            "Changwen Zheng",
            "Jiangmeng Li"
        ],
        "comments": "Accepted by AAAI2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11692",
        "abs_url": "https://arxiv.org/abs/2511.11692",
        "pdf_url": "https://arxiv.org/pdf/2511.11692",
        "title": "AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation",
        "authors": [
            "Jiayin Zhu",
            "Linlin Yang",
            "Yicong Li",
            "Angela Yao"
        ],
        "comments": "Accepted by AAAI 2026. Project page: this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to \"semantic over-smoothing\" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11693",
        "abs_url": "https://arxiv.org/abs/2511.11693",
        "pdf_url": "https://arxiv.org/pdf/2511.11693",
        "title": "Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation",
        "authors": [
            "Xin Zhao",
            "Xiaojun Chen",
            "Bingshan Liu",
            "Zeyao Liu",
            "Zhendong Zhao",
            "Xiaoyan Gu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11696",
        "abs_url": "https://arxiv.org/abs/2511.11696",
        "pdf_url": "https://arxiv.org/pdf/2511.11696",
        "title": "Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL",
        "authors": [
            "Xun Shao",
            "Aoba Otani",
            "Yuto Hirasuka",
            "Runji Cai",
            "Seng W. Loke"
        ],
        "comments": "This is the author's preprint version of a paper accepted for presentation at EAI MONAMI 2025 (to appear in Springer LNICST). The final authenticated version will be available online at Springer Link upon publication",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)",
        "abstract": "This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11704",
        "abs_url": "https://arxiv.org/abs/2511.11704",
        "pdf_url": "https://arxiv.org/pdf/2511.11704",
        "title": "Simple Vision-Language Math Reasoning via Rendered Text",
        "authors": [
            "Matvey Skripkin",
            "Elizaveta Goncharova",
            "Andrey Kuznetsov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11705",
        "abs_url": "https://arxiv.org/abs/2511.11705",
        "pdf_url": "https://arxiv.org/pdf/2511.11705",
        "title": "Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs",
        "authors": [
            "Arya Narang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11706",
        "abs_url": "https://arxiv.org/abs/2511.11706",
        "pdf_url": "https://arxiv.org/pdf/2511.11706",
        "title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling",
        "authors": [
            "Julia Peters",
            "Karin Mora",
            "Miguel D. Mahecha",
            "Chaonan Ji",
            "David Montero",
            "Clemens Mosig",
            "Guido Kraemer"
        ],
        "comments": "10 pages (incliding 2 pages of references), 7 figures",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11713",
        "abs_url": "https://arxiv.org/abs/2511.11713",
        "pdf_url": "https://arxiv.org/pdf/2511.11713",
        "title": "Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets",
        "authors": [
            "Yunkai Yu",
            "Yingying Wang",
            "Rong Zheng"
        ],
        "comments": "8 pages,4 figures, to be published in IEEE AIOT 2025",
        "subjects": "Computers and Society (cs.CY); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The Internet of Things (IoT) sensors have been widely employed to capture human locomotions to enable applications such as activity recognition, human pose estimation, and fall detection. Motion capture (MoCap) systems are frequently used to generate ground truth annotations for human poses when training models with data from wearable or ambient sensors, and have been shown to be effective to synthesize data in these modalities. However, the representation of older adults, an increasingly important demographic in healthcare, in existing MoCap locomotion datasets has not been thoroughly examined. This work surveyed 41 publicly available datasets, identifying eight that include older adult motions and four that contain motions performed by younger actors annotated as old style. Older adults represent a small portion of participants overall, and few datasets provide full-body motion data for this group. To assess the fidelity of old-style walking motions, quantitative metrics are introduced, defining high fidelity as the ability to capture age-related differences relative to normative walking. Using gait parameters that are age-sensitive, robust to noise, and resilient to data scarcity, we found that old-style walking motions often exhibit overly controlled patterns and fail to faithfully characterize aging. These findings highlight the need for improved representation of older adults in motion datasets and establish a method to quantitatively evaluate the quality of old-style walking motions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11722",
        "abs_url": "https://arxiv.org/abs/2511.11722",
        "pdf_url": "https://arxiv.org/pdf/2511.11722",
        "title": "Fast 3D Surrogate Modeling for Data Center Thermal Management",
        "authors": [
            "Soumyendu Sarkar",
            "Antonio Guillen-Perez",
            "Zachariah J Carmichael",
            "Avisek Naug",
            "Refik Mert Cam",
            "Vineet Gundecha",
            "Ashwin Ramesh Babu",
            "Sahand Ghorbanpour",
            "Ricardo Luna Gutierrez"
        ],
        "comments": "Submitted to AAAI 2026 Conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)",
        "abstract": "Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\\%) and reduced carbon footprint.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11727",
        "abs_url": "https://arxiv.org/abs/2511.11727",
        "pdf_url": "https://arxiv.org/pdf/2511.11727",
        "title": "Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm",
        "authors": [
            "Tongda Xu"
        ],
        "comments": "NIPS 25 Workshop: Frontiers in Probabilistic Inference: Sampling Meets Learning",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11753",
        "abs_url": "https://arxiv.org/abs/2511.11753",
        "pdf_url": "https://arxiv.org/pdf/2511.11753",
        "title": "Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain",
        "authors": [
            "Mehdi Khaleghi",
            "Nastaran Khaleghi",
            "Sobhan Sheykhivand",
            "Sebelan Danishvar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11777",
        "abs_url": "https://arxiv.org/abs/2511.11777",
        "pdf_url": "https://arxiv.org/pdf/2511.11777",
        "title": "Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review",
        "authors": [
            "Vinit Mehta",
            "Charu Sharma",
            "Karthick Thiyagarajan"
        ],
        "comments": "45 pages, 15 figures, MDPI Sensors Journal",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11781",
        "abs_url": "https://arxiv.org/abs/2511.11781",
        "pdf_url": "https://arxiv.org/pdf/2511.11781",
        "title": "Coordinate Descent for Network Linearization",
        "authors": [
            "Vlad Rakhlin",
            "Amir Jevnisek",
            "Shai Avidan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)",
        "abstract": "ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11787",
        "abs_url": "https://arxiv.org/abs/2511.11787",
        "pdf_url": "https://arxiv.org/pdf/2511.11787",
        "title": "Towards Mitigating Systematics in Large-Scale Surveys via Few-Shot Optimal Transport-Based Feature Alignment",
        "authors": [
            "Sultan Hassan",
            "Sambatra Andrianomena",
            "Benjamin D. Wandelt"
        ],
        "comments": "5 pages, 3 figures, accepted to NeurIPS Workshop on Unifying Representations in Neural Models (UniReps 2025)",
        "subjects": "Instrumentation and Methods for Astrophysics (astro-ph.IM); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Systematics contaminate observables, leading to distribution shifts relative to theoretically simulated signals-posing a major challenge for using pre-trained models to label such observables. Since systematics are often poorly understood and difficult to model, removing them directly and entirely may not be feasible. To address this challenge, we propose a novel method that aligns learned features between in-distribution (ID) and out-of-distribution (OOD) samples by optimizing a feature-alignment loss on the representations extracted from a pre-trained ID model. We first experimentally validate the method on the MNIST dataset using possible alignment losses, including mean squared error and optimal transport, and subsequently apply it to large-scale maps of neutral hydrogen. Our results show that optimal transport is particularly effective at aligning OOD features when parity between ID and OOD samples is unknown, even with limited data-mimicking real-world conditions in extracting information from large-scale surveys. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11831",
        "abs_url": "https://arxiv.org/abs/2511.11831",
        "pdf_url": "https://arxiv.org/pdf/2511.11831",
        "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models",
        "authors": [
            "Wenhao Zhou",
            "Hao Zheng",
            "Rong Zhao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11880",
        "abs_url": "https://arxiv.org/abs/2511.11880",
        "pdf_url": "https://arxiv.org/pdf/2511.11880",
        "title": "Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production",
        "authors": [
            "David Montero",
            "Miguel D. Mahecha",
            "Francesco Martinuzzi",
            "César Aybar",
            "Anne Klosterhalfen",
            "Alexander Knohl",
            "Jesús Anaya",
            "Clemens Mosig",
            "Sebastian Wieneke"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11899",
        "abs_url": "https://arxiv.org/abs/2511.11899",
        "pdf_url": "https://arxiv.org/pdf/2511.11899",
        "title": "End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction",
        "authors": [
            "Xi Li",
            "Nicholas Matsumoto",
            "Ujjwal Pasupulety",
            "Atharva Deo",
            "Cherine Yang",
            "Jay Moran",
            "Miguel E. Hernandez",
            "Peter Wager",
            "Jasmine Lin",
            "Jeanine Kim",
            "Alvin C. Goh",
            "Christian Wagner",
            "Geoffrey A. Sonn",
            "Andrew J. Hung"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11930",
        "abs_url": "https://arxiv.org/abs/2511.11930",
        "pdf_url": "https://arxiv.org/pdf/2511.11930",
        "title": "Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering",
        "authors": [
            "Tianyu Xu",
            "Jihan Li",
            "Penghe Zu",
            "Pranav Sahay",
            "Maruchi Kim",
            "Jack Obeng-Marnu",
            "Farley Miller",
            "Xun Qian",
            "Katrina Passarella",
            "Mahitha Rachumalla",
            "Rajeev Nongpiur",
            "D. Shin"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA's feasibility and efficacy in enhancing XR auditory realism.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11934",
        "abs_url": "https://arxiv.org/abs/2511.11934",
        "pdf_url": "https://arxiv.org/pdf/2511.11934",
        "title": "A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts",
        "authors": [
            "C. César Claros Olivares",
            "Austin J. Brockmeier"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.11937",
        "abs_url": "https://arxiv.org/abs/2511.11937",
        "pdf_url": "https://arxiv.org/pdf/2511.11937",
        "title": "A Deep Learning Framework for Thyroid Nodule Segmentation and Malignancy Classification from Ultrasound Images",
        "authors": [
            "Omar Abdelrazik",
            "Mohamed Elsayed",
            "Noorul Wahab",
            "Nasir Rajpoot",
            "Adam Shephard"
        ],
        "comments": "5 pages, 2 figures, 2 tables",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Ultrasound-based risk stratification of thyroid nodules is a critical clinical task, but it suffers from high inter-observer variability. While many deep learning (DL) models function as \"black boxes,\" we propose a fully automated, two-stage framework for interpretable malignancy prediction. Our method achieves interpretability by forcing the model to focus only on clinically relevant regions. First, a TransUNet model automatically segments the thyroid nodule. The resulting mask is then used to create a region of interest around the nodule, and this localised image is fed directly into a ResNet-18 classifier. We evaluated our framework using 5-fold cross-validation on a clinical dataset of 349 images, where it achieved a high F1-score of 0.852 for predicting malignancy. To validate its performance, we compared it against a strong baseline using a Random Forest classifier with hand-crafted morphological features, which achieved an F1-score of 0.829. The superior performance of our DL framework suggests that the implicit visual features learned from the localised nodule are more predictive than explicit shape features alone. This is the first fully automated end-to-end pipeline for both detecting thyroid nodules on ultrasound images and predicting their malignancy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12002",
        "abs_url": "https://arxiv.org/abs/2511.12002",
        "pdf_url": "https://arxiv.org/pdf/2511.12002",
        "title": "Selecting Fine-Tuning Examples by Quizzing VLMs",
        "authors": [
            "Tenghao Ji",
            "Eytan Adar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \\textit{do} exemplify the target concept (e.g., a \\textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12008",
        "abs_url": "https://arxiv.org/abs/2511.12008",
        "pdf_url": "https://arxiv.org/pdf/2511.12008",
        "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models",
        "authors": [
            "Yunqi Hong",
            "Johnson Kao",
            "Liam Edwards",
            "Nein-Tzu Liu",
            "Chung-Yen Huang",
            "Alex Oliveira-Kowaleski",
            "Cho-Jui Hsieh",
            "Neil Y.C. Lin"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12035",
        "abs_url": "https://arxiv.org/abs/2511.12035",
        "pdf_url": "https://arxiv.org/pdf/2511.12035",
        "title": "TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space",
        "authors": [
            "Wenxuan Miao",
            "Yulin Sun",
            "Aiyue Chen",
            "Jing Lin",
            "Yiwu Yao",
            "Yiming Gan",
            "Jieru Zhao",
            "Jingwen Leng",
            "Mingyi Guo",
            "Yu Feng"
        ],
        "comments": "",
        "subjects": "Hardware Architecture (cs.AR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations. In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\\% loss on VBench).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12046",
        "abs_url": "https://arxiv.org/abs/2511.12046",
        "pdf_url": "https://arxiv.org/pdf/2511.12046",
        "title": "BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning",
        "authors": [
            "Shanmin Wang",
            "Dongdong Zhao"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained \"teacher\" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy \"weak\" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12140",
        "abs_url": "https://arxiv.org/abs/2511.12140",
        "pdf_url": "https://arxiv.org/pdf/2511.12140",
        "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding",
        "authors": [
            "Pinxue Guo",
            "Chongruo Wu",
            "Xinyu Zhou",
            "Lingyi Hong",
            "Zhaoyu Chen",
            "Jinglun Li",
            "Kaixun Jiang",
            "Sen-ching Samson Cheung",
            "Wei Zhang",
            "Wenqiang Zhang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12143",
        "abs_url": "https://arxiv.org/abs/2511.12143",
        "pdf_url": "https://arxiv.org/pdf/2511.12143",
        "title": "Variation-Bounded Loss for Noise-Tolerant Learning",
        "authors": [
            "Jialiang Wang",
            "Xiong Zhou",
            "Xianming Liu",
            "Gangfeng Hu",
            "Deming Zhai",
            "Junjun Jiang",
            "Haoliang Li"
        ],
        "comments": "Accepted by AAAI2026",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12149",
        "abs_url": "https://arxiv.org/abs/2511.12149",
        "pdf_url": "https://arxiv.org/pdf/2511.12149",
        "title": "AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models",
        "authors": [
            "Jiayu Li",
            "Yunhan Zhao",
            "Xiang Zheng",
            "Zonghuan Xu",
            "Yige Li",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12212",
        "abs_url": "https://arxiv.org/abs/2511.12212",
        "pdf_url": "https://arxiv.org/pdf/2511.12212",
        "title": "Recursive Threshold Median Filter and Autoencoder for Salt-and-Pepper Denoising: SSIM analysis of Images and Entropy Maps",
        "authors": [
            "Petr Boriskov",
            "Kirill Rudkovskii",
            "Andrei Velichko"
        ],
        "comments": "14 pages, 13 figures, 4 tables",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper studies the removal of salt-and-pepper noise from images using median filter (MF) and simple three-layer autoencoder (AE) within recursive threshold algorithm. The performance of denoising is assessed with two metrics: the standard Structural Similarity Index SSIMImg of restored and clean images and a newly applied metric SSIMMap - the SSIM of entropy maps of these images computed via 2D Sample Entropy in sliding windows. We shown that SSIMMap is more sensitive to blur and local intensity transitions and complements SSIMImg. Experiments on low- and high-resolution grayscales images demonstrate that recursive threshold MF robustly restores images even under strong noise (50-60 %), whereas simple AE is only capable of restoring images with low levels of noise (<30 %). We propose two scalable schemes: (i) 2MF, which uses two MFs with different window sizes and a final thresholding step, effective for highlighting sharp local details at low resolution; and (ii) MFs-AE, which aggregates features from multiple MFs via an AE and is beneficial for restoring the overall scene structure at higher resolution. Owing to its simplicity and computational efficiency, MF remains preferable for deployment on resource-constrained platforms (edge/IoT), whereas AE underperforms without prior denoising. The results also validate the practical value of SSIMMap for objective blur assessment and denoising parameter tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12241",
        "abs_url": "https://arxiv.org/abs/2511.12241",
        "pdf_url": "https://arxiv.org/pdf/2511.12241",
        "title": "AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos",
        "authors": [
            "Junhyuk Seo",
            "Hyeyoon Moon",
            "Kyu-Hwan Jung",
            "Namkee Oh",
            "Taerim Kim"
        ],
        "comments": "12 pages, 5 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12248",
        "abs_url": "https://arxiv.org/abs/2511.12248",
        "pdf_url": "https://arxiv.org/pdf/2511.12248",
        "title": "Deep Unfolded BM3D: Unrolling Non-local Collaborative Filtering into a Trainable Neural Network",
        "authors": [
            "Kerem Basim",
            "Mehmet Ozan Unal",
            "Metin Ertas",
            "Isa Yildirim"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Block-Matching and 3D Filtering (BM3D) exploits non-local self-similarity priors for denoising but relies on fixed parameters. Deep models such as U-Net are more flexible but often lack interpretability and fail to generalize across noise regimes. In this study, we propose Deep Unfolded BM3D (DU-BM3D), a hybrid framework that unrolls BM3D into a trainable architecture by replacing its fixed collaborative filtering with a learnable U-Net denoiser. This preserves BM3D's non-local structural prior while enabling end-to-end optimization. We evaluate DU-BM3D on low-dose CT (LDCT) denoising and show that it outperforms classic BM3D and standalone U-Net across simulated LDCT at different noise levels, yielding higher PSNR and SSIM, especially in high-noise conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12257",
        "abs_url": "https://arxiv.org/abs/2511.12257",
        "pdf_url": "https://arxiv.org/pdf/2511.12257",
        "title": "Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems",
        "authors": [
            "Elhadji Cisse Faye",
            "Mame Diarra Fall",
            "Nicolas Dobigeon",
            "Eric Barat"
        ],
        "comments": "",
        "subjects": "Computation (stat.CO); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV); Machine Learning (stat.ML)",
        "abstract": "This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12265",
        "abs_url": "https://arxiv.org/abs/2511.12265",
        "pdf_url": "https://arxiv.org/pdf/2511.12265",
        "title": "Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks",
        "authors": [
            "Rui Wang",
            "Zeming Wei",
            "Xiyue Zhang",
            "Meng Sun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Optimization and Control (math.OC)",
        "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12268",
        "abs_url": "https://arxiv.org/abs/2511.12268",
        "pdf_url": "https://arxiv.org/pdf/2511.12268",
        "title": "Multimodal RGB-HSI Feature Fusion with Patient-Aware Incremental Heuristic Meta-Learning for Oral Lesion Classification",
        "authors": [
            "Rupam Mukherjee",
            "Rajkumar Daniel",
            "Soujanya Hazra",
            "Shirin Dasgupta",
            "Subhamoy Mandal"
        ],
        "comments": "4 pages, 1 figure, 2 tables",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Early detection of oral cancer and potentially malignant disorders is challenging in low-resource settings due to limited annotated data. We present a unified four-class oral lesion classifier that integrates deep RGB embeddings, hyperspectral reconstruction, handcrafted spectral-textural descriptors, and demographic metadata. A pathologist-verified subset of oral cavity images was curated and processed using a fine-tuned ConvNeXt-v2 encoder, followed by RGB-to-HSI reconstruction into 31-band hyperspectral cubes. Haemoglobin-sensitive indices, texture features, and spectral-shape measures were extracted and fused with deep and clinical features. Multiple machine-learning models were assessed with patient-wise validation. We further introduce an incremental heuristic meta-learner (IHML) that combines calibrated base classifiers through probabilistic stacking and patient-level posterior smoothing. On an unseen patient split, the proposed framework achieved a macro F1 of 66.23% and an accuracy of 64.56%. Results demonstrate that hyperspectral reconstruction and uncertainty-aware meta-learning substantially improve robustness for real-world oral lesion screening.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12269",
        "abs_url": "https://arxiv.org/abs/2511.12269",
        "pdf_url": "https://arxiv.org/pdf/2511.12269",
        "title": "RAA-MIL: A Novel Framework for Classification of Oral Cytology",
        "authors": [
            "Rupam Mukherjee",
            "Rajkumar Daniel",
            "Soujanya Hazra",
            "Shirin Dasgupta",
            "Subhamoy Mandal"
        ],
        "comments": "Under Review at IEEE ISBI 2026",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Cytology is a valuable tool for early detection of oral squamous cell carcinoma (OSCC). However, manual examination of cytology whole slide images (WSIs) is slow, subjective, and depends heavily on expert pathologists. To address this, we introduce the first weakly supervised deep learning framework for patient-level diagnosis of oral cytology whole slide images, leveraging the newly released Oral Cytology Dataset [1], which provides annotated cytology WSIs from ten medical centres across India. Each patient case is represented as a bag of cytology patches and assigned a diagnosis label (Healthy, Benign, Oral Potentially Malignant Disorders (OPMD), OSCC) by an in-house expert pathologist. These patient-level weak labels form a new extension to the dataset. We evaluate a baseline multiple-instance learning (MIL) model and a proposed Region-Affinity Attention MIL (RAA-MIL) that models spatial relationships between regions within each slide. The RAA-MIL achieves an average accuracy of 72.7%, weighted F1-score of 0.69 on an unseen test set, outperforming the baseline. This study establishes the first patient-level weakly supervised benchmark for oral cytology and moves toward reliable AI-assisted digital pathology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12373",
        "abs_url": "https://arxiv.org/abs/2511.12373",
        "pdf_url": "https://arxiv.org/pdf/2511.12373",
        "title": "MTMed3D: A Multi-Task Transformer-Based Model for 3D Medical Imaging",
        "authors": [
            "Fan Li",
            "Arun Iyengar",
            "Lanyu Xu"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In the field of medical imaging, AI-assisted techniques such as object detection, segmentation, and classification are widely employed to alleviate the workload of physicians and doctors. However, single-task models are predominantly used, overlooking the shared information across tasks. This oversight leads to inefficiencies in real-life applications. In this work, we propose MTMed3D, a novel end-to-end Multi-task Transformer-based model to address the limitations of single-task models by jointly performing 3D detection, segmentation, and classification in medical imaging. Our model uses a Transformer as the shared encoder to generate multi-scale features, followed by CNN-based task-specific decoders. The proposed framework was evaluated on the BraTS 2018 and 2019 datasets, achieving promising results across all three tasks, especially in detection, where our method achieves better results than prior works. Additionally, we compare our multi-task model with equivalent single-task variants trained separately. Our multi-task model significantly reduces computational costs and achieves faster inference speed while maintaining comparable performance to the single-task models, highlighting its efficiency advantage. To the best of our knowledge, this is the first work to leverage Transformers for multi-task learning that simultaneously covers detection, segmentation, and classification tasks in 3D medical imaging, presenting its potential to enhance diagnostic processes. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12396",
        "abs_url": "https://arxiv.org/abs/2511.12396",
        "pdf_url": "https://arxiv.org/pdf/2511.12396",
        "title": "DEMIST: \\underline{DE}coupled \\underline{M}ulti-stream latent d\\underline{I}ffusion for Quantitative Myelin Map \\underline{S}yn\\underline{T}hesis",
        "authors": [
            "Jiacheng Wang",
            "Hao Li",
            "Xing Yao",
            "Ahmad Toubasi",
            "Taegan Vinarsky",
            "Caroline Gheen",
            "Joy Derwenskus",
            "Chaoyang Jin",
            "Richard Dortch",
            "Junzhong Xu",
            "Francesca Bagnato",
            "Ipek Oguz"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Quantitative magnetization transfer (qMT) imaging provides myelin-sensitive biomarkers, such as the pool size ratio (PSR), which is valuable for multiple sclerosis (MS) assessment. However, qMT requires specialized 20-30 minute scans. We propose DEMIST to synthesize PSR maps from standard T1w and FLAIR images using a 3D latent diffusion model with three complementary conditioning mechanisms. Our approach has two stages: first, we train separate autoencoders for PSR and anatomical images to learn aligned latent representations. Second, we train a conditional diffusion model in this latent space on top of a frozen diffusion foundation backbone. Conditioning is decoupled into: (i) \\textbf{semantic} tokens via cross-attention, (ii) \\textbf{spatial} per-scale residual hints via a 3D ControlNet branch, and (iii) \\textbf{adaptive} LoRA-modulated attention. We include edge-aware loss terms to preserve lesion boundaries and alignment losses to maintain quantitative consistency, while keeping the number of trainable parameters low and retaining the inductive bias of the pretrained model. We evaluate on 163 scans from 99 subjects using 5-fold cross-validation. Our method outperforms VAE, GAN and diffusion baselines on multiple metrics, producing sharper boundaries and better quantitative agreement with ground truth. Our code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12502",
        "abs_url": "https://arxiv.org/abs/2511.12502",
        "pdf_url": "https://arxiv.org/pdf/2511.12502",
        "title": "BSO: Binary Spiking Online Optimization Algorithm",
        "authors": [
            "Yu Liang",
            "Yu Yang",
            "Wenjie Wei",
            "Ammar Belatreche",
            "Shuai Wang",
            "Malu Zhang",
            "Yang Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-11-18?abs=True",
        "arxiv_id": "2511.12564",
        "abs_url": "https://arxiv.org/abs/2511.12564",
        "pdf_url": "https://arxiv.org/pdf/2511.12564",
        "title": "Linear time small coresets for k-mean clustering of segments with applications",
        "authors": [
            "David Denisov",
            "Shlomi Dolev",
            "Dan Felmdan",
            "Michael Segal"
        ],
        "comments": "First published in WALCOM 2026 by Springer Nature",
        "subjects": "Machine Learning (cs.LG); Computational Geometry (cs.CG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We study the $k$-means problem for a set $\\mathcal{S} \\subseteq \\mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \\subseteq \\mathbb{R}^d$ that minimize $D(\\mathcal{S},X) := \\sum_{S \\in \\mathcal{S}} \\min_{x \\in X} D(S,x)$, where $D(S,x) := \\int_{p \\in S} |p - x| dp$ measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\\varepsilon > 0$, an $\\varepsilon$-coreset is a weighted subset $C \\subseteq \\mathbb{R}^d$ that approximates $D(\\mathcal{S},X)$ within a factor of $1 \\pm \\varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\\varepsilon$, it produces a coreset of size $O(\\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]