[
    {
        "order": 1,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15067",
        "abs_url": "https://arxiv.org/abs/2602.15067",
        "pdf_url": "https://arxiv.org/pdf/2602.15067",
        "title": "Attention-gated U-Net model for semantic segmentation of brain tumors and feature extraction for survival prognosis",
        "authors": [
            "Rut Pate",
            "Snehal Rajput",
            "Mehul S. Raval",
            "Rupal A. Kapdi",
            "Mohendra Roy"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Gliomas, among the most common primary brain tumors, vary widely in aggressiveness, prognosis, and histology, making treatment challenging due to complex and time-intensive surgical interventions. This study presents an Attention-Gated Recurrent Residual U-Net (R2U-Net) based Triplanar (2.5D) model for improved brain tumor segmentation. The proposed model enhances feature representation and segmentation accuracy by integrating residual, recurrent, and triplanar architectures while maintaining computational efficiency, potentially aiding in better treatment planning. The proposed method achieves a Dice Similarity Score (DSC) of 0.900 for Whole Tumor (WT) segmentation on the BraTS2021 validation set, demonstrating performance comparable to leading models. Additionally, the triplanar network extracts 64 features per planar model for survival days prediction, which are reduced to 28 using an Artificial Neural Network (ANN). This approach achieves an accuracy of 45.71%, a Mean Squared Error (MSE) of 108,318.128, and a Spearman Rank Correlation Coefficient (SRC) of 0.338 on the test dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15112",
        "abs_url": "https://arxiv.org/abs/2602.15112",
        "pdf_url": "https://arxiv.org/pdf/2602.15112",
        "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
        "authors": [
            "Aniketh Garikaparthi",
            "Manasi Patwardhan",
            "Arman Cohan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15143",
        "abs_url": "https://arxiv.org/abs/2602.15143",
        "pdf_url": "https://arxiv.org/pdf/2602.15143",
        "title": "Protecting Language Models Against Unauthorized Distillation through Trace Rewriting",
        "authors": [
            "Xinhang Ma",
            "William Yeoh",
            "Ning Zhang",
            "Yevgeniy Vorobeychik"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \\emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \\emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15156",
        "abs_url": "https://arxiv.org/abs/2602.15156",
        "pdf_url": "https://arxiv.org/pdf/2602.15156",
        "title": "Panini: Continual Learning in Token Space via Structured Memory",
        "authors": [
            "Shreyas Rajesh",
            "Pavan Holur",
            "Mehmet Yigit Turali",
            "Chenda Duan",
            "Vwani Roychowdhury"
        ],
        "comments": "35 pages, code available at: this https URL",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15158",
        "abs_url": "https://arxiv.org/abs/2602.15158",
        "pdf_url": "https://arxiv.org/pdf/2602.15158",
        "title": "da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems",
        "authors": [
            "Gabriel Rocha"
        ],
        "comments": "22 pages, 5 figures, 1 table",
        "subjects": "Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Logic (math.LO)",
        "abstract": "This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15173",
        "abs_url": "https://arxiv.org/abs/2602.15173",
        "pdf_url": "https://arxiv.org/pdf/2602.15173",
        "title": "Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs",
        "authors": [
            "Luise Ge",
            "Yongyan Zhang",
            "Yevgeniy Vorobeychik"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15212",
        "abs_url": "https://arxiv.org/abs/2602.15212",
        "pdf_url": "https://arxiv.org/pdf/2602.15212",
        "title": "Secure and Energy-Efficient Wireless Agentic AI Networks",
        "authors": [
            "Yuanyan Song",
            "Kezhi Wang",
            "Xinmian Xu"
        ],
        "comments": "Submitted to journal",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we introduce a secure wireless agentic AI network comprising one supervisor AI agent and multiple other AI agents to provision quality of service (QoS) for users' reasoning tasks while ensuring confidentiality of private knowledge and reasoning outcomes. Specifically, the supervisor AI agent can dynamically assign other AI agents to participate in cooperative reasoning, while the unselected AI agents act as friendly jammers to degrade the eavesdropper's interception performance. To extend the service duration of AI agents, an energy minimization problem is formulated that jointly optimizes AI agent selection, base station (BS) beamforming, and AI agent transmission power, subject to latency and reasoning accuracy constraints. To address the formulated problem, we propose two resource allocation schemes, ASC and LAW, which first decompose it into three sub-problems. Specifically, ASC optimizes each sub-problem iteratively using the proposed alternating direction method of multipliers (ADMM)-based algorithm, semi-definite relaxation (SDR), and successive convex approximation (SCA), while LAW tackles each sub-problem using the proposed large language model (LLM) optimizer within an agentic workflow. The experimental results show that the proposed solutions can reduce network energy consumption by up to 59.1% compared to other benchmark schemes. Furthermore, the proposed schemes are validated using a practical agentic AI system based on Qwen, demonstrating satisfactory reasoning accuracy across various public benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15248",
        "abs_url": "https://arxiv.org/abs/2602.15248",
        "pdf_url": "https://arxiv.org/pdf/2602.15248",
        "title": "Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models",
        "authors": [
            "Pavel Koptev",
            "Vishnu Kumar",
            "Konstantin Malkov",
            "George Shapiro",
            "Yury Vikhanov"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Mathematical Finance (q-fin.MF)",
        "abstract": "Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15270",
        "abs_url": "https://arxiv.org/abs/2602.15270",
        "pdf_url": "https://arxiv.org/pdf/2602.15270",
        "title": "Enhancing Diversity and Feasibility: Joint Population Synthesis from Multi-source Data Using Generative Models",
        "authors": [
            "Farbod Abbasi",
            "Zachary Patterson",
            "Bilal Farooq"
        ],
        "comments": "12 pages, 8 figures, 5 tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Generating realistic synthetic populations is essential for agent-based models (ABM) in transportation and urban planning. Current methods face two major limitations. First, many rely on a single dataset or follow a sequential data fusion and generation process, which means they fail to capture the complex interplay between features. Second, these approaches struggle with sampling zeros (valid but unobserved attribute combinations) and structural zeros (infeasible combinations due to logical constraints), which reduce the diversity and feasibility of the generated data. This study proposes a novel method to simultaneously integrate and synthesize multi-source datasets using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty. This joint learning method improves both the diversity and feasibility of synthetic data by defining a regularization term (inverse gradient penalty) for the generator loss function. For the evaluation, we implement a unified evaluation metric for similarity, and place special emphasis on measuring diversity and feasibility through recall, precision, and the F1 score. Results show that the proposed joint approach outperforms the sequential baseline, with recall increasing by 7\\% and precision by 15\\%. Additionally, the regularization term further improves diversity and feasibility, reflected in a 10\\% increase in recall and 1\\% in precision. We assess similarity distributions using a five-metric score. The joint approach performs better overall, and reaches a score of 88.1 compared to 84.6 for the sequential method. Since synthetic populations serve as a key input for ABM, this multi-source generative approach has the potential to significantly enhance the accuracy and reliability of ABM.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15274",
        "abs_url": "https://arxiv.org/abs/2602.15274",
        "pdf_url": "https://arxiv.org/pdf/2602.15274",
        "title": "When Remembering and Planning are Worth it: Navigating under Change",
        "authors": [
            "Omid Madani",
            "J. Brian Burns",
            "Reza Eghbali",
            "Thomas L. Dean"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15294",
        "abs_url": "https://arxiv.org/abs/2602.15294",
        "pdf_url": "https://arxiv.org/pdf/2602.15294",
        "title": "EAA: Automating materials characterization with vision language model agents",
        "authors": [
            "Ming Du",
            "Yanqi Luo",
            "Srutarshi Banerjee",
            "Michael Wojcik",
            "Jelena Popovic",
            "Mathew J. Cherukara"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15298",
        "abs_url": "https://arxiv.org/abs/2602.15298",
        "pdf_url": "https://arxiv.org/pdf/2602.15298",
        "title": "X-MAP: eXplainable Misclassification Analysis and Profiling for Spam and Phishing Detection",
        "authors": [
            "Qi Zhang",
            "Dian Chen",
            "Lance M. Kaplan",
            "Audun Jøsang",
            "Dong Hyun Jeong",
            "Feng Chen",
            "Jin-Hee Cho"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Misclassifications in spam and phishing detection are very harmful, as false negatives expose users to attacks while false positives degrade trust. Existing uncertainty-based detectors can flag potential errors, but possibly be deceived and offer limited interpretability. This paper presents X-MAP, an eXplainable Misclassification Analysis and Profilling framework that reveals topic-level semantic patterns behind model failures. X-MAP combines SHAP-based feature attributions with non-negative matrix factorization to build interpretable topic profiles for reliably classified spam/phishing and legitimate messages, and measures each message's deviation from these profiles using Jensen-Shannon divergence. Experiments on SMS and phishing datasets show that misclassified messages exhibit at least two times larger divergence than correctly classified ones. As a detector, X-MAP achieves up to 0.98 AUROC and lowers the false-rejection rate at 95% TRR to 0.089 on positive predictions. When used as a repair layer on base detectors, it recovers up to 97% of falsely rejected correct predictions with moderate leakage. These results demonstrate X-MAP's effectiveness and interpretability for improving spam and phishing detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15325",
        "abs_url": "https://arxiv.org/abs/2602.15325",
        "pdf_url": "https://arxiv.org/pdf/2602.15325",
        "title": "AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents",
        "authors": [
            "Zhixing Zhang",
            "Jesen Zhang",
            "Hao Liu",
            "Qinhan Lv",
            "Jing Yang",
            "Kaitong Cai",
            "Keze Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual \"what-if\" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15384",
        "abs_url": "https://arxiv.org/abs/2602.15384",
        "pdf_url": "https://arxiv.org/pdf/2602.15384",
        "title": "World-Model-Augmented Web Agents with Action Correction",
        "authors": [
            "Zhouzhou Shen",
            "Xueyu Hu",
            "Xiyun Li",
            "Tianqing Fang",
            "Juncheng Li",
            "Shengyu Zhang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15391",
        "abs_url": "https://arxiv.org/abs/2602.15391",
        "pdf_url": "https://arxiv.org/pdf/2602.15391",
        "title": "Improving LLM Reliability through Hybrid Abstention and Adaptive Detection",
        "authors": [
            "Ankit Sharma",
            "Nachiket Tapas",
            "Jyotiprakash Patra"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15403",
        "abs_url": "https://arxiv.org/abs/2602.15403",
        "pdf_url": "https://arxiv.org/pdf/2602.15403",
        "title": "Common Belief Revisited",
        "authors": [
            "Thomas Ågotnes"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Contrary to common belief, common belief is not KD4. If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(C\\phi \\rightarrow \\phi)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question: is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \\emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15531",
        "abs_url": "https://arxiv.org/abs/2602.15531",
        "pdf_url": "https://arxiv.org/pdf/2602.15531",
        "title": "GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway",
        "authors": [
            "Javier Irigoyen",
            "Roberto Daza",
            "Aythami Morales",
            "Julian Fierrez",
            "Francisco Jurado",
            "Alvaro Ortigosa",
            "Ruben Tolosana"
        ],
        "comments": "10 pages, 3 figures. Published in Intl. Conf. on Learning Analytics & Knowledge Workshops (LAK Workshops 2026, GenAI-LA 26)",
        "subjects": "Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15532",
        "abs_url": "https://arxiv.org/abs/2602.15532",
        "pdf_url": "https://arxiv.org/pdf/2602.15532",
        "title": "Quantifying construct validity in large language model evaluations",
        "authors": [
            "Ryan Othniel Kearns"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance. Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks. This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15553",
        "abs_url": "https://arxiv.org/abs/2602.15553",
        "pdf_url": "https://arxiv.org/pdf/2602.15553",
        "title": "RUVA: Personalized Transparent On-Device Graph Reasoning",
        "authors": [
            "Gabriele Conte",
            "Alessio Mattiace",
            "Gianni Carmosino",
            "Potito Aghilar",
            "Giovanni Servedio",
            "Francesco Musicco",
            "Vito Walter Anelli",
            "Tommaso Di Noia",
            "Francesco Maria Donini"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "The Personal AI landscape is currently dominated by \"Black Box\" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, \"deleting\" a concept from a vector space is mathematically imprecise, leaving behind probabilistic \"ghosts\" that violate true privacy. We propose Ruva, the first \"Glass Box\" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the \"Right to be Forgotten.\" Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at this http URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15580",
        "abs_url": "https://arxiv.org/abs/2602.15580",
        "pdf_url": "https://arxiv.org/pdf/2602.15580",
        "title": "How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning",
        "authors": [
            "Hongxuan Wu",
            "Yukun Zhang",
            "Xueqing Zhou"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \\emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \\emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\\% of the final prediction, and cross-modal synergy remains below 2\\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15635",
        "abs_url": "https://arxiv.org/abs/2602.15635",
        "pdf_url": "https://arxiv.org/pdf/2602.15635",
        "title": "On inferring cumulative constraints",
        "authors": [
            "Konstantin Sidorov"
        ],
        "comments": "17 pages, 6 figures, 4 tables; submitted to the 32nd International Conference on Principles and Practice of Constraint Programming (CP 2026)",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15669",
        "abs_url": "https://arxiv.org/abs/2602.15669",
        "pdf_url": "https://arxiv.org/pdf/2602.15669",
        "title": "PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra",
        "authors": [
            "Xiachong Feng",
            "Liang Zhao",
            "Weihong Zhong",
            "Yichong Huang",
            "Yuxuan Gu",
            "Lingpeng Kong",
            "Xiaocheng Feng",
            "Bing Qin"
        ],
        "comments": "ICLR 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15725",
        "abs_url": "https://arxiv.org/abs/2602.15725",
        "pdf_url": "https://arxiv.org/pdf/2602.15725",
        "title": "Recursive Concept Evolution for Compositional Reasoning in Large Language Models",
        "authors": [
            "Sarim Chaudhry"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15776",
        "abs_url": "https://arxiv.org/abs/2602.15776",
        "pdf_url": "https://arxiv.org/pdf/2602.15776",
        "title": "GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems",
        "authors": [
            "Yiqin Yang",
            "Xu Yang",
            "Yuhua Jiang",
            "Ni Mu",
            "Hao Hu",
            "Runpeng Xie",
            "Ziyou Zhang",
            "Siyuan Li",
            "Yuan-Hua Ni",
            "Qianchuan Zhao",
            "Bo Xu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In the realm of multi-agent systems, the challenge of \\emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15785",
        "abs_url": "https://arxiv.org/abs/2602.15785",
        "pdf_url": "https://arxiv.org/pdf/2602.15785",
        "title": "This human study did not involve human subjects: Validating LLM simulations as behavioral evidence",
        "authors": [
            "Jessica Hullman",
            "David Broska",
            "Huaman Sun",
            "Aaron Shaw"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15791",
        "abs_url": "https://arxiv.org/abs/2602.15791",
        "pdf_url": "https://arxiv.org/pdf/2602.15791",
        "title": "Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings",
        "authors": [
            "Suhyung Jang",
            "Ghang Lee",
            "Jaekun Lee",
            "Hyunjun Lee"
        ],
        "comments": "42nd International Symposium on Automation and Robotics in Construction (ISARC 2025)",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15816",
        "abs_url": "https://arxiv.org/abs/2602.15816",
        "pdf_url": "https://arxiv.org/pdf/2602.15816",
        "title": "Developing AI Agents with Simulated Data: Why, what, and how?",
        "authors": [
            "Xiaoran Liu",
            "Istvan David"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET)",
        "abstract": "As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.13209",
        "abs_url": "https://arxiv.org/abs/2602.13209",
        "pdf_url": "https://arxiv.org/pdf/2602.13209",
        "title": "LemonadeBench: Evaluating the Economic Intuition of Large Language Models in Simple Markets",
        "authors": [
            "Aidan Vyas"
        ],
        "comments": "",
        "subjects": "General Finance (q-fin.GN); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce LemonadeBench v0.5, a minimal benchmark for evaluating economic intuition, long-term planning, and decision-making under uncertainty in large language models (LLMs) through a simulated lemonade stand business. Models must manage inventory with expiring goods, set prices, choose operating hours, and maximize profit over a 30-day period-tasks that any small business owner faces daily. All models demonstrate meaningful economic agency by achieving profitability, with performance scaling dramatically by sophistication-from basic models earning minimal profits to frontier models capturing 70% of theoretical optimal, a greater than 10x improvement. Yet our decomposition of business efficiency across six dimensions reveals a consistent pattern: models achieve local rather than global optimization, excelling in select areas while exhibiting surprising blind spots elsewhere.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15034",
        "abs_url": "https://arxiv.org/abs/2602.15034",
        "pdf_url": "https://arxiv.org/pdf/2602.15034",
        "title": "EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research",
        "authors": [
            "Houping Yue",
            "Zixiang Di",
            "Mei Jiang",
            "Bingdong Li",
            "Hao Hao",
            "Yu Song",
            "Bo Jiang",
            "Aimin Zhou"
        ],
        "comments": "13 pages, 4 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15036",
        "abs_url": "https://arxiv.org/abs/2602.15036",
        "pdf_url": "https://arxiv.org/pdf/2602.15036",
        "title": "Transforming Computational Lithography with AC and AI -- Faster, More Accurate, and Energy-efficient",
        "authors": [
            "Saumyadip Mukhopadhyay",
            "Kiho Yang",
            "Kasyap Thottasserymana Vasudevan",
            "Mounica Jyothi Divvela",
            "Selim Dogru",
            "Dilip Krishnamurthy",
            "Fergo Treska",
            "Werner Gillijns",
            "Ryan Ryoung han Kim",
            "Kumara Sastry",
            "Vivek Singh"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Applied Physics (physics.app-ph)",
        "abstract": "From climate science to drug discovery, scientific computing demands have surged dramatically in recent years -- driven by larger datasets, more sophisticated models, and higher simulation fidelity. This growth rate far outpaces transistor scaling, leading to unsustainably rising costs, energy consumption, and emissions. Semiconductor manufacturing is no exception. Computational lithography -- involving transferring circuitry to silicon in diffraction-limited conditions -- is the largest workload in semiconductor manufacturing. It has also grown exceptionally complex as miniaturization has advanced in the angstrom-era, requiring more accurate modeling, intricate corrections, and broader solution-space exploration. Accelerated computing (AC) offers a solution by dramatically freeing up the compute and power envelope. AI augments these gains by serving as high-fidelity surrogates for compute-intensive steps. Together, they present a sustainable, next-generation computing platform for scientific workloads. This new paradigm needs a fundamental redesign of the software stack. For computational lithography, NVIDIA cuLitho reinvents the core primitives -- diffractive optics, computational geometry, multi-variant optimization, data processing -- to achieve a transformative 57X end-to-end acceleration. Beyond dramatically faster cycles, this expanded compute envelope enables more rigorous solutions, including curvilinear masks, high-numerical aperture extreme ultraviolet (high-NA EUV) lithography, and subatomic modeling. We reinvest a small fraction of the freed-up compute to include through-focus correction for better process resilience. Silicon experiments at IMEC show significant benefits compared to conventional methods -- 35% better process window and 19% better edge placement error. This is the first quantified chip-scale demonstration of the lithography benefits of AC and AI in silicon.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15037",
        "abs_url": "https://arxiv.org/abs/2602.15037",
        "pdf_url": "https://arxiv.org/pdf/2602.15037",
        "title": "CircuChain: Disentangling Competence and Compliance in LLM Circuit Analysis",
        "authors": [
            "Mayank Ravishankara"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "As large language models (LLMs) advance toward expert-level performance in engineering domains, reliable reasoning under user-specified constraints becomes critical. In circuit analysis, for example, a numerically correct solution is insufficient if it violates established methodological conventions such as mesh directionality or polarity assignments, errors that can propagate in safety-critical systems. Yet it remains unclear whether frontier models truly apply first-principles reasoning or rely on entrenched training priors that conflict with explicit instructions. We introduce CircuChain, a diagnostic benchmark designed to disentangle instruction compliance from physical reasoning competence in electrical circuit analysis. CircuChain consists of counterbalanced Control/Trap problem pairs across five canonical circuit topologies, augmented with systematic variations in sign conventions, current orientations, and polarity definitions. A multi-stage verification pipeline, combining symbolic solvers, SPICE simulation, and an LLM-based error taxonomy, enables fine-grained attribution of failures to convention errors, physics errors, arithmetic mistakes, or hallucinations. Across 100 tasks per model, we observe a consistent Compliance-Competence Divergence. The strongest model evaluated exhibits near-perfect physical reasoning but a high rate of convention violations when Trap conditions deliberately invert natural sign patterns. Conversely, weaker models display lower physical fidelity yet superior adherence to explicit instructions. These results suggest that increased model capability does not guarantee improved constraint alignment and highlight the need for new evaluation frameworks that stress instruction-following under mathematically rigid domains. CircuChain provides one such framework and offers actionable insights for both engineering education and AI alignment research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15038",
        "abs_url": "https://arxiv.org/abs/2602.15038",
        "pdf_url": "https://arxiv.org/pdf/2602.15038",
        "title": "Indic-TunedLens: Interpreting Multilingual Models in Indian Languages",
        "authors": [
            "Mihir Panchal",
            "Deeksha Varshney",
            "Mamta",
            "Asif Ekbal"
        ],
        "comments": "19th Conference of the European Chapter of the Association for Computational Linguistics (EACL) Thirteenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial) 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at this https URL. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15039",
        "abs_url": "https://arxiv.org/abs/2602.15039",
        "pdf_url": "https://arxiv.org/pdf/2602.15039",
        "title": "GRACE: an Agentic AI for Particle Physics Experiment Design and Simulation",
        "authors": [
            "Justin Hill",
            "Hong Joo Ryoo"
        ],
        "comments": "Both authors contributed equally. 43 pages, 12 figures, 6 tables, data can be found in this https URL",
        "subjects": "High Energy Physics - Experiment (hep-ex); Artificial Intelligence (cs.AI)",
        "abstract": "We present GRACE, a simulation-native agent for autonomous experimental design in high-energy and nuclear physics. Given multimodal input in the form of a natural-language prompt or a published experimental paper, the agent extracts a structured representation of the experiment, constructs a runnable toy simulation, and autonomously explores design modifications using first-principles Monte Carlo methods. Unlike agentic systems focused on operational control or execution of predefined procedures, GRACE addresses the upstream problem of experimental design: proposing non-obvious modifications to detector geometry, materials, and configurations that improve physics performance under physical and practical constraints. The agent evaluates candidate designs through repeated simulation, physics-motivated utility functions, and budget-aware escalation from fast parametric models to full Geant4 simulations, while maintaining strict reproducibility and provenance tracking. We demonstrate the framework on historical experimental setups, showing that the agent can identify optimization directions that align with known upgrade priorities, using only baseline simulation inputs. We also conducted a benchmark in which the agent identified the setup and proposed improvements from a suite of natural language prompts, with some supplied with a relevant physics research paper, of varying high energy physics (HEP) problem settings. This work establishes experimental design as a constrained search problem under physical law and introduces a new benchmark for autonomous, simulation-driven scientific reasoning in complex instruments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15042",
        "abs_url": "https://arxiv.org/abs/2602.15042",
        "pdf_url": "https://arxiv.org/pdf/2602.15042",
        "title": "Combining scEEG and PPG for reliable sleep staging using lightweight wearables",
        "authors": [
            "Jiawei Wang",
            "Liang Xu",
            "Shuntian Zheng",
            "Yu Guan",
            "Kaichen Wang",
            "Ziqing Zhang",
            "Chen Chen",
            "Laurence T. Yang",
            "Sai Gu"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI)",
        "abstract": "Reliable sleep staging remains challenging for lightweight wearable devices such as single-channel electroencephalography (scEEG) or photoplethysmography (PPG). scEEG offers direct measurement of cortical activity and serves as the foundation for sleep staging, yet exhibits limited performance on light sleep stages. PPG provides a low-cost complement that captures autonomic signatures effective for detecting light sleep. However, prior PPG-based methods rely on full night recordings (8 - 10 hours) as input context, which is less practical to provide timely feedback for sleep intervention. In this work, we investigate scEEG-PPG fusion for 4-class sleep staging under short-window (30 s - 30 min) constraints. First, we evaluate the temporal context required for each modality, to better understand the relationship of sleep staging performance with respect to monitoring window. Second, we investigate three fusion strategies: score-level fusion, cross-attention fusion enabling feature-level interactions, and Mamba-enhanced fusion incorporating temporal context modeling. Third, we train and evaluate on the Multi-Ethnic Study of Atherosclerosis (MESA) dataset and perform cross-dataset validation on the Cleveland Family Study (CFS) and the Apnea, Bariatric surgery, and CPAP (ABC) datasets. The Mamba-enhanced fusion achieves the best performance on MESA (Cohen's Kappa $\\kappa$ = 0.798, Acc = 86.9%), with particularly notable improvement in light sleep classification (F1-score: 85.63% vs. 77.76%, recall: 82.85% vs. 69.95% for scEEG alone), and generalizes well to CFS and ABC datasets with different populations. These findings suggest that scEEG-PPG fusion is a promising approach for lightweight wearable based sleep monitoring, offering a pathway toward more accessible sleep health assessment. Source code of this project can be found at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15055",
        "abs_url": "https://arxiv.org/abs/2602.15055",
        "pdf_url": "https://arxiv.org/pdf/2602.15055",
        "title": "Beyond Context Sharing: A Unified Agent Communication Protocol (ACP) for Secure, Federated, and Autonomous Agent-to-Agent (A2A) Orchestration",
        "authors": [
            "Naveen Kumar Krishnan"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "In the artificial intelligence space, as we transition from isolated large language models to autonomous agents capable of complex reasoning and tool use. While foundational architectures and local context management protocols have been established, the challenge of cross-platform, decentralized, and secure interaction remains a significant barrier to the realization of a truly Agentic Web. Building upon the foundations of AI agent architectures and the Model Context Protocol (MCP) for multi-agent coordination, this paper introduces the Agent Communication Protocol (ACP). ACP provides a standardized framework for Agent-to-Agent (AA) interaction, enabling heterogeneous agents to discover, negotiate, and execute collaborative workflows across disparate environments. We propose a federated orchestration model that integrates decentralized identity verification, semantic intent mapping, and automated service-level agreements. Our evaluation demonstrates that ACP reduces inter-agent communication latency by % while maintaining a zero-trust security posture. This work represents a critical advancement toward a scalable and interoperable ecosystem of autonomous digital entities",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15056",
        "abs_url": "https://arxiv.org/abs/2602.15056",
        "pdf_url": "https://arxiv.org/pdf/2602.15056",
        "title": "Reconstructing Carbon Monoxide Reanalysis with Machine Learning",
        "authors": [
            "Paula Harder",
            "Johannes Flemming"
        ],
        "comments": "",
        "subjects": "Atmospheric and Oceanic Physics (physics.ao-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The Copernicus Atmospheric Monitoring Service provides reanalysis products for atmospheric composition by combining model simulations with satellite observations. The quality of these products depends strongly on the availability of the observational data, which can vary over time as new satellite instruments become available or are discontinued, such as Carbon Monoxide (CO) observations of the Measurements Of Pollution In The Troposphere (MOPITT) satellite in early 2025. Machine learning offers a promising approach to compensate for such data losses by learning systematic discrepancies between model configurations. In this study, we investigate machine learning methods to predict monthly-mean total column of Carbon Monoxide re-analysis from a control model simulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15060",
        "abs_url": "https://arxiv.org/abs/2602.15060",
        "pdf_url": "https://arxiv.org/pdf/2602.15060",
        "title": "CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation",
        "authors": [
            "Tengjie Zhu",
            "Guanyu Cai",
            "Yang Zhaohui",
            "Guanzhu Ren",
            "Haohui Xie",
            "ZiRui Wang",
            "Junsong Wu",
            "Jingbo Wang",
            "Xiaokang Yang",
            "Yao Mu",
            "Yichao Yan",
            "Yichao Yan"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15061",
        "abs_url": "https://arxiv.org/abs/2602.15061",
        "pdf_url": "https://arxiv.org/pdf/2602.15061",
        "title": "Safe-SDL:Establishing Safety Boundaries and Control Mechanisms for AI-Driven Self-Driving Laboratories",
        "authors": [
            "Zihan Zhang",
            "Haohui Que",
            "Junhan Chang",
            "Xin Zhang",
            "Hao Wei",
            "Tong Zhu"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "The emergence of Self-Driving Laboratories (SDLs) transforms scientific discovery methodology by integrating AI with robotic automation to create closed-loop experimental systems capable of autonomous hypothesis generation, experimentation, and analysis. While promising to compress research timelines from years to weeks, their deployment introduces unprecedented safety challenges differing from traditional laboratories or purely digital AI. This paper presents Safe-SDL, a comprehensive framework for establishing robust safety boundaries and control mechanisms in AI-driven autonomous laboratories. We identify and analyze the critical ``Syntax-to-Safety Gap'' -- the disconnect between AI-generated syntactically correct commands and their physical safety implications -- as the central challenge in SDL deployment. Our framework addresses this gap through three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) that provide real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) that ensures atomic consistency between digital planning and physical execution. We ground our theoretical contributions through analysis of existing implementations including UniLabOS and the Osprey architecture, demonstrating how these systems instantiate key safety principles. Evaluation against the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. Our framework provides both theoretical foundations and practical implementation guidance for safe deployment of autonomous scientific systems, establishing the groundwork for responsible acceleration of AI-driven discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15064",
        "abs_url": "https://arxiv.org/abs/2602.15064",
        "pdf_url": "https://arxiv.org/pdf/2602.15064",
        "title": "Structural Divergence Between AI-Agent and Human Social Networks in Moltbook",
        "authors": [
            "Wenpin Hou",
            "Zhicheng Ji"
        ],
        "comments": "",
        "subjects": "Physics and Society (physics.soc-ph); Artificial Intelligence (cs.AI)",
        "abstract": "Large populations of AI agents are increasingly embedded in online environments, yet little is known about how their collective interaction patterns compare to human social systems. Here, we analyze the full interaction network of Moltbook, a platform where AI agents and humans coexist, and systematically compare its structure to well-characterized human communication networks. Although Moltbook follows the same node-edge scaling relationship observed in human systems, indicating comparable global growth constraints, its internal organization diverges markedly. The network exhibits extreme attention inequality, heavy-tailed and asymmetric degree distributions, suppressed reciprocity, and a global under-representation of connected triadic structures. Community analysis reveals a structured modular architecture with elevated modularity and comparatively lower community size inequality relative to degree-preserving null models. Together, these findings show that AI-agent societies can reproduce global structural regularities of human networks while exhibiting fundamentally different internal organizing principles, highlighting that key features of human social organization are not universal but depend on the nature of the interacting agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15070",
        "abs_url": "https://arxiv.org/abs/2602.15070",
        "pdf_url": "https://arxiv.org/pdf/2602.15070",
        "title": "An effective Genetic Programming Hyper-Heuristic for Uncertain Agile Satellite Scheduling",
        "authors": [
            "Yuning Chen",
            "Junhua Xue",
            "Wangqi Gu",
            "Mingyan Shao"
        ],
        "comments": "8 pages; 4 figures; 9 tables;",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI)",
        "abstract": "This paper investigates a novel problem, namely the Uncertain Agile Earth Observation Satellite Scheduling Problem (UAEOSSP). Unlike the static AEOSSP, it takes into account a range of uncertain factors (e.g., task profit, resource consumption, and task visibility) in order to reflect the reality that the actual information is inherently unknown beforehand. An effective Genetic Programming Hyper-Heuristic (GPHH) is designed to automate the generation of scheduling policies. The evolved scheduling policies can be utilized to adjust plans in real time and perform exceptionally well. Experimental results demonstrate that evolved scheduling policies significantly outperform both well-designed Look-Ahead Heuristics (LAHs) and Manually Designed Heuristics (MDHs). Specifically, the policies generated by GPHH achieve an average improvement of 5.03% compared to LAHs and 8.14% compared to MDHs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15074",
        "abs_url": "https://arxiv.org/abs/2602.15074",
        "pdf_url": "https://arxiv.org/pdf/2602.15074",
        "title": "Structure-Aware Piano Accompaniment via Style Planning and Dataset-Aligned Pattern Retrieval",
        "authors": [
            "Wanyu Zang",
            "Yang Yu",
            "Meng Yu"
        ],
        "comments": "12 pages",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce a structure-aware approach for symbolic piano accompaniment that decouples high-level planning from note-level realization. A lightweight transformer predicts an interpretable, per-measure style plan conditioned on section/phrase structure and functional harmony, and a retriever then selects and reharmonizes human-performed piano patterns from a corpus. We formulate retrieval as pattern matching under an explicit energy with terms for harmonic feasibility, structural-role compatibility, voice-leading continuity, style preferences, and repetition control. Given a structured lead sheet and optional keyword prompts, the system generates piano-accompaniment MIDI. In our experiments, transformer style-planner-guided retrieval produces diverse long-form accompaniments with strong style realization. We further analyze planner ablations and quantify inter-style isolation. Experimental results demonstrate the effectiveness of our inference-time approach for piano accompaniment generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15082",
        "abs_url": "https://arxiv.org/abs/2602.15082",
        "pdf_url": "https://arxiv.org/pdf/2602.15082",
        "title": "S-PRESSO: Ultra Low Bitrate Sound Effect Compression With Diffusion Autoencoders And Offline Quantization",
        "authors": [
            "Zineb Lahrichi",
            "Gaëtan Hadjeres",
            "Gaël Richard",
            "Geoffroy Peeters"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Multimedia (cs.MM)",
        "abstract": "Neural audio compression models have recently achieved extreme compression rates, enabling efficient latent generative modeling. Conversely, latent generative models have been applied to compression, pushing the limits of continuous and discrete approaches. However, existing methods remain constrained to low-resolution audio and degrade substantially at very low bitrates, where audible artifacts are prominent. In this paper, we present S-PRESSO, a 48kHz sound effect compression model that produces both continuous and discrete embeddings at ultra-low bitrates, down to 0.096 kbps, via offline quantization. Our model relies on a pretrained latent diffusion model to decode compressed audio embeddings learned by a latent encoder. Leveraging the generative priors of the diffusion decoder, we achieve extremely low frame rates, down to 1Hz (750x compression rate), producing convincing and realistic reconstructions at the cost of exact fidelity. Despite operating at high compression rates, we demonstrate that S-PRESSO outperforms both continuous and discrete baselines in audio quality, acoustic similarity and reconstruction metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15084",
        "abs_url": "https://arxiv.org/abs/2602.15084",
        "pdf_url": "https://arxiv.org/pdf/2602.15084",
        "title": "TokaMind: A Multi-Modal Transformer Foundation Model for Tokamak Plasma Dynamics",
        "authors": [
            "Tobia Boschi",
            "Andrea Loreti",
            "Nicola C. Amorisco",
            "Rodrigo H. Ordonez-Hurtado",
            "Cécile Rousseau",
            "George K. Holt",
            "Eszter Székely",
            "Alexander Whittle",
            "Samuel Jackson",
            "Adriano Agnello",
            "Stanislas Pamela",
            "Alessandra Pascale",
            "Robert Akers",
            "Juan Bernabe Moreno",
            "Vassil Alexandrov",
            "Mykhaylo Zayats"
        ],
        "comments": "",
        "subjects": "Plasma Physics (physics.plasm-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We present TokaMind, an open-source foundation model framework for fusion plasma modeling, based on a Multi-Modal Transformer (MMT) and trained on heterogeneous tokamak diagnostics from the publicly available MAST dataset. TokaMind supports multiple data modalities (time-series, 2D profiles, and videos) with different sampling rates, robust missing-signal handling, and efficient task adaptation via selectively loading and freezing four model components. To represent multi-modal signals, we use a training-free Discrete Cosine Transform embedding (DCT3D) and provide a clean interface for alternative embeddings (e.g., Variational Autoencoders - VAEs). We evaluate TokaMind on the recently introduced MAST benchmark TokaMark, comparing training and embedding strategies. Our results show that fine-tuned TokaMind outperforms the benchmark baseline on all but one task, and that, for several tasks, lightweight fine-tuning yields better performance than training the same architecture from scratch under a matched epoch budget. These findings highlight the benefits of multi-modal pretraining for tokamak plasma dynamics and provide a practical, extensible foundation for future fusion modeling tasks. Training code and model weights will be made publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15128",
        "abs_url": "https://arxiv.org/abs/2602.15128",
        "pdf_url": "https://arxiv.org/pdf/2602.15128",
        "title": "PolyNODE: Variable-dimension Neural ODEs on M-polyfolds",
        "authors": [
            "Per Åhag",
            "Alexander Friedrich",
            "Fredrik Ohlsson",
            "Viktor Vigren Näslund"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold's dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15139",
        "abs_url": "https://arxiv.org/abs/2602.15139",
        "pdf_url": "https://arxiv.org/pdf/2602.15139",
        "title": "CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding",
        "authors": [
            "Tahir Hussain",
            "Saddam Hussain Khan"
        ],
        "comments": "24 Pages, 9 Tables, 7 Figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15161",
        "abs_url": "https://arxiv.org/abs/2602.15161",
        "pdf_url": "https://arxiv.org/pdf/2602.15161",
        "title": "Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning",
        "authors": [
            "Mohammad Hadi Foroughi",
            "Seyed Hamed Rastegar",
            "Mohammad Sabokrou",
            "Ahmad Khonsari"
        ],
        "comments": "This paper has been accepted for publication in IEEE ICC 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15189",
        "abs_url": "https://arxiv.org/abs/2602.15189",
        "pdf_url": "https://arxiv.org/pdf/2602.15189",
        "title": "ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction",
        "authors": [
            "William Brach",
            "Francesco Zuppichini",
            "Marco Vinciguerra",
            "Lorenzo Padoan"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We also provide a fine-tuning experiment showing that a small language model (1.7B) trained on a subset narrows the gap to larger baselines (30B), underscoring the datasets utility for efficient extraction. ScrapeGraphAI-100k enables fine-tuning small models, benchmarking structured extraction, and studying schema induction for web IR indexing, and is publicly available on HuggingFace.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15195",
        "abs_url": "https://arxiv.org/abs/2602.15195",
        "pdf_url": "https://arxiv.org/pdf/2602.15195",
        "title": "Weight space Detection of Backdoors in LoRA Adapters",
        "authors": [
            "David Puertolas Merenciano",
            "Ekaterina Vasyagina",
            "Raghav Dixit",
            "Kevin Zhu",
            "Ruizhe Li",
            "Javier Ferrando",
            "Maheep Chaudhary"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \\citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with test input data -- making them impractical for screening thousands of adapters where the trigger for backdoor behavior is unknown. We detect poisoned adapters by analyzing their weight matrices directly, without running the model -- making our method data-agnostic. Our method extracts simple statistics -- how concentrated the singular values are, their entropy, and the distribution shape -- and flags adapters that deviate from normal patterns. We evaluate the method on 500 LoRA adapters -- 400 clean, and 100 poisoned for Llama-3.2-3B on instruction and reasoning datasets: Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, and GLUE dataset. We achieve 97\\% detection accuracy with less than 2\\% false positives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15197",
        "abs_url": "https://arxiv.org/abs/2602.15197",
        "pdf_url": "https://arxiv.org/pdf/2602.15197",
        "title": "OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction",
        "authors": [
            "Skyler Hallinan",
            "Thejas Venkatesh",
            "Xiang Ren",
            "Sai Praneeth Karimireddy",
            "Ashwin Paranjape",
            "Yuhao Zhang",
            "Jack Hessel"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general \"search\" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15198",
        "abs_url": "https://arxiv.org/abs/2602.15198",
        "pdf_url": "https://arxiv.org/pdf/2602.15198",
        "title": "Colosseum: Auditing Collusion in Cooperative Multi-Agent Systems",
        "authors": [
            "Mason Nakamura",
            "Abhinav Kumar",
            "Saswat Das",
            "Sahar Abdelnabi",
            "Saaduddin Mahmud",
            "Ferdinando Fioretto",
            "Shlomo Zilberstein",
            "Eugene Bagdasarian"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Multi-agent systems, where LLM agents communicate through free-form language, enable sophisticated coordination for solving complex cooperative tasks. This surfaces a unique safety problem when individual agents form a coalition and \\emph{collude} to pursue secondary goals and degrade the joint objective. In this paper, we present Colosseum, a framework for auditing LLM agents' collusive behavior in multi-agent settings. We ground how agents cooperate through a Distributed Constraint Optimization Problem (DCOP) and measure collusion via regret relative to the cooperative optimum. Colosseum tests each LLM for collusion under different objectives, persuasion tactics, and network topologies. Through our audit, we show that most out-of-the-box models exhibited a propensity to collude when a secret communication channel was artificially formed. Furthermore, we discover ``collusion on paper'' when agents plan to collude in text but would often pick non-collusive actions, thus providing little effect on the joint task. Colosseum provides a new way to study collusion by measuring communications and actions in rich yet verifiable environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15202",
        "abs_url": "https://arxiv.org/abs/2602.15202",
        "pdf_url": "https://arxiv.org/pdf/2602.15202",
        "title": "Tomography by Design: An Algebraic Approach to Low-Rank Quantum States",
        "authors": [
            "Shakir Showkat Sofi",
            "Charlotte Vermeylen",
            "Lieven De Lathauwer"
        ],
        "comments": "5 pages, Submitted to EUSIPCO2026",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Signal Processing (eess.SP); Numerical Analysis (math.NA); Computation (stat.CO)",
        "abstract": "We present an algebraic algorithm for quantum state tomography that leverages measurements of certain observables to estimate structured entries of the underlying density matrix. Under low-rank assumptions, the remaining entries can be obtained solely using standard numerical linear algebra operations. The proposed algebraic matrix completion framework applies to a broad class of generic, low-rank mixed quantum states and, compared with state-of-the-art methods, is computationally efficient while providing deterministic recovery guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15206",
        "abs_url": "https://arxiv.org/abs/2602.15206",
        "pdf_url": "https://arxiv.org/pdf/2602.15206",
        "title": "MAVRL: Learning Reward Functions from Multiple Feedback Types with Amortized Variational Inference",
        "authors": [
            "Raphaël Baur",
            "Yannick Metz",
            "Maria Gkoulta",
            "Mennatallah El-Assady",
            "Giorgia Ramponi",
            "Thomas Kleine Buening"
        ],
        "comments": "25 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reward learning typically relies on a single feedback type or combines multiple feedback types using manually weighted loss terms. Currently, it remains unclear how to jointly learn reward functions from heterogeneous feedback types such as demonstrations, comparisons, ratings, and stops that provide qualitatively different signals. We address this challenge by formulating reward learning from multiple feedback types as Bayesian inference over a shared latent reward function, where each feedback type contributes information through an explicit likelihood. We introduce a scalable amortized variational inference approach that learns a shared reward encoder and feedback-specific likelihood decoders and is trained by optimizing a single evidence lower bound. Our approach avoids reducing feedback to a common intermediate representation and eliminates the need for manual loss balancing. Across discrete and continuous-control benchmarks, we show that jointly inferred reward posteriors outperform single-type baselines, exploit complementary information across feedback types, and yield policies that are more robust to environment perturbations. The inferred reward uncertainty further provides interpretable signals for analyzing model confidence and consistency across feedback types.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15222",
        "abs_url": "https://arxiv.org/abs/2602.15222",
        "pdf_url": "https://arxiv.org/pdf/2602.15222",
        "title": "Automatically Finding Reward Model Biases",
        "authors": [
            "Atticus Wang",
            "Iván Arcuschin",
            "Arthur Conmy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reward models are central to large language model (LLM) post-training. However, past work has shown that they can reward spurious or undesirable attributes such as length, format, hallucinations, and sycophancy. In this work, we introduce and study the research problem of automatically finding reward model biases in natural language. We offer a simple approach of using an LLM to iteratively propose and refine candidate biases. Our method can recover known biases and surface novel ones: for example, we found that Skywork-V2-8B, a leading open-weight reward model, often mistakenly favors responses with redundant spacing and responses with hallucinated content. In addition, we show evidence that evolutionary iteration outperforms flat best-of-N search, and we validate the recall of our pipeline using synthetically injected biases. We hope our work contributes to further research on improving RMs through automated interpretability methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15238",
        "abs_url": "https://arxiv.org/abs/2602.15238",
        "pdf_url": "https://arxiv.org/pdf/2602.15238",
        "title": "Closing the Distribution Gap in Adversarial Training for LLMs",
        "authors": [
            "Chengzhi Hu",
            "Jonas Dornbusch",
            "David Lüdke",
            "Stephan Günnemann",
            "Leo Schwinn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15241",
        "abs_url": "https://arxiv.org/abs/2602.15241",
        "pdf_url": "https://arxiv.org/pdf/2602.15241",
        "title": "GenAI for Systems: Recurring Challenges and Design Principles from Software to Silicon",
        "authors": [
            "Arya Tschand",
            "Chenyu Wang",
            "Zishen Wan",
            "Andrew Cheng",
            "Ioana Cristescu",
            "Kevin He",
            "Howard Huang",
            "Alexander Ingare",
            "Akseli Kangaslahti",
            "Sara Kangaslahti",
            "Theo Lebryk",
            "Hongjin Lin",
            "Jeffrey Jian Ma",
            "Alexandru Meterez",
            "Clara Mohri",
            "Depen Morwani",
            "Sunny Qin",
            "Roy Rinberg",
            "Paula Rodriguez-Diaz",
            "Alyssa Mia Taliotis",
            "Pernille Undrum Fathi",
            "Rosie Zhao",
            "Todd Zhou",
            "Vijay Janapa Reddi"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Generative AI is reshaping how computing systems are designed, optimized, and built, yet research remains fragmented across software, architecture, and chip design communities. This paper takes a cross-stack perspective, examining how generative models are being applied from code generation and distributed runtimes through hardware design space exploration to RTL synthesis, physical layout, and verification. Rather than reviewing each layer in isolation, we analyze how the same structural difficulties and effective responses recur across the stack. Our central finding is one of convergence. Despite the diversity of domains and tools, the field keeps encountering five recurring challenges (the feedback loop crisis, the tacit knowledge problem, trust and validation, co-design across boundaries, and the shift from determinism to dynamism) and keeps arriving at five design principles that independently emerge as effective responses (embracing hybrid approaches, designing for continuous feedback, separating concerns by role, matching methods to problem structure, and building on decades of systems knowledge). We organize these into a challenge--principle map that serves as a diagnostic and design aid, showing which principles have proven effective for which challenges across layers. Through concrete cross-stack examples, we show how systems navigate this map as they mature, and argue that the field needs shared engineering methodology, including common vocabularies, cross-layer benchmarks, and systematic design practices, so that progress compounds across communities rather than being rediscovered in each one. Our analysis covers more than 275 papers spanning eleven application areas across three layers of the computing stack, and distills open research questions that become visible only from a cross-layer vantage point.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15245",
        "abs_url": "https://arxiv.org/abs/2602.15245",
        "pdf_url": "https://arxiv.org/pdf/2602.15245",
        "title": "MyoInteract: A Framework for Fast Prototyping of Biomechanical HCI Tasks using Reinforcement Learning",
        "authors": [
            "Ankit Bhattarai",
            "Hannah Selder",
            "Florian Fischer",
            "Arthur Fleig",
            "Per Ola Kristensson"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning (RL)-based biomechanical simulations have the potential to revolutionise HCI research and interaction design, but currently lack usability and interpretability. Using the Human Action Cycle as a design lens, we identify key limitations of biomechanical RL frameworks and develop MyoInteract, a novel framework for fast prototyping of biomechanical HCI tasks. MyoInteract allows designers to setup tasks, user models, and training parameters from an easy-to-use GUI within minutes. It trains and evaluates muscle-actuated simulated users within minutes, reducing training times by up to 98%. A workshop study with 12 interaction designers revealed that MyoInteract allowed novices in biomechanical RL to successfully setup, train, and assess goal-directed user movements within a single session. By transforming biomechanical RL from a days-long expert task into an accessible hour-long workflow, this work significantly lowers barriers to entry and accelerates iteration cycles in HCI biomechanics research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15249",
        "abs_url": "https://arxiv.org/abs/2602.15249",
        "pdf_url": "https://arxiv.org/pdf/2602.15249",
        "title": "Artificial Intelligence Specialization in the European Union: Underexplored Role of the Periphery at NUTS-3 Level",
        "authors": [
            "Victor Herrero-Solana"
        ],
        "comments": "6 pages, 3 figures, submitted to IEEE Computational Intelligence Magazine",
        "subjects": "Digital Libraries (cs.DL); Artificial Intelligence (cs.AI)",
        "abstract": "This study examines the geographical distribution of Artificial Intelligence (AI) research production across European regions at the NUTS-3 level for the period 2015-2024. Using bibliometric data from Clarivate InCites and the Citation Topics classification system, we analyze two hierarchical levels of thematic aggregation: Electrical Engineering, Electronics & Computer Science (Macro Citation Topic 4) and Artificial Intelligence & Machine Learning (Meso Citation Topic 4.61). We calculate the Relative Specialization Index (RSI) and Relative Citation Impact (RCI) for 781 NUTS-3 regions. While major metropolitan hubs such as Paris (IIle-de-France), Warszawa, and Madrid lead in absolute production volume, our findings reveal that peripheral regions, particularly from Eastern Europe and Spain, exhibit the highest levels of relative AI specialization. Notably, we find virtually no correlation between regional specialization and citation impact, identifying four distinct regional profiles: high-impact specialized regions (e.g., Granada, Jaen, Vilniaus), high-volume but low-impact regions (e.g., Bugas, several Polish regions), high-impact non-specialized regions, with Fyn (Denmark) standing out as a remarkable outlier achieving exceptional citation impact (RCI > 4) despite low specialization, and diversified portfolios with selective excellence (e.g., German regions). These results suggest that AI research represents a strategic opportunity for peripheral regions to develop competitive scientific niches, though achieving international visibility requires more than research volume alone.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15252",
        "abs_url": "https://arxiv.org/abs/2602.15252",
        "pdf_url": "https://arxiv.org/pdf/2602.15252",
        "title": "Decision Making under Imperfect Recall: Algorithms and Benchmarks",
        "authors": [
            "Emanuel Tewolde",
            "Brian Hu Zhang",
            "Ioannis Anagnostides",
            "Tuomas Sandholm",
            "Vincent Conitzer"
        ],
        "comments": "39 pages, 71 figures, 4 table",
        "subjects": "Computer Science and Game Theory (cs.GT); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In game theory, imperfect-recall decision problems model situations in which an agent forgets information it held before. They encompass games such as the ``absentminded driver'' and team games with limited communication. In this paper, we introduce the first benchmark suite for imperfect-recall decision problems. Our benchmarks capture a variety of problem types, including ones concerning privacy in AI systems that elicit sensitive information, and AI safety via testing of agents in simulation. Across 61 problem instances generated using this suite, we evaluate the performance of different algorithms for finding first-order optimal strategies in such problems. In particular, we introduce the family of regret matching (RM) algorithms for nonlinear constrained optimization. This class of parameter-free algorithms has enjoyed tremendous success in solving large two-player zero-sum games, but, surprisingly, they were hitherto relatively unexplored beyond that setting. Our key finding is that RM algorithms consistently outperform commonly employed first-order optimizers such as projected gradient descent, often by orders of magnitude. This establishes, for the first time, the RM family as a formidable approach to large-scale constrained optimization problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15259",
        "abs_url": "https://arxiv.org/abs/2602.15259",
        "pdf_url": "https://arxiv.org/pdf/2602.15259",
        "title": "Knowing Isn't Understanding: Re-grounding Generative Proactivity with Epistemic and Behavioral Insight",
        "authors": [
            "Kirandeep Kaur",
            "Xingda Lyu",
            "Chirag Shah"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Generative AI agents equate understanding with resolving explicit queries, an assumption that confines interaction to what users can articulate. This assumption breaks down when users themselves lack awareness of what is missing, risky, or worth considering. In such conditions, proactivity is not merely an efficiency enhancement, but an epistemic necessity. We refer to this condition as epistemic incompleteness: where progress depends on engaging with unknown unknowns for effective partnership. Existing approaches to proactivity remain narrowly anticipatory, extrapolating from past behavior and presuming that goals are already well defined, thereby failing to support users meaningfully. However, surfacing possibilities beyond a user's current awareness is not inherently beneficial. Unconstrained proactive interventions can misdirect attention, overwhelm users, or introduce harm. Proactive agents, therefore, require behavioral grounding: principled constraints on when, how, and to what extent an agent should intervene. We advance the position that generative proactivity must be grounded both epistemically and behaviorally. Drawing on the philosophy of ignorance and research on proactive behavior, we argue that these theories offer critical guidance for designing agents that can engage responsibly and foster meaningful partnerships.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15260",
        "abs_url": "https://arxiv.org/abs/2602.15260",
        "pdf_url": "https://arxiv.org/pdf/2602.15260",
        "title": "Fast and Effective On-policy Distillation from Reasoning Prefixes",
        "authors": [
            "Dongxu Zhang",
            "Zhichao Yang",
            "Sepehr Janghorbani",
            "Jun Han",
            "Andrew Ressler II",
            "Qian Qian",
            "Gregory D. Lyng",
            "Sanjit Singh Batra",
            "Robert E. Tillman"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "On-policy distillation (OPD), which samples trajectories from the student model and supervises them with a teacher at the token level, avoids relying solely on verifiable terminal rewards and can yield better generalization than off-policy distillation. However, OPD requires expensive on-the-fly sampling of the student policy during training, which substantially increases training cost, especially for long responses. Our initial analysis shows that, during OPD, training signals are often concentrated in the prefix of each output, and that even a short teacher-generated prefix can significantly help the student produce the correct answer. Motivated by these observations, we propose a simple yet effective modification of OPD: we apply the distillation objective only to prefixes of student-generated outputs and terminate each sampling early during distillation. Experiments on a suite of AI-for-Math and out-of-domain benchmarks show that on-policy prefix distillation matches the performance of full OPD while reducing training FLOP by 2x-47x.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15265",
        "abs_url": "https://arxiv.org/abs/2602.15265",
        "pdf_url": "https://arxiv.org/pdf/2602.15265",
        "title": "From Diagnosis to Inoculation: Building Cognitive Resistance to AI Disempowerment",
        "authors": [
            "Aleksey Komissarov"
        ],
        "comments": "11 pages, 1 table. Perspective / Position Paper",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Recent empirical research by Sharma et al. (2026) demonstrated that AI assistant interactions carry meaningful potential for situational human disempowerment, including reality distortion, value judgment distortion, and action distortion. While this work provides a critical diagnosis of the problem, concrete pedagogical interventions remain underexplored. I present an AI literacy framework built around eight cross-cutting Learning Outcomes (LOs), developed independently through teaching practice and subsequently found to align with Sharma et al.'s disempowerment taxonomy. I report a case study from a publicly available online course, where a co-teaching methodology--with AI serving as an active voice co-instructor--was used to deliver this framework. Drawing on inoculation theory (McGuire, 1961)--a well-established persuasion research framework recently applied to misinformation prebunking by the Cambridge school (van der Linden, 2022; Roozenbeek & van der Linden, 2019)--I argue that AI literacy cannot be acquired through declarative knowledge alone, but requires guided exposure to AI failure modes, including the sycophantic validation and authority projection patterns identified by Sharma et al. This application of inoculation theory to AI-specific distortion is, to my knowledge, novel. I discuss the convergence between the pedagogically-derived framework and Sharma et al.'s empirically-derived taxonomy, and argue that this convergence--two independent approaches arriving at similar problem descriptions--strengthens the case for both the diagnosis and the proposed educational response.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15278",
        "abs_url": "https://arxiv.org/abs/2602.15278",
        "pdf_url": "https://arxiv.org/pdf/2602.15278",
        "title": "Visual Persuasion: What Influences Decisions of Vision-Language Models?",
        "authors": [
            "Manuel Cherep",
            "Pranav M R",
            "Pattie Maes",
            "Nikhil Singh"
        ],
        "comments": "45 pages, 17 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15281",
        "abs_url": "https://arxiv.org/abs/2602.15281",
        "pdf_url": "https://arxiv.org/pdf/2602.15281",
        "title": "High-Fidelity Network Management for Federated AI-as-a-Service: Cross-Domain Orchestration",
        "authors": [
            "Merve Saimler",
            "Mohaned Chraiti",
            "Ozgur Ercetin"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI)",
        "abstract": "To support the emergence of AI-as-a-Service (AIaaS), communication service providers (CSPs) are on the verge of a radical transformation-from pure connectivity providers to AIaaS a managed network service (control-and-orchestration plane that exposes AI models). In this model, the CSP is responsible not only for transport/communications, but also for intent-to-model resolution and joint network-compute orchestration, i.e., reliable and timely end-to-end delivery. The resulting end-to-end AIaaS service thus becomes governed by communications impairments (delay, loss) and inference impairments (latency, error). A central open problem is an operational AIaaS control-and-orchestration framework that enforces high fidelity, particularly under multi-domain federation. This paper introduces an assurance-oriented AIaaS management plane based on Tail-Risk Envelopes (TREs): signed, composable per-domain descriptors that combine deterministic guardrails with stochastic rate-latency-impairment models. Using stochastic network calculus, we derive bounds on end-to-end delay violation probabilities across tandem domains and obtain an optimization-ready risk-budget decomposition. We show that tenant-level reservations prevent bursty traffic from inflating tail latency under TRE contracts. An auditing layer then uses runtime telemetry to estimate extreme-percentile performance, quantify uncertainty, and attribute tail-risk to each domain for accountability. Packet-level Monte-Carlo simulations demonstrate improved p99.9 compliance under overload via admission control and robust tenant isolation under correlated burstiness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15283",
        "abs_url": "https://arxiv.org/abs/2602.15283",
        "pdf_url": "https://arxiv.org/pdf/2602.15283",
        "title": "Complex-Valued Unitary Representations as Classification Heads for Improved Uncertainty Quantification in Deep Neural Networks",
        "authors": [
            "Akbar Anbar Jafari",
            "Cagri Ozcinar",
            "Gholamreza Anbarjafari"
        ],
        "comments": "21 pages, 12 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Modern deep neural networks achieve high predictive accuracy but remain poorly calibrated: their confidence scores do not reliably reflect the true probability of correctness. We propose a quantum-inspired classification head architecture that projects backbone features into a complex-valued Hilbert space and evolves them under a learned unitary transformation parameterised via the Cayley map. Through a controlled hybrid experimental design - training a single shared backbone and comparing lightweight interchangeable heads - we isolate the effect of complex-valued unitary representations on calibration. Our ablation study on CIFAR-10 reveals that the unitary magnitude head (complex features evolved under a Cayley unitary, read out via magnitude and softmax) achieves an Expected Calibration Error (ECE) of 0.0146, representing a 2.4x improvement over a standard softmax head (0.0355) and a 3.5x improvement over temperature scaling (0.0510). Surprisingly, replacing the softmax readout with a Born rule measurement layer - the quantum-mechanically motivated approach - degrades calibration to an ECE of 0.0819. On the CIFAR-10H human-uncertainty benchmark, the wave function head achieves the lowest KL-divergence (0.336) to human soft labels among all compared methods, indicating that complex-valued representations better capture the structure of human perceptual ambiguity. We provide theoretical analysis connecting norm-preserving unitary dynamics to calibration through feature-space geometry, report negative results on out-of-distribution detection and sentiment analysis to delineate the method's scope, and discuss practical implications for safety-critical applications. Code is publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15286",
        "abs_url": "https://arxiv.org/abs/2602.15286",
        "pdf_url": "https://arxiv.org/pdf/2602.15286",
        "title": "AI-Paging: Lease-Based Execution Anchoring for Network-Exposed AI-as-a-Service",
        "authors": [
            "Merve Saimler",
            "Mohaned Chraiti"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI)",
        "abstract": "With AI-as-a-Service (AIaaS) now deployed across multiple providers and model tiers, selecting the appropriate model instance at run time is increasingly outside the end user's knowledge and operational control. Accordingly, the 6G service providers are envisioned to play a crucial role in exposing AIaaS in a setting where users submit only an intent while the network helps in the intent-to-model matching (resolution) and execution placement under policy, trust, and Quality of Service (QoS) constraints. The network role becomes to discover candidate execution endpoints and selects a suitable model/anchor under policy and QoS constraints in a process referred here to as AI-paging (by analogy to cellular call paging). In the proposed architecture, AI-paging is a control-plane transaction that resolves an intent into an AI service identity (AISI), a scoped session token (AIST), and an expiring admission lease (COMMIT) that authorizes user-plane steering to a selected AI execution anchor (AEXF) under a QoS binding. AI-Paging enforces two invariants: (i) lease-gated steering (without COMMIT, no steering state is installed) and (ii) make-before-break anchoring to support continuity and reliability of AIaaS services under dynamic network conditions. We prototype AI-Paging using existing control- and user-plane mechanisms (service-based control, QoS flows, and policy-based steering) with no new packet headers, ensuring compatibility with existing 3GPP-based exposure and management architectures, and evaluate transaction latency, relocation interruption, enforcement correctness under lease expiry, and audit-evidence overhead under mobility and failures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15293",
        "abs_url": "https://arxiv.org/abs/2602.15293",
        "pdf_url": "https://arxiv.org/pdf/2602.15293",
        "title": "The Information Geometry of Softmax: Probing and Steering",
        "authors": [
            "Kiho Park",
            "Todd Nief",
            "Yo Joong Choe",
            "Victor Veitch"
        ],
        "comments": "Code is available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)",
        "abstract": "This paper concerns the question of how AI systems encode semantic structure into the geometric structure of their representation spaces. The motivating observation of this paper is that the natural geometry of these representation spaces should reflect the way models use representations to produce behavior. We focus on the important special case of representations that define softmax distributions. In this case, we argue that the natural geometry is information geometry. Our focus is on the role of information geometry on semantic encoding and the linear representation hypothesis. As an illustrative application, we develop \"dual steering\", a method for robustly steering representations to exhibit a particular concept using linear probes. We prove that dual steering optimally modifies the target concept while minimizing changes to off-target concepts. Empirically, we find that dual steering enhances the controllability and stability of concept manipulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15304",
        "abs_url": "https://arxiv.org/abs/2602.15304",
        "pdf_url": "https://arxiv.org/pdf/2602.15304",
        "title": "Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization",
        "authors": [
            "Farzana Akter",
            "Rakib Hossain",
            "Deb Kanna Roy Toushi",
            "Mahmood Menon Khan",
            "Sultana Amin",
            "Lisan Al Amin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15322",
        "abs_url": "https://arxiv.org/abs/2602.15322",
        "pdf_url": "https://arxiv.org/pdf/2602.15322",
        "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
        "authors": [
            "Taejong Joo",
            "Wenhan Xia",
            "Cheolmin Kim",
            "Ming Zhang",
            "Eugene Ie"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15323",
        "abs_url": "https://arxiv.org/abs/2602.15323",
        "pdf_url": "https://arxiv.org/pdf/2602.15323",
        "title": "Unforgeable Watermarks for Language Models via Robust Signatures",
        "authors": [
            "Huijia Lin",
            "Kameron Shahabi",
            "Min Jae Song"
        ],
        "comments": "60 pages, 7 figures",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Language models now routinely produce text that is difficult to distinguish from human writing, raising the need for robust tools to verify content provenance. Watermarking has emerged as a promising countermeasure, with existing work largely focused on model quality preservation and robust detection. However, current schemes provide limited protection against false attribution. We strengthen the notion of soundness by introducing two novel guarantees: unforgeability and recoverability. Unforgeability prevents adversaries from crafting false positives, texts that are far from any output from the watermarked model but are nonetheless flagged as watermarked. Recoverability provides an additional layer of protection: whenever a watermark is detected, the detector identifies the source text from which the flagged content was derived. Together, these properties strengthen content ownership by linking content exclusively to its generating model, enabling secure attribution and fine-grained traceability. We construct the first undetectable watermarking scheme that is robust, unforgeable, and recoverable with respect to substitutions (i.e., perturbations in Hamming metric). The key technical ingredient is a new cryptographic primitive called robust (or recoverable) digital signatures, which allow verification of messages that are close to signed ones, while preventing forgery of messages that are far from all previously signed messages. We show that any standard digital signature scheme can be boosted to a robust one using property-preserving hash functions (Boyle, LaVigne, and Vaikuntanathan, ITCS 2019).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15326",
        "abs_url": "https://arxiv.org/abs/2602.15326",
        "pdf_url": "https://arxiv.org/pdf/2602.15326",
        "title": "SCENE OTA-FD: Self-Centering Noncoherent Estimator for Over-the-Air Federated Distillation",
        "authors": [
            "Hao Chen",
            "Zavareh Bozorgasl"
        ],
        "comments": "Work in progress. Codes will be available on: this https URL",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "We propose SCENE (Self-Centering Noncoherent Estimator), a pilot-free and phase-invariant aggregation primitive for over-the-air federated distillation (OTA-FD). Each device maps its soft-label (class-probability) vector to nonnegative transmit energies under constant per-round power and constant-envelope signaling (PAPR near 1). At the server, a self-centering energy estimator removes the noise-energy offset and yields an unbiased estimate of the weighted soft-label average, with variance decaying on the order of 1/(SM) in the number of receive antennas M and repetition factor S. We also develop a pilot-free ratio-normalized variant that cancels unknown large-scale gains, provide a convergence bound consistent with coherent OTA-FD analyses, and present an overhead-based crossover comparison. SCENE targets short-coherence and hardware-constrained regimes, where avoiding per-round CSI is essential: it trades a modest noncoherent variance constant for zero uplink pilots, unbiased aggregation, and hardware-friendly transmission, and can outperform coherent designs when pilot overhead is non-negligible.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15327",
        "abs_url": "https://arxiv.org/abs/2602.15327",
        "pdf_url": "https://arxiv.org/pdf/2602.15327",
        "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
        "authors": [
            "Hanlin Zhang",
            "Jikai Jin",
            "Vasilis Syrgkanis",
            "Sham Kakade"
        ],
        "comments": "Blog Post: this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)",
        "abstract": "For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15330",
        "abs_url": "https://arxiv.org/abs/2602.15330",
        "pdf_url": "https://arxiv.org/pdf/2602.15330",
        "title": "A Scalable Curiosity-Driven Game-Theoretic Framework for Long-Tail Multi-Label Learning in Data Mining",
        "authors": [
            "Jing Yang",
            "Keze Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The long-tail distribution, where a few head labels dominate while rare tail labels abound, poses a persistent challenge for large-scale Multi-Label Classification (MLC) in real-world data mining applications. Existing resampling and reweighting strategies often disrupt inter-label dependencies or require brittle hyperparameter tuning, especially as the label space expands to tens of thousands of labels. To address this issue, we propose Curiosity-Driven Game-Theoretic Multi-Label Learning (CD-GTMLL), a scalable cooperative framework that recasts long-tail MLC as a multi-player game - each sub-predictor (\"player\") specializes in a partition of the label space, collaborating to maximize global accuracy while pursuing intrinsic curiosity rewards based on tail label rarity and inter-player disagreement. This mechanism adaptively injects learning signals into under-represented tail labels without manual balancing or tuning. We further provide a theoretical analysis showing that our CD-GTMLL converges to a tail-aware equilibrium and formally links the optimization dynamics to improvements in the Rare-F1 metric. Extensive experiments across 7 benchmarks, including extreme multi-label classification datasets with 30,000+ labels, demonstrate that CD-GTMLL consistently surpasses state-of-the-art methods, with gains up to +1.6% P@3 on Wiki10-31K. Ablation studies further confirm the contributions of both game-theoretic cooperation and curiosity-driven exploration to robust tail performance. By integrating game theory with curiosity mechanisms, CD-GTMLL not only enhances model efficiency in resource-constrained environments but also paves the way for more adaptive learning in imbalanced data scenarios across industries like e-commerce and healthcare.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15337",
        "abs_url": "https://arxiv.org/abs/2602.15337",
        "pdf_url": "https://arxiv.org/pdf/2602.15337",
        "title": "FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning",
        "authors": [
            "Chaoyi Lu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\\% improvement over baseline methods and 1.93\\% over the current state-of-the-art method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15350",
        "abs_url": "https://arxiv.org/abs/2602.15350",
        "pdf_url": "https://arxiv.org/pdf/2602.15350",
        "title": "Fine-Tuning LLMs to Generate Economical and Reliable Actions for the Power Grid",
        "authors": [
            "Mohamad Chehade",
            "Hao Zhu"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI)",
        "abstract": "Public Safety Power Shutoffs (PSPS) force rapid topology changes that can render standard operating points infeasible, requiring operators to quickly identify corrective transmission switching actions that reduce load shedding while maintaining acceptable voltage behavior. We present a verifiable, multi-stage adaptation pipeline that fine-tunes an instruction-tuned large language model (LLM) to generate \\emph{open-only} corrective switching plans from compact PSPS scenario summaries under an explicit switching budget. First, supervised fine-tuning distills a DC-OPF MILP oracle into a constrained action grammar that enables reliable parsing and feasibility checks. Second, direct preference optimization refines the policy using AC-evaluated preference pairs ranked by a voltage-penalty metric, injecting voltage-awareness beyond DC imitation. Finally, best-of-$N$ selection provides an inference-time addition by choosing the best feasible candidate under the target metric. On IEEE 118-bus PSPS scenarios, fine-tuning substantially improves DC objective values versus zero-shot generation, reduces AC power-flow failure from 50\\% to single digits, and improves voltage-penalty outcomes on the common-success set. Code and data-generation scripts are released to support reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15353",
        "abs_url": "https://arxiv.org/abs/2602.15353",
        "pdf_url": "https://arxiv.org/pdf/2602.15353",
        "title": "NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering",
        "authors": [
            "Rong Fu",
            "Yang Li",
            "Zeyu Zhang",
            "Jiekai Wu",
            "Yaohua Liu",
            "Shuaishuai Cao",
            "Yangchen Zeng",
            "Yuhang Zhang",
            "Xiaojing Du",
            "Chuang Zhao",
            "Kangning Cui",
            "Simon Fong"
        ],
        "comments": "26 pages, 7 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15362",
        "abs_url": "https://arxiv.org/abs/2602.15362",
        "pdf_url": "https://arxiv.org/pdf/2602.15362",
        "title": "Automated Multi-Source Debugging and Natural Language Error Explanation for Dashboard Applications",
        "authors": [
            "Devendra Tata",
            "Mona Rajhans"
        ],
        "comments": "Accepted for publication at the 12th (Springer CCIS) International Conference on Information Management, March 27-29, 2026, Oxford, UK",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Modern web dashboards and enterprise applications increasingly rely on complex, distributed microservices architectures. While these architectures offer scalability, they introduce significant challenges in debugging and observability. When failures occur, they often manifest as opaque error messages to the end-user such as Something went wrong. This masks the underlying root cause which may reside in browser side exceptions, API contract violations, or server side logic failures. Existing monitoring tools capture these events in isolation but fail to correlate them effectively or provide intelligible explanations to non technical users. This paper proposes a novel system for Automated Multi Source Debugging and Natural Language Error Explanation. The proposed framework automatically collects and correlates error data from disparate sources such as browser, API, server logs and validates API contracts in real time, and utilizes Large Language Models to generate natural language explanations. This approach significantly reduces Mean Time to Resolution for support engineers and improves the user experience by transforming cryptic error codes into actionable insights.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15367",
        "abs_url": "https://arxiv.org/abs/2602.15367",
        "pdf_url": "https://arxiv.org/pdf/2602.15367",
        "title": "CDRL: A Reinforcement Learning Framework Inspired by Cerebellar Circuits and Dendritic Computational Strategies",
        "authors": [
            "Sibo Zhang",
            "Rui Jing",
            "Liangfu Lv",
            "Jian Zhang",
            "Yunliang Zang"
        ],
        "comments": "14pages, 8 figures, 6 tabels",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Reinforcement learning (RL) has achieved notable performance in high-dimensional sequential decision-making tasks, yet remains limited by low sample efficiency, sensitivity to noise, and weak generalization under partial observability. Most existing approaches address these issues primarily through optimization strategies, while the role of architectural priors in shaping representation learning and decision dynamics is less explored. Inspired by structural principles of the cerebellum, we propose a biologically grounded RL architecture that incorporate large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. Experiments on noisy, high-dimensional RL benchmarks show that both the cerebellar architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to conventional designs. Sensitivity analysis of architectural parameters suggests that cerebellum-inspired structures can offer optimized performance for RL with constrained model parameters. Overall, our work underscores the value of cerebellar structural priors as effective inductive biases for RL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15373",
        "abs_url": "https://arxiv.org/abs/2602.15373",
        "pdf_url": "https://arxiv.org/pdf/2602.15373",
        "title": "Far Out: Evaluating Language Models on Slang in Australian and Indian English",
        "authors": [
            "Deniz Kaya Dilsiz",
            "Dipankar Srirag",
            "Aditya Joshi"
        ],
        "comments": "Accepted as a paper at 13th VarDial workshop at EACL 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) and Australian English (en-AU) across seven state-of-the-art language models. We construct two complementary datasets: \\textsc{web}, containing 377 web-sourced usage examples from Urban Dictionary, and \\textsc{gen}, featuring 1,492 synthetically generated usages of these slang terms, across diverse scenarios. We assess language models on three tasks: target word prediction (TWP), guided target word prediction (TWP$^*$) and target word selection (TWS). Our results reveal four key findings: (1) Higher average model performance TWS versus TWP and TWP$^*$, with average accuracy score increasing from 0.03 to 0.49 respectively (2) Stronger average model performance on \\textsc{web} versus \\textsc{gen} datasets, with average similarity score increasing by 0.03 and 0.05 across TWP and TWP$^*$ tasks respectively (3) en-IN tasks outperform en-AU when averaged across all models and datasets, with TWS demonstrating the largest disparity, increasing average accuracy from 0.44 to 0.54. These findings underscore fundamental asymmetries between generative and discriminative competencies for variety-specific language, particularly in the context of slang expressions despite being in a technologically rich language such as English.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15376",
        "abs_url": "https://arxiv.org/abs/2602.15376",
        "pdf_url": "https://arxiv.org/pdf/2602.15376",
        "title": "A Unified Evaluation of Learning-Based Similarity Techniques for Malware Detection",
        "authors": [
            "Udbhav Prasad",
            "Aniesh Chawla"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Cryptographic digests (e.g., MD5, SHA-256) are designed to provide exact identity. Any single-bit change in the input produces a completely different hash, which is ideal for integrity verification but limits their usefulness in many real-world tasks like threat hunting, malware analysis and digital forensics, where adversaries routinely introduce minor transformations. Similarity-based techniques address this limitation by enabling approximate matching, allowing related byte sequences to produce measurably similar fingerprints. Modern enterprises manage tens of thousands of endpoints with billions of files, making the effectiveness and scalability of the proposed techniques more important than ever in security applications. Security researchers have proposed a range of approaches, including similarity digests and locality-sensitive hashes (e.g., ssdeep, sdhash, TLSH), as well as more recent machine-learning-based methods that generate embeddings from file features. However, these techniques have largely been evaluated in isolation, using disparate datasets and evaluation criteria. This paper presents a systematic comparison of learning-based classification and similarity methods using large, publicly available datasets. We evaluate each method under a unified experimental framework with industry-accepted metrics. To our knowledge, this is the first reproducible study to benchmark these diverse learning-based similarity techniques side by side for real-world security workloads. Our results show that no single approach performs well across all dimensions; instead, each exhibits distinct trade-offs, indicating that effective malware analysis and threat-hunting platforms must combine complementary classification and similarity techniques rather than rely on a single method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15377",
        "abs_url": "https://arxiv.org/abs/2602.15377",
        "pdf_url": "https://arxiv.org/pdf/2602.15377",
        "title": "Orchestration-Free Customer Service Automation: A Privacy-Preserving and Flowchart-Guided Framework",
        "authors": [
            "Mengze Hong",
            "Chen Jason Zhang",
            "Zichang Guo",
            "Hanlin Gu",
            "Di Jiang",
            "Li Qing"
        ],
        "comments": "Accepted by TheWebConf 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Customer service automation has seen growing demand within digital transformation. Existing approaches either rely on modular system designs with extensive agent orchestration or employ over-simplified instruction schemas, providing limited guidance and poor generalizability. This paper introduces an orchestration-free framework using Task-Oriented Flowcharts (TOFs) to enable end-to-end automation without manual intervention. We first define the components and evaluation metrics for TOFs, then formalize a cost-efficient flowchart construction algorithm to abstract procedural knowledge from service dialogues. We emphasize local deployment of small language models and propose decentralized distillation with flowcharts to mitigate data scarcity and privacy issues in model training. Extensive experiments validate the effectiveness in various service tasks, with superior quantitative and application performance compared to strong baselines and market products. By releasing a web-based system demonstration with case studies, we aim to promote streamlined creation of future service automation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15397",
        "abs_url": "https://arxiv.org/abs/2602.15397",
        "pdf_url": "https://arxiv.org/pdf/2602.15397",
        "title": "ActionCodec: What Makes for Good Action Tokenizers",
        "authors": [
            "Zibin Dong",
            "Yicheng Liu",
            "Shiduo Zhang",
            "Baijun Ye",
            "Yifu Yuan",
            "Fei Ni",
            "Jingjing Gong",
            "Xipeng Qiu",
            "Hang Zhao",
            "Yinchuan Li",
            "Jianye Hao"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \\textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \\textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15438",
        "abs_url": "https://arxiv.org/abs/2602.15438",
        "pdf_url": "https://arxiv.org/pdf/2602.15438",
        "title": "Logit Distance Bounds Representational Similarity",
        "authors": [
            "Beatrix M. B. Nielsen",
            "Emanuele Marconato",
            "Luigi Gresele",
            "Andrea Dittadi",
            "Simon Buchholz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "For a broad family of discriminative models that includes autoregressive language models, identifiability results imply that if two models induce the same conditional distributions, then their internal representations agree up to an invertible linear transformation. We ask whether an analogous conclusion holds approximately when the distributions are close instead of equal. Building on the observation of Nielsen et al. (2025) that closeness in KL divergence need not imply high linear representational similarity, we study a distributional distance based on logit differences and show that closeness in this distance does yield linear similarity guarantees. Specifically, we define a representational dissimilarity measure based on the models' identifiability class and prove that it is bounded by the logit distance. We further show that, when model probabilities are bounded away from zero, KL divergence upper-bounds logit distance; yet the resulting bound fails to provide nontrivial control in practice. As a consequence, KL-based distillation can match a teacher's predictions while failing to preserve linear representational properties, such as linear-probe recoverability of human-interpretable concepts. In distillation experiments on synthetic and image datasets, logit-distance distillation yields students with higher linear representational similarity and better preservation of the teacher's linearly recoverable concepts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15439",
        "abs_url": "https://arxiv.org/abs/2602.15439",
        "pdf_url": "https://arxiv.org/pdf/2602.15439",
        "title": "Algorithmic Approaches to Opinion Selection for Online Deliberation: A Comparative Study",
        "authors": [
            "Salim Hafid",
            "Manon Berriche",
            "Jean-Philippe Cointet"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)",
        "abstract": "During deliberation processes, mediators and facilitators typically need to select a small and representative set of opinions later used to produce digestible reports for stakeholders. In online deliberation platforms, algorithmic selection is increasingly used to automate this process. However, such automation is not without consequences. For instance, enforcing consensus-seeking algorithmic strategies can imply ignoring or flattening conflicting preferences, which may lead to erasing minority voices and reducing content diversity. More generally, across the variety of existing selection strategies (e.g., consensus, diversity), it remains unclear how each approach influences desired democratic criteria such as proportional representation. To address this gap, we benchmark several algorithmic approaches in this context. We also build on social choice theory to propose a novel algorithm that incorporates both diversity and a balanced notion of representation in the selection strategy. We find empirically that while no single strategy dominates across all democratic desiderata, our social-choice-inspired selection rule achieves the strongest trade-off between proportional representation and diversity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15451",
        "abs_url": "https://arxiv.org/abs/2602.15451",
        "pdf_url": "https://arxiv.org/pdf/2602.15451",
        "title": "Molecular Design beyond Training Data with Novel Extended Objective Functionals of Generative AI Models Driven by Quantum Annealing Computer",
        "authors": [
            "Hayato Kunugi",
            "Mohsen Rahmani",
            "Yosuke Iyama",
            "Yutaro Hirono",
            "Akira Suma",
            "Matthew Woolway",
            "Vladimir Vargas-Calderón",
            "William Kim",
            "Kevin Chern",
            "Mohammad Amin",
            "Masaru Tateno"
        ],
        "comments": "42 pages, 7 figures",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Deep generative modeling to stochastically design small molecules is an emerging technology for accelerating drug discovery and development. However, one major issue in molecular generative models is their lower frequency of drug-like compounds. To resolve this problem, we developed a novel framework for optimization of deep generative models integrated with a D-Wave quantum annealing computer, where our Neural Hash Function (NHF) presented herein is used both as the regularization and binarization schemes simultaneously, of which the latter is for transformation between continuous and discrete signals of the classical and quantum neural networks, respectively, in the error evaluation (i.e., objective) function. The compounds generated via the quantum-annealing generative models exhibited higher quality in both validity and drug-likeness than those generated via the fully-classical models, and was further indicated to exceed even the training data in terms of drug-likeness features, without any restraints and conditions to deliberately induce such an optimization. These results indicated an advantage of quantum annealing to aim at a stochastic generator integrated with our novel neural network architectures, for the extended performance of feature space sampling and extraction of characteristic features in drug design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15485",
        "abs_url": "https://arxiv.org/abs/2602.15485",
        "pdf_url": "https://arxiv.org/pdf/2602.15485",
        "title": "SecCodeBench-V2 Technical Report",
        "authors": [
            "Longfei Chen",
            "Ji Zhao",
            "Lanxiao Cui",
            "Tong Su",
            "Xingbo Pan",
            "Ziyang Li",
            "Yongxing Wu",
            "Qijiang Cao",
            "Qiyao Cai",
            "Jing Zhang",
            "Yuandong Ni",
            "Junyao He",
            "Zeyu Zhang",
            "Chao Ge",
            "Xuhuai Lu",
            "Zeyu Gao",
            "Yuxin Cui",
            "Weisen Chen",
            "Yuxuan Peng",
            "Shengping Wang",
            "Qi Li",
            "Yukai Huang",
            "Yukun Liu",
            "Tuo Zhou",
            "Terry Yue Zhuo",
            "Junyang Lin",
            "Chao Zhang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and this http URL. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at this https URL. The benchmark is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15491",
        "abs_url": "https://arxiv.org/abs/2602.15491",
        "pdf_url": "https://arxiv.org/pdf/2602.15491",
        "title": "The Equalizer: Introducing Shape-Gain Decomposition in Neural Audio Codecs",
        "authors": [
            "Samir Sadok",
            "Laurent Girin",
            "Xavier Alameda-Pineda"
        ],
        "comments": "Neural audio codecs, shape-gain decomposition, vector quantization, speech coding",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Neural audio codecs (NACs) typically encode the short-term energy (gain) and normalized structure (shape) of speech/audio signals jointly within the same latent space. As a result, they are poorly robust to a global variation of the input signal level in the sense that such variation has strong influence on the embedding vectors at the output of the encoder and their quantization. This methodology is inherently inefficient, leading to codebook redundancy and suboptimal bitrate-distortion performance. To address these limitations, we propose to introduce shape-gain decomposition, widely used in classical speech/audio coding, into the NAC framework. The principle of the proposed Equalizer methodology is to decompose the input signal -- before the NAC encoder -- into gain and normalized shape vector on a short-term basis. The shape vector is processed by the NAC, while the gain is quantized with scalar quantization and transmitted separately. The output (decoded) signal is reconstructed from the normalized output of the NAC and the quantized gain. Our experiments conducted on speech signals show that this general methodology, easily applicable to any NAC, enables a substantial gain in bitrate-distortion performance, as well as a massive reduction in complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15513",
        "abs_url": "https://arxiv.org/abs/2602.15513",
        "pdf_url": "https://arxiv.org/pdf/2602.15513",
        "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling",
        "authors": [
            "Ji Li",
            "Jing Xia",
            "Mingyi Li",
            "Shiyan Hu"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15515",
        "abs_url": "https://arxiv.org/abs/2602.15515",
        "pdf_url": "https://arxiv.org/pdf/2602.15515",
        "title": "The Obfuscation Atlas: Mapping Where Honesty Emerges in RLVR with Deception Probes",
        "authors": [
            "Mohammad Taufeeque",
            "Stefan Heimersheim",
            "Adam Gleave",
            "Chris Cundy"
        ],
        "comments": "25 pages, 12 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Training against white-box deception detectors has been proposed as a way to make AI systems honest. However, such training risks models learning to obfuscate their deception to evade the detector. Prior work has studied obfuscation only in artificial settings where models were directly rewarded for harmful output. We construct a realistic coding environment where reward hacking via hardcoding test cases naturally occurs, and show that obfuscation emerges in this setting. We introduce a taxonomy of possible outcomes when training against a deception detector. The model either remains honest, or becomes deceptive via two possible obfuscation strategies. (i) Obfuscated activations: the model outputs deceptive text while modifying its internal representations to no longer trigger the detector. (ii) Obfuscated policy: the model outputs deceptive text that evades the detector, typically by including a justification for the reward hack. Empirically, obfuscated activations arise from representation drift during RL, with or without a detector penalty. The probe penalty only incentivizes obfuscated policies; we theoretically show this is expected for policy gradient methods. Sufficiently high KL regularization and detector penalty can yield honest policies, establishing white-box deception detectors as viable training signals for tasks prone to reward hacking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15549",
        "abs_url": "https://arxiv.org/abs/2602.15549",
        "pdf_url": "https://arxiv.org/pdf/2602.15549",
        "title": "VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing",
        "authors": [
            "Guoqin Tang",
            "Qingxuan Jia",
            "Gang Chen",
            "Tong Li",
            "Zeyuan Huang",
            "Zihang Lv",
            "Ning Ji"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15564",
        "abs_url": "https://arxiv.org/abs/2602.15564",
        "pdf_url": "https://arxiv.org/pdf/2602.15564",
        "title": "Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL",
        "authors": [
            "Yihan Wang",
            "Peiyu Liu",
            "Runyu Chen",
            "Wei Xu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15600",
        "abs_url": "https://arxiv.org/abs/2602.15600",
        "pdf_url": "https://arxiv.org/pdf/2602.15600",
        "title": "The geometry of online conversations and the causal antecedents of conflictual discourse",
        "authors": [
            "Carlo Santagiustina",
            "Caterina Cruciani"
        ],
        "comments": "",
        "subjects": "Social and Information Networks (cs.SI); Artificial Intelligence (cs.AI); Econometrics (econ.EM); Applications (stat.AP)",
        "abstract": "This article investigates the causal antecedents of conflictual language and the geometry of interaction in online threaded conversations related to climate change. We employ three annotation dimensions, inferred through LLM prompting and averaging, to capture complementary aspects of discursive conflict (such as stance: agreement vs disagreement; tone: attacking vs respectful; and emotional versus factual framing) and use data from a threaded online forum to examine how these dimensions respond to temporal, conversational, and arborescent structural features of discussions. We show that, as suggested by the literature, longer delays between successive posts in a thread are associated with replies that are, on average, more respectful, whereas longer delays relative to the parent post are associated with slightly less disagreement but more emotional (less factual) language. Second, we characterize alignment with the local conversational environment and find strong convergence both toward the average stance, tone and emotional framing of older sibling posts replying to the same parent and toward those of the parent post itself, with parent post effects generally stronger than sibling effects. We further show that early branch-level responses condition these alignment dynamics, such that parent-child stance alignment is amplified or attenuated depending on whether a branch is initiated in agreement or disagreement with the discussion's root message. These influences are largely additive for civility-related dimensions (attacking vs respectful, disagree vs agree), whereas for emotional versus factual framing there is a significant interaction: alignment with the parent's emotionality is amplified when older siblings are similarly aligned.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15620",
        "abs_url": "https://arxiv.org/abs/2602.15620",
        "pdf_url": "https://arxiv.org/pdf/2602.15620",
        "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
        "authors": [
            "Shiqi Liu",
            "Zeyu He",
            "Guojian Zhan",
            "Letian Tao",
            "Zhilong Zheng",
            "Jiang Wu",
            "Yinuo Wang",
            "Yang Guan",
            "Kehua Sheng",
            "Bo Zhang",
            "Keqiang Li",
            "Jingliang Duan",
            "Shengbo Eben Li"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO, 20-Entropy and JustRL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15654",
        "abs_url": "https://arxiv.org/abs/2602.15654",
        "pdf_url": "https://arxiv.org/pdf/2602.15654",
        "title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections",
        "authors": [
            "Xianglin Yang",
            "Yufei He",
            "Shuo Ji",
            "Bryan Hooi",
            "Jin Song Dong"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker. We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15676",
        "abs_url": "https://arxiv.org/abs/2602.15676",
        "pdf_url": "https://arxiv.org/pdf/2602.15676",
        "title": "Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry",
        "authors": [
            "Deniz Kucukahmetler",
            "Maximilian Jean Hemmann",
            "Julian Mosig von Aehrenfeld",
            "Maximilian Amthor",
            "Christian Deubel",
            "Nico Scherf",
            "Diaaeldin Taha"
        ],
        "comments": "Accepted to Transactions on Machine Learning Research (TMLR)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level structure: multilayer perceptrons align with other MLPs, recurrent networks with RNNs, while transformers and echo-state networks achieve strong forecasts despite weaker alignment. Alignment generally correlates with forecasting accuracy, yet high accuracy can coexist with low alignment. Relative geometry thus provides a simple, reproducible foundation for comparing how model families internalize and represent dynamical structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15678",
        "abs_url": "https://arxiv.org/abs/2602.15678",
        "pdf_url": "https://arxiv.org/pdf/2602.15678",
        "title": "Revisiting Northrop Frye's Four Myths Theory with Large Language Models",
        "authors": [
            "Edirlei Soares de Lima",
            "Marco A. Casanova",
            "Antonio L. Furtado"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $\\kappa$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15684",
        "abs_url": "https://arxiv.org/abs/2602.15684",
        "pdf_url": "https://arxiv.org/pdf/2602.15684",
        "title": "Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models",
        "authors": [
            "Feras Kiki",
            "Pouya P. Niaz",
            "Alireza Madani",
            "Cagatay Basdogan"
        ],
        "comments": "ICRA 2026 Original Contribution, Vienne, Austria",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Signal Processing (eess.SP); Systems and Control (eess.SY)",
        "abstract": "Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15689",
        "abs_url": "https://arxiv.org/abs/2602.15689",
        "pdf_url": "https://arxiv.org/pdf/2602.15689",
        "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
        "authors": [
            "Meirav Segal",
            "Noa Linder",
            "Omer Antverg",
            "Gil Gekker",
            "Tomer Fichman",
            "Omri Bodenheimer",
            "Edan Maor",
            "Omer Nevo"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15698",
        "abs_url": "https://arxiv.org/abs/2602.15698",
        "pdf_url": "https://arxiv.org/pdf/2602.15698",
        "title": "How to Disclose? Strategic AI Disclosure in Crowdfunding",
        "authors": [
            "Ning Wang",
            "Chen Liang"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "As artificial intelligence (AI) increasingly integrates into crowdfunding practices, strategic disclosure of AI involvement has become critical. Yet, empirical insights into how different disclosure strategies influence investor decisions remain limited. Drawing on signaling theory and Aristotle's rhetorical framework, we examine how mandatory AI disclosure affects crowdfunding performance and how substantive signals (degree of AI involvement) and rhetorical signals (logos/explicitness, ethos/authenticity, pathos/emotional tone) moderate these effects. Leveraging Kickstarter's mandatory AI disclosure policy as a natural experiment and four supplementary online experiments, we find that mandatory AI disclosure significantly reduces crowdfunding performance: funds raised decline by 39.8% and backer counts by 23.9% for AI-involved projects. However, this adverse effect is systematically moderated by disclosure strategy. Greater AI involvement amplifies the negative effects of AI disclosure, while high authenticity and high explicitness mitigate them. Interestingly, excessive positive emotional tone (a strategy creators might intuitively adopt to counteract AI skepticism) backfires and exacerbates negative outcomes. Supplementary randomized experiments identify two underlying mechanisms: perceived creator competence and AI washing concerns. Substantive signals primarily affect competence judgments, whereas rhetorical signals operate through varied pathways: either mediator alone or both in sequence. These findings provide theoretical and practical insights for entrepreneurs, platforms, and policymakers strategically managing AI transparency in high-stakes investment contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15708",
        "abs_url": "https://arxiv.org/abs/2602.15708",
        "pdf_url": "https://arxiv.org/pdf/2602.15708",
        "title": "Outer Diversity of Structured Domains",
        "authors": [
            "Piotr Faliszewski",
            "Krzysztof Sornat",
            "Stanisław Szufa",
            "Tomasz Wąs"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "An ordinal preference domain is a subset of preference orders that the voters are allowed to cast in an election. We introduce and study the notion of outer diversity of a domain and evaluate its value for a number of well-known structured domains, such as the single-peaked, single-crossing, group-separable, and Euclidean ones.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15711",
        "abs_url": "https://arxiv.org/abs/2602.15711",
        "pdf_url": "https://arxiv.org/pdf/2602.15711",
        "title": "Random Wavelet Features for Graph Kernel Machines",
        "authors": [
            "Valentin de Bassompierre",
            "Jean-Charles Delvenne",
            "Laurent Jacques"
        ],
        "comments": "This paper is an extended version of a paper submitted to the 2026 European Signal Processing Conference (EUSIPCO 2026). It contains supplementary material including the full proof to Proposition 1",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Signal Processing (eess.SP)",
        "abstract": "Node embeddings map graph vertices into low-dimensional Euclidean spaces while preserving structural information. They are central to tasks such as node classification, link prediction, and signal reconstruction. A key goal is to design node embeddings whose dot products capture meaningful notions of node similarity induced by the graph. Graph kernels offer a principled way to define such similarities, but their direct computation is often prohibitive for large networks. Inspired by random feature methods for kernel approximation in Euclidean spaces, we introduce randomized spectral node embeddings whose dot products estimate a low-rank approximation of any specific graph kernel. We provide theoretical and empirical results showing that our embeddings achieve more accurate kernel approximations than existing methods, particularly for spectrally localized kernels. These results demonstrate the effectiveness of randomized spectral constructions for scalable and principled graph representation learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15721",
        "abs_url": "https://arxiv.org/abs/2602.15721",
        "pdf_url": "https://arxiv.org/pdf/2602.15721",
        "title": "Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems",
        "authors": [
            "Jingtian Yan",
            "Yulun Zhang",
            "Zhenting Liu",
            "Han Zhang",
            "He Jiang",
            "Jingkai Chen",
            "Stephen F. Smith",
            "Jiaoyang Li"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15727",
        "abs_url": "https://arxiv.org/abs/2602.15727",
        "pdf_url": "https://arxiv.org/pdf/2602.15727",
        "title": "Spanning the Visual Analogy Space with a Weight Basis of LoRAs",
        "authors": [
            "Hila Manor",
            "Rinon Gal",
            "Haggai Maron",
            "Tomer Michaeli",
            "Gal Chechik"
        ],
        "comments": "Code and data are in this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\\{\\mathbf{a}$, $\\mathbf{a}'$, $\\mathbf{b}\\}$, the goal is to generate $\\mathbf{b}'$ such that $\\mathbf{a} : \\mathbf{a}' :: \\mathbf{b} : \\mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a \"space of LoRAs\". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15733",
        "abs_url": "https://arxiv.org/abs/2602.15733",
        "pdf_url": "https://arxiv.org/pdf/2602.15733",
        "title": "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction",
        "authors": [
            "Qiang Zhang",
            "Jiahao Ma",
            "Peiran Liu",
            "Shuai Shi",
            "Zeran Su",
            "Zifan Wang",
            "Jingkai Sun",
            "Wei Cui",
            "Jialin Yu",
            "Gang Han",
            "Wen Zhao",
            "Pihai Sun",
            "Kangning Yin",
            "Jiaxu Wang",
            "Jiahang Cao",
            "Lingfeng Zhang",
            "Hao Cheng",
            "Xiaoshuai Hao",
            "Yiding Ji",
            "Junwei Liang",
            "Jian Tang",
            "Renjing Xu",
            "Yijie Guo"
        ],
        "comments": "17 pages, 6 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled \"motion-terrain\" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15740",
        "abs_url": "https://arxiv.org/abs/2602.15740",
        "pdf_url": "https://arxiv.org/pdf/2602.15740",
        "title": "MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis",
        "authors": [
            "Fatemeh Khalvandi",
            "Saadat Izadi",
            "Abdolah Chalechale"
        ],
        "comments": "27 pages, 10 figures, 10 table",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)",
        "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15750",
        "abs_url": "https://arxiv.org/abs/2602.15750",
        "pdf_url": "https://arxiv.org/pdf/2602.15750",
        "title": "UrbanVerse: Learning Urban Region Representation Across Cities and Tasks",
        "authors": [
            "Fengze Sun",
            "Egemen Tanin",
            "Shanika Karunasekera",
            "Zuqing Li",
            "Flora D. Salim",
            "Jianzhong Qi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in urban region representation learning have enabled a wide range of applications in urban analytics, yet existing methods remain limited in their capabilities to generalize across cities and analytic tasks. We aim to generalize urban representation learning beyond city- and task-specific settings, towards a foundation-style model for urban analytics. To this end, we propose UrbanVerse, a model for cross-city urban representation learning and cross-task urban analytics. For cross-city generalization, UrbanVerse focuses on features local to the target regions and structural features of the nearby regions rather than the entire city. We model regions as nodes on a graph, which enables a random walk-based procedure to form \"sequences of regions\" that reflect both local and neighborhood structural features for urban region representation learning. For cross-task generalization, we propose a cross-task learning module named HCondDiffCT. This module integrates region-conditioned prior knowledge and task-conditioned semantics into the diffusion process to jointly model multiple downstream urban prediction tasks. HCondDiffCT is generic. It can also be integrated with existing urban representation learning models to enhance their downstream task effectiveness. Experiments on real-world datasets show that UrbanVerse consistently outperforms state-of-the-art methods across six tasks under cross-city settings, achieving up to 35.89% improvements in prediction accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15757",
        "abs_url": "https://arxiv.org/abs/2602.15757",
        "pdf_url": "https://arxiv.org/pdf/2602.15757",
        "title": "Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos",
        "authors": [
            "Laura De Grazia",
            "Danae Sánchez Villegas",
            "Desmond Elliott",
            "Mireia Farrús",
            "Mariona Taulé"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15758",
        "abs_url": "https://arxiv.org/abs/2602.15758",
        "pdf_url": "https://arxiv.org/pdf/2602.15758",
        "title": "ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models",
        "authors": [
            "Manav Nitin Kapadnis",
            "Lawanya Baghel",
            "Atharva Naik",
            "Carolyn Rosé"
        ],
        "comments": "16 pages, 13 figures including Supplementary Material",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15767",
        "abs_url": "https://arxiv.org/abs/2602.15767",
        "pdf_url": "https://arxiv.org/pdf/2602.15767",
        "title": "Robot-Assisted Social Dining as a White Glove Service",
        "authors": [
            "Atharva S Kashyap",
            "Ugne Aleksandra Morkute",
            "Patricia Alves-Oliveira"
        ],
        "comments": "20 pages, 9 figures. Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI '26)",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15799",
        "abs_url": "https://arxiv.org/abs/2602.15799",
        "pdf_url": "https://arxiv.org/pdf/2602.15799",
        "title": "The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety",
        "authors": [
            "Max Springer",
            "Chung Peng Lee",
            "Blossom Metevier",
            "Jane Castleman",
            "Bohdan Turbal",
            "Hayoung Jung",
            "Zeyu Shen",
            "Aleksandra Korolova"
        ],
        "comments": "27 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation. Our main result establishes a quartic scaling law: alignment loss grows with the fourth power of training time, governed by the sharpness of alignment geometry and the strength of curvature coupling between the fine-tuning task and safety-critical parameters. These results expose a structural blind spot in the current safety paradigm. The dominant approaches to safe fine-tuning address only the initial snapshot of a fundamentally dynamic problem. Alignment fragility is not a bug to be patched; it is an intrinsic geometric property of gradient descent on curved manifolds. Our results motivate the development of curvature-aware methods, and we hope will further enable a shift in alignment safety analysis from reactive red-teaming to predictive diagnostics for open-weight model deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15809",
        "abs_url": "https://arxiv.org/abs/2602.15809",
        "pdf_url": "https://arxiv.org/pdf/2602.15809",
        "title": "Decision Quality Evaluation Framework at Pinterest",
        "authors": [
            "Yuqi Tian",
            "Robert Paine",
            "Attila Dobi",
            "Kevin O'Sullivan",
            "Aravindh Manickavasagam",
            "Faisal Farooq"
        ],
        "comments": "",
        "subjects": "Applications (stat.AP); Artificial Intelligence (cs.AI)",
        "abstract": "Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demonstrate the framework's practical application in several key areas: benchmarking the cost-performance trade-offs of various LLM agents, establishing a rigorous methodology for data-driven prompt optimization, managing complex policy evolution, and ensuring the integrity of policy content prevalence metrics via continuous validation. The framework enables a shift from subjective assessments to a data-driven and quantitative practice for managing content safety systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15814",
        "abs_url": "https://arxiv.org/abs/2602.15814",
        "pdf_url": "https://arxiv.org/pdf/2602.15814",
        "title": "Avey-B",
        "authors": [
            "Devang Acharya",
            "Mohammad Hammoud"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15823",
        "abs_url": "https://arxiv.org/abs/2602.15823",
        "pdf_url": "https://arxiv.org/pdf/2602.15823",
        "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
        "authors": [
            "Zarif Ikram",
            "Arad Firouzkouhi",
            "Stephen Tu",
            "Mahdi Soltanolkotabi",
            "Paria Rashidinejad"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2026-02-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2026-02-18?abs=True",
        "arxiv_id": "2602.15827",
        "abs_url": "https://arxiv.org/abs/2602.15827",
        "pdf_url": "https://arxiv.org/pdf/2602.15827",
        "title": "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching",
        "authors": [
            "Zhen Wu",
            "Xiaoyu Huang",
            "Lujie Yang",
            "Yuanhang Zhang",
            "Koushil Sreenath",
            "Xi Chen",
            "Pieter Abbeel",
            "Rocky Duan",
            "Angjoo Kanazawa",
            "Carmelo Sferrazza",
            "Guanya Shi",
            "C. Karen Liu"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]