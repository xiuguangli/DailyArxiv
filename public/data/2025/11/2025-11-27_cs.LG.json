[
    {
        "order": 1,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20698",
        "abs_url": "https://arxiv.org/abs/2511.20698",
        "pdf_url": "https://arxiv.org/pdf/2511.20698",
        "title": "On the Role of Hidden States of Modern Hopfield Network in Transformer",
        "authors": [
            "Tsubasa Masumura",
            "Masato Taki"
        ],
        "comments": "NeurIPS 2025 accepted",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Associative memory models based on Hopfield networks and self-attention based on key-value mechanisms have been popular approaches in the study of memory mechanisms in deep learning. It has been pointed out that the state update rule of the modern Hopfield network (MHN) in the adiabatic approximation is in agreement with the self-attention layer of Transformer. In this paper, we go beyond this approximation and investigate the relationship between MHN and self-attention. Our results show that the correspondence between Hopfield networks and Transformers can be established in a more generalized form by adding a new variable, the hidden state derived from the MHN, to self-attention. This new attention mechanism, modern Hopfield attention (MHA), allows the inheritance of attention scores from the input layer of the Transformer to the output layer, which greatly improves the nature of attention weights. In particular, we show both theoretically and empirically that MHA hidden states significantly improve serious problem of deep Transformers known as rank collapse and token uniformity. We also confirm that MHA can systematically improve accuracy without adding training parameters to the Vision Transformer or GPT. Our results provide a new case in which Hopfield networks can be a useful perspective for improving the Transformer architecture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20704",
        "abs_url": "https://arxiv.org/abs/2511.20704",
        "pdf_url": "https://arxiv.org/pdf/2511.20704",
        "title": "Pretraining Transformer-Based Models on Diffusion-Generated Synthetic Graphs for Alzheimer's Disease Prediction",
        "authors": [
            "Abolfazl Moslemi",
            "Hossein Peyvandi"
        ],
        "comments": "14 pages. Preprint",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Early and accurate detection of Alzheimer's disease (AD) is crucial for enabling timely intervention and improving outcomes. However, developing reliable machine learning (ML) models for AD diagnosis is challenging due to limited labeled data, multi-site heterogeneity, and class imbalance. We propose a Transformer-based diagnostic framework that combines diffusion-based synthetic data generation with graph representation learning and transfer learning. A class-conditional denoising diffusion probabilistic model (DDPM) is trained on the real-world NACC dataset to generate a large synthetic cohort that mirrors multimodal clinical and neuroimaging feature distributions while balancing diagnostic classes. Modality-specific Graph Transformer encoders are first pretrained on this synthetic data to learn robust, class-discriminative representations and are then frozen while a neural classifier is trained on embeddings from the original NACC data. We quantify distributional alignment between real and synthetic cohorts using metrics such as Maximum Mean Discrepancy (MMD), Frechet distance, and energy distance, and complement discrimination metrics with calibration and fixed-specificity sensitivity analyses. Empirically, our framework outperforms standard baselines, including early and late fusion deep neural networks and the multimodal graph-based model MaGNet, yielding higher AUC, accuracy, sensitivity, and specificity under subject-wise cross-validation on NACC. These results show that diffusion-based synthetic pretraining with Graph Transformers can improve generalization in low-sample, imbalanced clinical prediction settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20713",
        "abs_url": "https://arxiv.org/abs/2511.20713",
        "pdf_url": "https://arxiv.org/pdf/2511.20713",
        "title": "Active Slice Discovery in Large Language Models",
        "authors": [
            "Minhui Zhang",
            "Prahar Ijner",
            "Yoav Wald",
            "Elliot Creager"
        ],
        "comments": "Accepted for presentation at NeurIPS 2025 - Reliable ML Workshop",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) often exhibit systematic errors on specific subsets of data, known as error slices. For instance, a slice can correspond to a certain demographic, where a model does poorly in identifying toxic comments regarding that demographic. Identifying error slices is crucial to understanding and improving models, but it is also challenging. An appealing approach to reduce the amount of manual annotation required is to actively group errors that are likely to belong to the same slice, while using limited access to an annotator to verify whether the chosen samples share the same pattern of model mistake. In this paper, we formalize this approach as Active Slice Discovery and explore it empirically on a problem of discovering human-defined slices in toxicity classification. We examine the efficacy of active slice discovery under different choices of feature representations and active learning algorithms. On several slices, we find that uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information, while significantly outperforming baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20718",
        "abs_url": "https://arxiv.org/abs/2511.20718",
        "pdf_url": "https://arxiv.org/pdf/2511.20718",
        "title": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training",
        "authors": [
            "Chenliang Li",
            "Adel Elmahdy",
            "Alex Boyd",
            "Zhongruo Wang",
            "Alfredo Garcia",
            "Parminder Bhatia",
            "Taha Kass-Hout",
            "Cao Xiao",
            "Mingyi Hong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20725",
        "abs_url": "https://arxiv.org/abs/2511.20725",
        "pdf_url": "https://arxiv.org/pdf/2511.20725",
        "title": "Gradient Descent Algorithm Survey",
        "authors": [
            "Deng Fucheng",
            "Wang Wanjie",
            "Gong Ao",
            "Wang Xiaoqi",
            "Wang Fan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Focusing on the practical configuration needs of optimization algorithms in deep learning, this article concentrates on five major algorithms: SGD, Mini-batch SGD, Momentum, Adam, and Lion. It systematically analyzes the core advantages, limitations, and key practical recommendations of each algorithm. The research aims to gain an in-depth understanding of these algorithms and provide a standardized reference for the reasonable selection, parameter tuning, and performance improvement of optimization algorithms in both academic research and engineering practice, helping to solve optimization challenges in different scales of models and various training scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20729",
        "abs_url": "https://arxiv.org/abs/2511.20729",
        "pdf_url": "https://arxiv.org/pdf/2511.20729",
        "title": "Spatio-Temporal Trajectory Foundation Model - Recent Advances and Future Directions",
        "authors": [
            "Sean Bin Yang",
            "Ying Sun",
            "Yunyao Cheng",
            "Yan Lin",
            "Kristian Torp",
            "Jilin Hu"
        ],
        "comments": "This paper has been accepted by CIKM 2025 STIntelligence Workshop",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Foundation models (FMs) have emerged as a powerful paradigm, enabling a diverse range of data analytics and knowledge discovery tasks across scientific fields. Inspired by the success of FMs, particularly large language models, researchers have recently begun to explore spatio-temporal foundation models (STFMs) to improve adaptability and generalization across a wide spectrum of spatio-temporal (ST) tasks. Despite rapid progress, a systematic investigation of trajectory foundation models (TFMs), a crucial subclass of STFMs, is largely lacking. This tutorial addresses this gap by offering a comprehensive overview of recent advances in TFMs, including a taxonomy of existing methodologies and a critical analysis of their strengths and limitations. In addition, the tutorial highlights open challenges and outlines promising research directions to advance spatio-temporal general intelligence through the development of robust, responsible, and transferable TFMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20798",
        "abs_url": "https://arxiv.org/abs/2511.20798",
        "pdf_url": "https://arxiv.org/pdf/2511.20798",
        "title": "Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model",
        "authors": [
            "Rio Alexa Fear",
            "Payel Mukhopadhyay",
            "Michael McCabe",
            "Alberto Bietti",
            "Miles Cranmer"
        ],
        "comments": "16 Pages, 9 Figures. Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph)",
        "abstract": "Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute \"delta\" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20826",
        "abs_url": "https://arxiv.org/abs/2511.20826",
        "pdf_url": "https://arxiv.org/pdf/2511.20826",
        "title": "Effects of Initialization Biases on Deep Neural Network Training Dynamics",
        "authors": [
            "Nicholas Pellegrino",
            "David Szczecina",
            "Paul W. Fieguth"
        ],
        "comments": "5 pages, 2 figures, submitted to the 11th Annual Conference on Vision and Intelligent Systems",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Untrained large neural networks, just after random initialization, tend to favour a small subset of classes, assigning high predicted probabilities to these few classes and approximately zero probability to all others. This bias, termed Initial Guessing Bias, affects the early training dynamics, when the model is fitting to the coarse structure of the data. The choice of loss function against which to train the model has a large impact on how these early dynamics play out. Two recent loss functions, Blurry and Piecewise-zero loss, were designed for robustness to label errors but can become unable to steer the direction of training when exposed to this initial bias. Results indicate that the choice of loss function has a dramatic effect on the early phase training of networks, and highlights the need for careful consideration of how Initial Guessing Bias may interact with various components of the training scheme.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20830",
        "abs_url": "https://arxiv.org/abs/2511.20830",
        "pdf_url": "https://arxiv.org/pdf/2511.20830",
        "title": "Autoregressive Surrogate Modeling of the Solar Wind with Spherical Fourier Neural Operator",
        "authors": [
            "Reza Mansouri",
            "Dustin Kempton",
            "Pete Riley",
            "Rafal Angryk"
        ],
        "comments": "IEEE Conference on Data Mining (ICDM 2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The solar wind, a continuous outflow of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Accurate prediction of features such as high-speed streams and coronal mass ejections is critical for space weather forecasting, but traditional three-dimensional magnetohydrodynamic (MHD) models are computationally expensive, limiting rapid exploration of boundary condition uncertainties. We introduce the first autoregressive machine learning surrogate for steady-state solar wind radial velocity using the Spherical Fourier Neural Operator (SFNO). By predicting a limited radial range and iteratively propagating the solution outward, the model improves accuracy in distant regions compared to a single-step approach. Compared with the numerical HUX surrogate, SFNO demonstrates superior or comparable performance while providing a flexible, trainable, and data-driven alternative, establishing a novel methodology for high-fidelity solar wind modeling. The source code and additional visual results are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20844",
        "abs_url": "https://arxiv.org/abs/2511.20844",
        "pdf_url": "https://arxiv.org/pdf/2511.20844",
        "title": "Pre-train to Gain: Robust Learning Without Clean Labels",
        "authors": [
            "David Szczecina",
            "Nicholas Pellegrino",
            "Paul Fieguth"
        ],
        "comments": "5 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Training deep networks with noisy labels leads to poor generalization and degraded accuracy due to overfitting to label noise. Existing approaches for learning with noisy labels often rely on the availability of a clean subset of data. By pre-training a feature extractor backbone without labels using self-supervised learning (SSL), followed by standard supervised training on the noisy dataset, we can train a more noise robust model without requiring a subset with clean labels. We evaluate the use of SimCLR and Barlow~Twins as SSL methods on CIFAR-10 and CIFAR-100 under synthetic and real world noise. Across all noise rates, self-supervised pre-training consistently improves classification accuracy and enhances downstream label-error detection (F1 and Balanced Accuracy). The performance gap widens as the noise rate increases, demonstrating improved robustness. Notably, our approach achieves comparable results to ImageNet pre-trained models at low noise levels, while substantially outperforming them under high noise conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20873",
        "abs_url": "https://arxiv.org/abs/2511.20873",
        "pdf_url": "https://arxiv.org/pdf/2511.20873",
        "title": "Representation Integrity in Temporal Graph Learning Methods",
        "authors": [
            "Elahe Kooshafar"
        ],
        "comments": "70 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Real-world systems ranging from airline routes to cryptocurrency transfers are naturally modelled as dynamic graphs whose topology changes over time. Conventional benchmarks judge dynamic-graph learners by a handful of task-specific scores, yet seldom ask whether the embeddings themselves remain a truthful, interpretable reflection of the evolving network. We formalize this requirement as representation integrity and derive a family of indexes that measure how closely embedding changes follow graph changes. Three synthetic scenarios, Gradual Merge, Abrupt Move, and Periodic Re-wiring, are used to screen forty-two candidate indexes. Based on which we recommend one index that passes all of our theoretical and empirical tests. In particular, this validated metric consistently ranks the provably stable UASE and IPP models highest. We then use this index to do a comparative study on representation integrity of common dynamic graph learning models. This study exposes the scenario-specific strengths of neural methods, and shows a strong positive rank correlation with one-step link-prediction AUC. The proposed integrity framework, therefore, offers a task-agnostic and interpretable evaluation tool for dynamic-graph representation quality, providing more explicit guidance for model selection and future architecture design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20893",
        "abs_url": "https://arxiv.org/abs/2511.20893",
        "pdf_url": "https://arxiv.org/pdf/2511.20893",
        "title": "Probabilistic Hash Embeddings for Online Learning of Categorical Features",
        "authors": [
            "Aodong Li",
            "Abishek Sankararaman",
            "Balakrishnan Narayanaswamy"
        ],
        "comments": "AAAI 2026 Oral",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We study streaming data with categorical features where the vocabulary of categorical feature values is changing and can even grow unboundedly over time. Feature hashing is commonly used as a pre-processing step to map these categorical values into a feature space of fixed size before learning their embeddings. While these methods have been developed and evaluated for offline or batch settings, in this paper we consider online settings. We show that deterministic embeddings are sensitive to the arrival order of categories and suffer from forgetting in online learning, leading to performance deterioration. To mitigate this issue, we propose a probabilistic hash embedding (PHE) model that treats hash embeddings as stochastic and applies Bayesian online learning to learn incrementally from data. Based on the structure of PHE, we derive a scalable inference algorithm to learn model parameters and infer/update the posteriors of hash embeddings and other latent variables. Our algorithm (i) can handle an evolving vocabulary of categorical items, (ii) is adaptive to new items without forgetting old items, (iii) is implementable with a bounded set of parameters that does not grow with the number of distinct observed values on the stream, and (iv) is invariant to the item arrival order. Experiments in classification, sequence modeling, and recommendation systems in online learning setups demonstrate the superior performance of PHE while maintaining high memory efficiency (consumes as low as 2~4 memory of a one-hot embedding table). Supplementary materials are at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20913",
        "abs_url": "https://arxiv.org/abs/2511.20913",
        "pdf_url": "https://arxiv.org/pdf/2511.20913",
        "title": "Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment",
        "authors": [
            "Yingchuan Sun",
            "Shengpu Tang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Existing studies on reinforcement learning (RL) for sepsis management have mostly followed an established problem setup, in which patient data are aggregated into 4-hour time steps. Although concerns have been raised regarding the coarseness of this time-step size, which might distort patient dynamics and lead to suboptimal treatment policies, the extent to which this is a problem in practice remains unexplored. In this work, we conducted empirical experiments for a controlled comparison of four time-step sizes ($\\Delta t\\!=\\!1,2,4,8$ h) on this domain, following an identical offline RL pipeline. To enable a fair comparison across time-step sizes, we designed action re-mapping methods that allow for evaluation of policies on datasets with different time-step sizes, and conducted cross-$\\Delta t$ model selections under two policy learning setups. Our goal was to quantify how time-step size influences state representation learning, behavior cloning, policy training, and off-policy evaluation. Our results show that performance trends across $\\Delta t$ vary as learning setups change, while policies learned at finer time-step sizes ($\\Delta t = 1$ h and $2$ h) using a static behavior policy achieve the overall best performance and stability. Our work highlights time-step size as a core design choice in offline RL for healthcare and provides evidence supporting alternatives beyond the conventional 4-hour setup.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20927",
        "abs_url": "https://arxiv.org/abs/2511.20927",
        "pdf_url": "https://arxiv.org/pdf/2511.20927",
        "title": "Operationalizing Quantized Disentanglement",
        "authors": [
            "Vitoria Barin-Pacela",
            "Kartik Ahuja",
            "Simon Lacoste-Julien",
            "Pascal Vincent"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Recent theoretical work established the unsupervised identifiability of quantized factors under any diffeomorphism. The theory assumes that quantization thresholds correspond to axis-aligned discontinuities in the probability density of the latent factors. By constraining a learned map to have a density with axis-aligned discontinuities, we can recover the quantization of the factors. However, translating this high-level principle into an effective practical criterion remains challenging, especially under nonlinear maps. Here, we develop a criterion for unsupervised disentanglement by encouraging axis-aligned discontinuities. Discontinuities manifest as sharp changes in the estimated density of factors and form what we call cliffs. Following the definition of independent discontinuities from the theory, we encourage the location of the cliffs along a factor to be independent of the values of the other factors. We show that our method, Cliff, outperforms the baselines on all disentanglement benchmarks, demonstrating its effectiveness in unsupervised disentanglement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20944",
        "abs_url": "https://arxiv.org/abs/2511.20944",
        "pdf_url": "https://arxiv.org/pdf/2511.20944",
        "title": "Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection",
        "authors": [
            "Yaw Osei Adjei"
        ],
        "comments": "8 pages, 12 figures, 7 tables",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies and exploits psychological vulnerabilities, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual adjusted losses, presenting significant economic asymmetry: the cost of a False Negative (fraud loss) exceeds the cost of a False Positive (manual review) by orders of magnitude (approximately 1 to 5,480). This paper examines two detection paradigms for BEC: the Forensic Psycholinguistic Stream, which utilizes CatBoost to analyze psycholinguistic cues with high interpretability and low latency, and the Semantic Stream, which employs DistilBERT for deep learning-based contextual language understanding, offering superior accuracy at higher computational cost. We evaluated DistilBERT on an adversarially poisoned dataset (N = 7,990) generated via our Black Hole protocol, benchmarked on Tesla T4 GPU infrastructure, achieving superior detection (AUC = 1.0000, F1 = 0.9981) with acceptable real-time latency (7.403 milliseconds). CatBoost achieves competitive detection (AUC = 0.9905, F1 = 0.9486) at 8.4x lower latency (0.885 milliseconds), consuming negligible computational resources. For organizations with GPU infrastructure, DistilBERT offers superior accuracy. CatBoost is preferable for edge deployments or cost-sensitive environments due to comparable security and lower operational costs. Both approaches demonstrate return on investment exceeding 99.96% when optimized through cost-sensitive learning, by significantly reducing false negatives and associated financial losses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20992",
        "abs_url": "https://arxiv.org/abs/2511.20992",
        "pdf_url": "https://arxiv.org/pdf/2511.20992",
        "title": "Dataset Poisoning Attacks on Behavioral Cloning Policies",
        "authors": [
            "Akansha Kalra",
            "Soumil Datta",
            "Ethan Gilmore",
            "Duc La",
            "Guanhong Tao",
            "Daniel S. Brown"
        ],
        "comments": "Accepted at EAI SmartSP 2025",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Robotics (cs.RO)",
        "abstract": "Behavior Cloning (BC) is a popular framework for training sequential decision policies from expert demonstrations via supervised learning. As these policies are increasingly being deployed in the real world, their robustness and potential vulnerabilities are an important concern. In this work, we perform the first analysis of the efficacy of clean-label backdoor attacks on BC policies. Our backdoor attacks poison a dataset of demonstrations by injecting a visual trigger to create a spurious correlation that can be exploited at test time. We evaluate how policy vulnerability scales with the fraction of poisoned data, the strength of the trigger, and the trigger type. We also introduce a novel entropy-based test-time trigger attack that substantially degrades policy performance by identifying critical states where test-time triggering of the backdoor is expected to be most effective at degrading performance. We empirically demonstrate that BC policies trained on even minimally poisoned datasets exhibit deceptively high, near-baseline task performance despite being highly vulnerable to backdoor trigger attacks during deployment. Our results underscore the urgent need for more research into the robustness of BC policies, particularly as large-scale datasets are increasingly used to train policies for real-world cyber-physical systems. Videos and code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20997",
        "abs_url": "https://arxiv.org/abs/2511.20997",
        "pdf_url": "https://arxiv.org/pdf/2511.20997",
        "title": "FANoise: Singular Value-Adaptive Noise Modulation for Robust Multimodal Representation Learning",
        "authors": [
            "Jiaoyang Li",
            "Jun Fang",
            "Tianhao Gao",
            "Xiaohui Zhang",
            "Zhiyuan Liu",
            "Chao Liu",
            "Pengzhang Liu",
            "Qixia Jiang"
        ],
        "comments": "13 pages, 5 figures, accept to AAAI2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Representation learning is fundamental to modern machine learning, powering applications such as text retrieval and multimodal understanding. However, learning robust and generalizable representations remains challenging. While prior work has demonstrated that active noise injection, a form of data augmentation, can enhance encoding performance, most existing methods rely on heuristic or static noise, overlooking the dynamic nature of feature distributions during training. In this work, we systematically study the role of noise in representation learning from both gradient-based and feature distribution perspectives, using InfoNCE loss as a representative example. Focusing on multimodal representation learning, we propose FANoise, a novel feature-adaptive noise injection strategy. By leveraging the dynamics of contrastive learning, FANoise effectively mitigates the negative impacts of noise while preserving its benefits. Under this theoretically grounded framework, comprehensive experiments demonstrate that FANoise consistently improves overall performance on multimodal tasks across various base VLM models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21008",
        "abs_url": "https://arxiv.org/abs/2511.21008",
        "pdf_url": "https://arxiv.org/pdf/2511.21008",
        "title": "Estimating Ising Models in Total Variation Distance",
        "authors": [
            "Constantinos Daskalakis",
            "Vardis Kandiros",
            "Rui Yao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We consider the problem of estimating Ising models over $n$ variables in Total Variation (TV) distance, given $l$ independent samples from the model. While the statistical complexity of the problem is well-understood [DMR20], identifying computationally and statistically efficient algorithms has been challenging. In particular, remarkable progress has occurred in several settings, such as when the underlying graph is a tree [DP21, BGPV21], when the entries of the interaction matrix follow a Gaussian distribution [GM24, CK24], or when the bulk of its eigenvalues lie in a small interval [AJK+24, KLV24], but no unified framework for polynomial-time estimation in TV exists so far. Our main contribution is a unified analysis of the Maximum Pseudo-Likelihood Estimator (MPLE) for two general classes of Ising models. The first class includes models that have bounded operator norm and satisfy the Modified Log-Sobolev Inequality (MLSI), a functional inequality that was introduced to study the convergence of the associated Glauber dynamics to stationarity. In the second class of models, the interaction matrix has bounded infinity norm (or bounded width), which is the most common assumption in the literature for structure learning of Ising models. We show how our general results for these classes yield polynomial-time algorithms and optimal or near-optimal sample complexity guarantees in a variety of settings. Our proofs employ a variety of tools from tensorization inequalities to measure decompositions and concentration bounds.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21009",
        "abs_url": "https://arxiv.org/abs/2511.21009",
        "pdf_url": "https://arxiv.org/pdf/2511.21009",
        "title": "ChatGpt Content detection: A new approach using xlm-roberta alignment",
        "authors": [
            "Md Tasnin Tanvir",
            "Dr Santanu Kumar Dash",
            "Ishan Shahnan",
            "Nafis Fuad",
            "Tanvir Rahman",
            "Abdullah Al Faisal",
            "Asadullah Al Mamun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The challenge of separating AI-generated text from human-authored content is becoming more urgent as generative AI technologies like ChatGPT become more widely available. In this work, we address this issue by looking at both the detection of content that has been entirely generated by AI and the identification of human text that has been reworded by AI. In our work, a comprehensive methodology to detect AI- generated text using XLM-RoBERTa, a state-of-the-art multilingual transformer model. Our approach includes rigorous preprocessing, and feature extraction involving perplexity, semantic, and readability features. We fine-tuned the XLM-RoBERTa model on a balanced dataset of human and AI-generated texts and evaluated its performance. The model demonstrated high accuracy and robust performance across various text genres. Additionally, we conducted feature analysis to understand the model's decision-making process, revealing that perplexity and attention-based features are critical in differentiating between human and AI-generated texts. Our findings offer a valuable tool for maintaining academic integrity and contribute to the broader field of AI ethics by promoting transparency and accountability in AI systems. Future research directions include exploring other advanced models and expanding the dataset to enhance the model's generalizability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21011",
        "abs_url": "https://arxiv.org/abs/2511.21011",
        "pdf_url": "https://arxiv.org/pdf/2511.21011",
        "title": "Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning",
        "authors": [
            "Sid Bharthulwar",
            "Stone Tao",
            "Hao Su"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21016",
        "abs_url": "https://arxiv.org/abs/2511.21016",
        "pdf_url": "https://arxiv.org/pdf/2511.21016",
        "title": "Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression",
        "authors": [
            "Liangzu Peng",
            "Aditya Chattopadhyay",
            "Luca Zancato",
            "Elvis Nunez",
            "Wei Xia",
            "Stefano Soatto"
        ],
        "comments": "30 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces this gap by accounting for the full past when predicting the next token, while maintaining SSM-style efficiency. GKA achieves this by solving an online ridge regression problem at test time, with constant memory and linear compute cost in the sequence length. Drawing inspiration from the Kalman Filter, we iteratively solve the online ridge regression problem. However, a critical insight is that standard Kalman filter equations are numerically unstable in low-precision environments (like bfloat16) and difficult to parallelize in modern hardware. We address both challenges through two key innovations: (1) an adaptive regularization strategy with input-dependent gating that controls the condition number of the ridge regression, ensuring numerical stability while balancing memory retention. And (2) the use of Chebyshev Iteration instead of other conventional iterative solvers, which we demonstrate to be more stable in low-precision settings. To further improve scalability, we develop a hardware-aware chunk-wise implementation of Chebyshev Iteration along with custom kernels for backpropagating through our adaptive regularization and gating mechanisms. Empirically, GKA shows strong language understanding capabilites on short-context tasks outperforming existing SSM layers (like Mamba2, GLA and Gated DeltaNet). On long-context, GKA excels at real-world RAG and LongQA tasks up to 128k tokens, achieving more than $10$% relative improvement over other fading memory baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21032",
        "abs_url": "https://arxiv.org/abs/2511.21032",
        "pdf_url": "https://arxiv.org/pdf/2511.21032",
        "title": "A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems",
        "authors": [
            "Yuxuan Zhu",
            "Cong Fu",
            "Yabo Ni",
            "Anxiang Zeng",
            "Yuan Fang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21034",
        "abs_url": "https://arxiv.org/abs/2511.21034",
        "pdf_url": "https://arxiv.org/pdf/2511.21034",
        "title": "Prediction of Herd Life in Dairy Cows Using Multi-Head Attention Transformers",
        "authors": [
            "Mahdi Saki",
            "Justin Lipman"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Dairy farmers should decide to keep or cull a cow based on an objective assessment of her likely performance in the herd. For this purpose, farmers need to identify more resilient cows, which can cope better with farm conditions and complete more lactations. This decision-making process is inherently complex, with significant environmental and economic implications. In this study, we develop an AI-driven model to predict cow longevity using historical multivariate time-series data recorded from birth. Leveraging advanced AI techniques, specifically Multi-Head Attention Transformers, we analysed approximately 780,000 records from 19,000 unique cows across 7 farms in Australia. The results demonstrate that our model achieves an overall determination coefficient of 83% in predicting herd life across the studied farms, highlighting its potential for practical application in dairy herd management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21035",
        "abs_url": "https://arxiv.org/abs/2511.21035",
        "pdf_url": "https://arxiv.org/pdf/2511.21035",
        "title": "RAVQ-HoloNet: Rate-Adaptive Vector-Quantized Hologram Compression",
        "authors": [
            "Shima Rafiei",
            "Zahra Nabizadeh Shahr Babak",
            "Shadrokh Samavi",
            "Shahram Shirani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Holography offers significant potential for AR/VR applications, yet its adoption is limited by the high demands of data compression. Existing deep learning approaches generally lack rate adaptivity within a single network. We present RAVQ-HoloNet, a rate-adaptive vector quantization framework that achieves high-fidelity reconstructions at low and ultra-low bit rates, outperforming current state-of-the-art methods. In low bit, our method exceeds by -33.91% in BD-Rate and achieves a BD-PSNR of 1.02 dB from the best existing method demonstrated by the rate-distortion curve.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21048",
        "abs_url": "https://arxiv.org/abs/2511.21048",
        "pdf_url": "https://arxiv.org/pdf/2511.21048",
        "title": "FedAPA: Federated Learning with Adaptive Prototype Aggregation Toward Heterogeneous Wi-Fi CSI-based Crowd Counting",
        "authors": [
            "Jingtao Guo",
            "Yuyi Mao",
            "Ivan Wang-Hei Ho"
        ],
        "comments": "17 pages, 11 figures, this article was submitted to IEEE for possible publication",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Wi-Fi channel state information (CSI)-based sensing provides a non-invasive, device-free approach for tasks such as human activity recognition and crowd counting, but large-scale deployment is hindered by the need for extensive site-specific training data. Federated learning (FL) offers a way to avoid raw data sharing but is challenged by heterogeneous sensing data and device resources. This paper proposes FedAPA, a collaborative Wi-Fi CSI-based sensing algorithm that uses adaptive prototype aggregation (APA) strategy to assign similarity-based weights to peer prototypes, enabling adaptive client contributions and yielding a personalized global prototype for each client instead of a fixed-weight aggregation. During local training, we adopt a hybrid objective that combines classification learning with representation contrastive learning to align local and global knowledge. We provide a convergence analysis of FedAPA and evaluate it in a real-world distributed Wi-Fi crowd counting scenario with six environments and up to 20 people. The results show that our method outperform multiple baselines in terms of accuracy, F1 score, mean absolute error (MAE), and communication overhead, with FedAPA achieving at least a 9.65% increase in accuracy, a 9% gain in F1 score, a 0.29 reduction in MAE, and a 95.94% reduction in communication overhead.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21054",
        "abs_url": "https://arxiv.org/abs/2511.21054",
        "pdf_url": "https://arxiv.org/pdf/2511.21054",
        "title": "Efficient Diffusion Planning with Temporal Diffusion",
        "authors": [
            "Jiaming Guo",
            "Rui Zhang",
            "Zerun Li",
            "Yunkai Gao",
            "Shaohui Peng",
            "Siming Lan",
            "Xing Hu",
            "Zidong Du",
            "Xishan Zhang",
            "Ling Li"
        ],
        "comments": "Accepted by the AAAI26 Conference Main Track",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21056",
        "abs_url": "https://arxiv.org/abs/2511.21056",
        "pdf_url": "https://arxiv.org/pdf/2511.21056",
        "title": "A Unified Understanding of Offline Data Selection and Online Self-refining Generation for Post-training LLMs",
        "authors": [
            "Quan Xiao",
            "Tianyi Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Optimization and Control (math.OC)",
        "abstract": "Offline data selection and online self-refining generation, which enhance the data quality, are crucial steps in adapting large language models (LLMs) to specific downstream tasks. We tackle offline data selection and online self-refining generations through an optimization perspective. Specifically, bilevel data selection is used for offline data selection with respect to the validation dataset, and we treat online self-refining generation as a model adaptation step of selecting the model trained on current responses that best fits the validation data. Our framework offers a unified understanding of offline data selection and self-refining generation by assigning a learned data weight to each question and response, either explicitly or implicitly. For the first time, we theoretically demonstrate the effectiveness of the bilevel data selection framework and demonstrate its performance gains over unfiltered direct mixing baselines. By combining offline data with validation-weighted online generations, our method enhances fine-tuning performance. Experiments on quality enhancement and safety-aware LLM fine-tuning validate its effectiveness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21063",
        "abs_url": "https://arxiv.org/abs/2511.21063",
        "pdf_url": "https://arxiv.org/pdf/2511.21063",
        "title": "G-Net: A Provably Easy Construction of High-Accuracy Random Binary Neural Networks",
        "authors": [
            "Alireza Aghasi",
            "Nicholas Marshall",
            "Saeid Pourmand",
            "Wyatt Whiting"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We propose a novel randomized algorithm for constructing binary neural networks with tunable accuracy. This approach is motivated by hyperdimensional computing (HDC), which is a brain-inspired paradigm that leverages high-dimensional vector representations, offering efficient hardware implementation and robustness to model corruptions. Unlike traditional low-precision methods that use quantization, we consider binary embeddings of data as points in the hypercube equipped with the Hamming distance. We propose a novel family of floating-point neural networks, G-Nets, which are general enough to mimic standard network layers. Each floating-point G-Net has a randomized binary embedding, an embedded hyperdimensional (EHD) G-Net, that retains the accuracy of its floating-point counterparts, with theoretical guarantees, due to the concentration of measure. Empirically, our binary models match convolutional neural network accuracies and outperform prior HDC models by large margins, for example, we achieve almost 30\\% higher accuracy on CIFAR-10 compared to prior HDC models. G-Nets are a theoretically justified bridge between neural networks and randomized binary neural networks, opening a new direction for constructing robust binary/quantized deep learning models. Our implementation is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21076",
        "abs_url": "https://arxiv.org/abs/2511.21076",
        "pdf_url": "https://arxiv.org/pdf/2511.21076",
        "title": "Deceptron: Learned Local Inverses for Fast and Stable Physics Inversion",
        "authors": [
            "Aaditya L. Kachhadiya"
        ],
        "comments": "10 pages, 11 main figures. Accepted for poster presentation at the NeurIPS 2025 Machine Learning and the Physical Sciences Workshop",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Inverse problems in the physical sciences are often ill-conditioned in input space, making progress step-size sensitive. We propose the Deceptron, a lightweight bidirectional module that learns a local inverse of a differentiable forward surrogate. Training combines a supervised fit, forward-reverse consistency, a lightweight spectral penalty, a soft bias tie, and a Jacobian Composition Penalty (JCP) that encourages $J_g(f(x))\\,J_f(x)\\!\\approx\\!I$ via JVP/VJP probes. At solve time, D-IPG (Deceptron Inverse-Preconditioned Gradient) takes a descent step in output space, pulls it back through $g$, and projects under the same backtracking and stopping rules as baselines. On Heat-1D initial-condition recovery and a Damped Oscillator inverse problem, D-IPG reaches a fixed normalized tolerance with $\\sim$20$\\times$ fewer iterations on Heat and $\\sim$2-3$\\times$ fewer on Oscillator than projected gradient, competitive in iterations and cost with Gauss-Newton. Diagnostics show JCP reduces a measured composition error and tracks iteration gains. We also preview a single-scale 2D instantiation, DeceptronNet (v0), that learns few-step corrections under a strict fairness protocol and exhibits notably fast convergence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21092",
        "abs_url": "https://arxiv.org/abs/2511.21092",
        "pdf_url": "https://arxiv.org/pdf/2511.21092",
        "title": "MNM : Multi-level Neuroimaging Meta-analysis with Hyperbolic Brain-Text Representations",
        "authors": [
            "Seunghun Baek",
            "Jaejin Lee",
            "Jaeyoon Sim",
            "Minjae Jeong",
            "Won Hwa Kim"
        ],
        "comments": "MICCAI 2025 (Provisional Accept; top ~9%)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Various neuroimaging studies suffer from small sample size problem which often limit their reliability. Meta-analysis addresses this challenge by aggregating findings from different studies to identify consistent patterns of brain activity. However, traditional approaches based on keyword retrieval or linear mappings often overlook the rich hierarchical structure in the brain. In this work, we propose a novel framework that leverages hyperbolic geometry to bridge the gap between neuroscience literature and brain activation maps. By embedding text from research articles and corresponding brain images into a shared hyperbolic space via the Lorentz model, our method captures both semantic similarity and hierarchical organization inherent in neuroimaging data. In the hyperbolic space, our method performs multi-level neuroimaging meta-analysis (MNM) by 1) aligning brain and text embeddings for semantic correspondence, 2) guiding hierarchy between text and brain activations, and 3) preserving the hierarchical relationships within brain activation patterns. Experimental results demonstrate that our model outperforms baselines, offering a robust and interpretable paradigm of multi-level neuroimaging meta-analysis via hyperbolic brain-text representation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21095",
        "abs_url": "https://arxiv.org/abs/2511.21095",
        "pdf_url": "https://arxiv.org/pdf/2511.21095",
        "title": "Generative Early Stage Ranking",
        "authors": [
            "Juhee Hong",
            "Meng Liu",
            "Shengzhi Wang",
            "Xiaoheng Mao",
            "Huihui Cheng",
            "Leon Gao",
            "Christopher Leung",
            "Jin Zhou",
            "Chandra Mouli Sekar",
            "Zhao Zhu",
            "Ruochen Liu",
            "Tuan Trieu",
            "Dawei Sun",
            "Jeet Kanjani",
            "Rui Li",
            "Jing Qian",
            "Xuan Cao",
            "Minjie Fan",
            "Mingze Gao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the \"user-item decoupling\" approach, where independently learned user and item representations are only combined at the final layer. While efficient, this design is limited in effectiveness, as it struggles to capture fine-grained user-item affinities and cross-signals. To address these, we propose the Generative Early Stage Ranking (GESR) paradigm, introducing the Mixture of Attention (MoA) module which leverages diverse attention mechanisms to bridge the effectiveness gap: the Hard Matching Attention (HMA) module encodes explicit cross-signals by computing raw match counts between user and item features; the Target-Aware Self Attention module generates target-aware user representations conditioned on the item, enabling more personalized learning; and the Cross Attention modules facilitate early and more enriched interactions between user-item features. MoA's specialized attention encodings are further refined in the final layer through a Multi-Logit Parameterized Gating (MLPG) module, which integrates the newly learned embeddings via gating and produces secondary logits that are fused with the primary logit. To address the efficiency and latency challenges, we have introduced a comprehensive suite of optimization techniques. These span from custom kernels that maximize the capabilities of the latest hardware to efficient serving solutions powered by caching mechanisms. The proposed GESR paradigm has shown substantial improvements in topline metrics, engagement, and consumption tasks, as validated by both offline and online experiments. To the best of our knowledge, this marks the first successful deployment of full target-aware attention sequence modeling within an ESR stage at such a scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21103",
        "abs_url": "https://arxiv.org/abs/2511.21103",
        "pdf_url": "https://arxiv.org/pdf/2511.21103",
        "title": "From Bits to Rounds: Parallel Decoding with Exploration for Diffusion Language Models",
        "authors": [
            "Hengyu Fu",
            "Baihe Huang",
            "Virginia Adams",
            "Charles Wang",
            "Venkat Srinivasan",
            "Jiantao Jiao"
        ],
        "comments": "24 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Diffusion Language Models (DLMs) have recently emerged as a strong alternative to autoregressive language models (LMs). DLMs offer comparable accuracy with faster inference speed via parallel decoding. However, standard DLM decoding strategies relying on high-confidence tokens encounter an inherent information-theoretic bottleneck that restricts decoding progress and ultimately slows generation. We demonstrate both theoretically and empirically that prioritizing high-confidence tokens is inherently inefficient. High-probability tokens carry negligible information and strictly relying on them limits the effective progress made in each decoding round. We prove that the number of decoding rounds must grow linearly with the sample's total information (negative log-likelihood) and inversely with the per-round information budget, establishing a bits-to-rounds principle. We also propose Explore-Then-Exploit (ETE), a training-free decoding strategy that maximizes information throughput and decoding efficiency. ETE combines cross-block decoding with targeted exploration of high-uncertainty tokens to reshape the conditional distribution and trigger cascades of confident predictions. Experiments verify our theoretical bounds and demonstrate that ETE consistently reduces the required number of decoding rounds compared to confidence-only baselines without compromising generation quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21104",
        "abs_url": "https://arxiv.org/abs/2511.21104",
        "pdf_url": "https://arxiv.org/pdf/2511.21104",
        "title": "BRIDGE: Building Representations In Domain Guided Program Verification",
        "authors": [
            "Robert Joseph George",
            "Carson Eisenach",
            "Udaya Ghai",
            "Dominique Perrault-Joncas",
            "Anima Anandkumar",
            "Dean Foster"
        ],
        "comments": "Approx. 31 pages including appendices, 11 figures, 4 tables. Empirical study of LLM-based verified program synthesis in Lean4 (code, specs, and proofs)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) have achieved impressive results in code generation, yet struggle with program verification, especially in interactive proof frameworks such as Lean4. A central challenge is scalability: verified synthesis requires not just code, but also precise specifications and correctness proofs, and existing approaches rarely span all three domains. We present BRIDGE, the first systematic study of structured prompting for scalable verified program generation. BRIDGE decomposes verification into three interconnected domains: Code (executable implementations), Specifications (formal intent statements), and Proofs (constructive correctness arguments). Our key idea is to elicit distinct reasoning behaviors functional, specification-driven, and proof-oriented as intermediate representations that preserve semantic structure and connect these domains. Through systematic ablations, we show that this approach substantially improves both accuracy and efficiency beyond standard error feedback methods. For example, functional reasoning improves correctness of code in formal languages (Lean4) by nearly 1.5x (pass@5) over direct baselines. In inference-time compute, functional reasoning is also 2x more efficient, achieving higher pass rates with fewer generations and lower total sampling budgets. Similarly, we find that specification-driven prompting boosts Python coding pass rates by up to 17.5%. These findings suggest that structured domain alignment is a promising direction for advancing verified synthesis. BRIDGE establishes a foundation for training via expert iteration or RLVR, enabling models to internalize these reasoning strategies across code, specifications, and proofs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21109",
        "abs_url": "https://arxiv.org/abs/2511.21109",
        "pdf_url": "https://arxiv.org/pdf/2511.21109",
        "title": "Interpretable Fair Clustering",
        "authors": [
            "Mudi Jiang",
            "Jiahui Zhou",
            "Xinying Liu",
            "Zengyou He",
            "Zhikui Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Fair clustering has gained increasing attention in recent years, especially in applications involving socially sensitive attributes. However, existing fair clustering methods often lack interpretability, limiting their applicability in high-stakes scenarios where understanding the rationale behind clustering decisions is essential. In this work, we address this limitation by proposing an interpretable and fair clustering framework, which integrates fairness constraints into the structure of decision trees. Our approach constructs interpretable decision trees that partition the data while ensuring fair treatment across protected groups. To further enhance the practicality of our framework, we also introduce a variant that requires no fairness hyperparameter tuning, achieved through post-pruning a tree constructed without fairness constraints. Extensive experiments on both real-world and synthetic datasets demonstrate that our method not only delivers competitive clustering performance and improved fairness, but also offers additional advantages such as interpretability and the ability to handle multiple sensitive attributes. These strengths enable our method to perform robustly under complex fairness constraints, opening new possibilities for equitable and transparent clustering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21118",
        "abs_url": "https://arxiv.org/abs/2511.21118",
        "pdf_url": "https://arxiv.org/pdf/2511.21118",
        "title": "Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination",
        "authors": [
            "Pius Onobhayedo",
            "Paul Osemudiame Oamen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large this http URL, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21120",
        "abs_url": "https://arxiv.org/abs/2511.21120",
        "pdf_url": "https://arxiv.org/pdf/2511.21120",
        "title": "Learning Cell-Aware Hierarchical Multi-Modal Representations for Robust Molecular Modeling",
        "authors": [
            "Mengran Li",
            "Zelin Zang",
            "Wenbin Xing",
            "Junzhou Chen",
            "Ronghui Zhang",
            "Jiebo Luo",
            "Stan Z. Li"
        ],
        "comments": "Accepted to AAAI 2026 (Oral)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Understanding how chemical perturbations propagate through biological systems is essential for robust molecular property prediction. While most existing methods focus on chemical structures alone, recent advances highlight the crucial role of cellular responses such as morphology and gene expression in shaping drug effects. However, current cell-aware approaches face two key limitations: (1) modality incompleteness in external biological data, and (2) insufficient modeling of hierarchical dependencies across molecular, cellular, and genomic levels. We propose CHMR (Cell-aware Hierarchical Multi-modal Representations), a robust framework that jointly models local-global dependencies between molecules and cellular responses and captures latent biological hierarchies via a novel tree-structured vector quantization module. Evaluated on nine public benchmarks spanning 728 tasks, CHMR outperforms state-of-the-art baselines, yielding average improvements of 3.6% on classification and 17.2% on regression tasks. These results demonstrate the advantage of hierarchy-aware, multimodal learning for reliable and biologically grounded molecular representations, offering a generalizable framework for integrative biomedical modeling. The code is in this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21140",
        "abs_url": "https://arxiv.org/abs/2511.21140",
        "pdf_url": "https://arxiv.org/pdf/2511.21140",
        "title": "How to Correctly Report LLM-as-a-Judge Evaluations",
        "authors": [
            "Chungpa Lee",
            "Thomas Zeng",
            "Jongwon Jeong",
            "Jy-yong Sohn",
            "Kangwook Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Applications (stat.AP); Machine Learning (stat.ML)",
        "abstract": "Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21208",
        "abs_url": "https://arxiv.org/abs/2511.21208",
        "pdf_url": "https://arxiv.org/pdf/2511.21208",
        "title": "I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation",
        "authors": [
            "Lucas Thil",
            "Jesse Read",
            "Rim Kaddah",
            "Guillaume Doquet"
        ],
        "comments": "Included in the conference series: Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21211",
        "abs_url": "https://arxiv.org/abs/2511.21211",
        "pdf_url": "https://arxiv.org/pdf/2511.21211",
        "title": "Robust Gene Prioritization via Fast-mRMR Feature Selection in high-dimensional omics data",
        "authors": [
            "Rubn Fernndez-Farelo",
            "Jorge Paz-Ruza",
            "Bertha Guijarro-Berdias",
            "Amparo Alonso-Betanzos",
            "Alex A. Freitas"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Gene prioritization (identifying genes potentially associated with a biological process) is increasingly tackled with Artificial Intelligence. However, existing methods struggle with the high dimensionality and incomplete labelling of biomedical data. This work proposes a more robust and efficient pipeline that leverages Fast-mRMR feature selection to retain only relevant, non-redundant features for classifiers. This enables us to build simpler and more effective models, as well as to combine different biological feature sets. Experiments on Dietary Restriction datasets show significant improvements over existing methods, proving that feature selection can be critical for reliable gene prioritization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21276",
        "abs_url": "https://arxiv.org/abs/2511.21276",
        "pdf_url": "https://arxiv.org/pdf/2511.21276",
        "title": "A Physics-Informed U-net-LSTM Network for Data-Driven Seismic Response Modeling of Structures",
        "authors": [
            "Sutirtha Biswas",
            "Kshitij Kumar Yadav"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate and efficient seismic response prediction is essential for the design of resilient structures. While the Finite Element Method (FEM) remains the standard for nonlinear seismic analysis, its high computational demands limit its scalability and real time applicability. Recent developments in deep learning, particularly Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short Term Memory (LSTM) models, have shown promise in reducing the computational cost of nonlinear seismic analysis of structures. However, these data driven models often struggle to generalize and capture the underlying physics, leading to reduced reliability. We propose a novel Physics Informed U Net LSTM framework that integrates physical laws with deep learning to enhance both accuracy and efficiency. By embedding domain specific constraints into the learning process, the proposed model achieves improved predictive performance over conventional Machine Learning architectures. This hybrid approach bridges the gap between purely data driven methods and physics based modeling, offering a robust and computationally efficient alternative for seismic response prediction of structures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21320",
        "abs_url": "https://arxiv.org/abs/2511.21320",
        "pdf_url": "https://arxiv.org/pdf/2511.21320",
        "title": "Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models",
        "authors": [
            "Heiko Oppel",
            "Andreas Spilz",
            "Michael Munz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21335",
        "abs_url": "https://arxiv.org/abs/2511.21335",
        "pdf_url": "https://arxiv.org/pdf/2511.21335",
        "title": "TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models",
        "authors": [
            "Haksoo Lim",
            "Jaehoon Lee",
            "Sewon Park",
            "Minjung Kim",
            "Noseong Park"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Score-based generative models (SGMs) have demonstrated unparalleled sampling quality and diversity in numerous fields, such as image generation, voice synthesis, and tabular data synthesis, etc. Inspired by those outstanding results, we apply SGMs to synthesize time-series by learning its conditional score function. To this end, we present a conditional score network for time-series synthesis, deriving a denoising score matching loss tailored for our purposes. In particular, our presented denoising score matching loss is the conditional denoising score matching loss for time-series synthesis. In addition, our framework is such flexible that both regular and irregular time-series can be synthesized with minimal changes to our model design. Finally, we obtain exceptional synthesis performance on various time-series datasets, achieving state-of-the-art sampling diversity and quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21338",
        "abs_url": "https://arxiv.org/abs/2511.21338",
        "pdf_url": "https://arxiv.org/pdf/2511.21338",
        "title": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
        "authors": [
            "Julianna Piskorz",
            "Cristina Pinneri",
            "Alvaro Correia",
            "Motasem Alfarra",
            "Risheek Garrepalli",
            "Christos Louizos"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21354",
        "abs_url": "https://arxiv.org/abs/2511.21354",
        "pdf_url": "https://arxiv.org/pdf/2511.21354",
        "title": "Best Practices for Machine Learning Experimentation in Scientific Applications",
        "authors": [
            "Umberto Michelucci",
            "Francesca Venturini"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Machine learning (ML) is increasingly adopted in scientific research, yet the quality and reliability of results often depend on how experiments are designed and documented. Poor baselines, inconsistent preprocessing, or insufficient validation can lead to misleading conclusions about model performance. This paper presents a practical and structured guide for conducting ML experiments in scientific applications, focussing on reproducibility, fair comparison, and transparent reporting. We outline a step-by-step workflow, from dataset preparation to model selection and evaluation, and propose metrics that account for overfitting and instability across validation folds, including the Logarithmic Overfitting Ratio (LOR) and the Composite Overfitting Score (COS). Through recommended practices and example reporting formats, this work aims to support researchers in establishing robust baselines and drawing valid evidence-based insights from ML models applied to scientific problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21363",
        "abs_url": "https://arxiv.org/abs/2511.21363",
        "pdf_url": "https://arxiv.org/pdf/2511.21363",
        "title": "The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods",
        "authors": [
            "Kevin Iselborn",
            "David Dembinsky",
            "Adriano Lucieri",
            "Andreas Dengel"
        ],
        "comments": "13 pages, 10 figures, 5 tables, accepted at AAAI SECURE-AI4H workshop",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The utility of an explanation method critically depends on its fidelity to the underlying machine learning model. Especially in high-stakes medical settings, clinicians and regulators require explanations that faithfully reflect the model's decision process. Existing fidelity metrics such as Infidelity rely on Monte Carlo approximation, which demands numerous model evaluations and introduces uncertainty due to random sampling. This work proposes a novel metric for evaluating the fidelity of local feature attribution methods by modifying the existing Prediction Change (PC) metric within the Guided Perturbation Experiment. By incorporating the direction of both perturbation and attribution, the proposed Directed Prediction Change (DPC) metric achieves an almost tenfold speedup and eliminates randomness, resulting in a deterministic and trustworthy evaluation procedure that measures the same property as local Infidelity. DPC is evaluated on two datasets (skin lesion images and financial tabular data), two black-box models, seven explanation algorithms, and a wide range of hyperparameters. Across $4\\,744$ distinct explanations, the results demonstrate that DPC, together with PC, enables a holistic and computationally efficient evaluation of both baseline-oriented and local feature attribution methods, while providing deterministic and reproducible outcomes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21377",
        "abs_url": "https://arxiv.org/abs/2511.21377",
        "pdf_url": "https://arxiv.org/pdf/2511.21377",
        "title": "Controlling changes to attention logits",
        "authors": [
            "Ben Anson",
            "Laurence Aitchison"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying normalization to queries and keys, known as `QK norm', fixes stability issues in practice, but is not always applicable. For example, QK norm is not compatible with Multi Latent Attention (MLA) because QK norm requires full materialization of queries and keys during inference, which is not done in MLA. In this paper we suggest that controlling the changes to logits is important for stability. We show that these changes are controllable by assigning parameter-dependent learning rates to the query and key weights. We find that our cheap intervention allows us to increase the base learning rate of the network, outperform other methods in the MLA setting, and achieve performance competitive with QK norm when using Multi-head Attention.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21381",
        "abs_url": "https://arxiv.org/abs/2511.21381",
        "pdf_url": "https://arxiv.org/pdf/2511.21381",
        "title": "BanglaASTE: A Novel Framework for Aspect-Sentiment-Opinion Extraction in Bangla E-commerce Reviews Using Ensemble Deep Learning",
        "authors": [
            "Ariful Islam",
            "Md Rifat Hossen",
            "Abir Ahmed",
            "B M Taslimul Haque"
        ],
        "comments": "Presented at the 2025 IEEE International Conference on Signal Processing, Information, Communication and Systems (SPICSCON), November 21-22, 2025, University of Rajshahi, Bangladesh. 6 pages, ensemble deep learning, 3,345 annotated Bangla product reviews",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Aspect-Based Sentiment Analysis (ABSA) has emerged as a critical tool for extracting fine-grained sentiment insights from user-generated content, particularly in e-commerce and social media domains. However, research on Bangla ABSA remains significantly underexplored due to the absence of comprehensive datasets and specialized frameworks for triplet extraction in this language. This paper introduces BanglaASTE, a novel framework for Aspect Sentiment Triplet Extraction (ASTE) that simultaneously identifies aspect terms, opinion expressions, and sentiment polarities from Bangla product reviews. Our contributions include: (1) creation of the first annotated Bangla ASTE dataset containing 3,345 product reviews collected from major e-commerce platforms including Daraz, Facebook, and Rokomari; (2) development of a hybrid classification framework that employs graph-based aspect-opinion matching with semantic similarity techniques; and (3) implementation of an ensemble model combining BanglaBERT contextual embeddings with XGBoost boosting algorithms for enhanced triplet extraction performance. Experimental results demonstrate that our ensemble approach achieves superior performance with 89.9% accuracy and 89.1% F1-score, significantly outperforming baseline models across all evaluation metrics. The framework effectively addresses key challenges in Bangla text processing including informal expressions, spelling variations, and data sparsity. This research advances the state-of-the-art in low-resource language sentiment analysis and provides a scalable solution for Bangla e-commerce analytics applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21414",
        "abs_url": "https://arxiv.org/abs/2511.21414",
        "pdf_url": "https://arxiv.org/pdf/2511.21414",
        "title": "SUPN: Shallow Universal Polynomial Networks",
        "authors": [
            "Zachary Morrow",
            "Michael Penwarden",
            "Brian Chen",
            "Aurya Javeed",
            "Akil Narayan",
            "John D. Jakeman"
        ],
        "comments": "25 pages, supplementary material",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "Deep neural networks (DNNs) and Kolmogorov-Arnold networks (KANs) are popular methods for function approximation due to their flexibility and expressivity. However, they typically require a large number of trainable parameters to produce a suitable approximation. Beyond making the resulting network less transparent, overparameterization creates a large optimization space, likely producing local minima in training that have quite different generalization errors. In this case, network initialization can have an outsize impact on the model's out-of-sample accuracy. For these reasons, we propose shallow universal polynomial networks (SUPNs). These networks replace all but the last hidden layer with a single layer of polynomials with learnable coefficients, leveraging the strengths of DNNs and polynomials to achieve sufficient expressivity with far fewer parameters. We prove that SUPNs converge at the same rate as the best polynomial approximation of the same degree, and we derive explicit formulas for quasi-optimal SUPN parameters. We complement theory with an extensive suite of numerical experiments involving SUPNs, DNNs, KANs, and polynomial projection in one, two, and ten dimensions, consisting of over 13,000 trained models. On the target functions we numerically studied, for a given number of trainable parameters, the approximation error and variability are often lower for SUPNs than for DNNs and KANs by an order of magnitude. In our examples, SUPNs even outperform polynomial projection on non-smooth functions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21465",
        "abs_url": "https://arxiv.org/abs/2511.21465",
        "pdf_url": "https://arxiv.org/pdf/2511.21465",
        "title": "Ensemble Performance Through the Lens of Linear Independence of Classifier Votes in Data Streams",
        "authors": [
            "Enes Bektas",
            "Fazli Can"
        ],
        "comments": "14 pages, 3 figures, 5 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Ensemble learning improves classification performance by combining multiple base classifiers. While increasing the number of classifiers generally enhances accuracy, excessively large ensembles can lead to computational inefficiency and diminishing returns. This paper investigates the relationship between ensemble size and performance through the lens of linear independence among classifier votes in data streams. We propose that ensembles composed of linearly independent classifiers maximize representational capacity, particularly under a geometric model. We then generalize the importance of linear independence to the weighted majority voting problem. By modeling the probability of achieving linear independence among classifier outputs, we derive a theoretical framework that explains the trade-off between ensemble size and accuracy. Our analysis leads to a theoretical estimate of the ensemble size required to achieve a user-specified probability of linear independence. We validate our theory through experiments on both real-world and synthetic datasets using two ensemble methods, OzaBagging and GOOWE. Our results confirm that this theoretical estimate effectively identifies the point of performance saturation for robust ensembles like OzaBagging. Conversely, for complex weighting schemes like GOOWE, our framework reveals that high theoretical diversity can trigger algorithmic instability. Our implementation is publicly available to support reproducibility and future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21466",
        "abs_url": "https://arxiv.org/abs/2511.21466",
        "pdf_url": "https://arxiv.org/pdf/2511.21466",
        "title": "Mean-Field Limits for Two-Layer Neural Networks Trained with Consensus-Based Optimization",
        "authors": [
            "William De Deyn",
            "Michael Herty",
            "Giovanni Samaey"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "We study two-layer neural networks and train these with a particle-based method called consensus-based optimization (CBO). We compare the performance of CBO against Adam on two test cases and demonstrate how a hybrid approach, combining CBO with Adam, provides faster convergence than CBO. In the context of multi-task learning, we recast CBO into a formulation that offers less memory overhead. The CBO method allows for a mean-field limit formulation, which we couple with the mean-field limit of the neural network. To this end, we first reformulate CBO within the optimal transport framework. Finally, in the limit of infinitely many particles, we define the corresponding dynamics on the Wasserstein-over-Wasserstein space and show that the variance decreases monotonically.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21500",
        "abs_url": "https://arxiv.org/abs/2511.21500",
        "pdf_url": "https://arxiv.org/pdf/2511.21500",
        "title": "Lost in Time? A Meta-Learning Framework for Time-Shift-Tolerant Physiological Signal Transformation",
        "authors": [
            "Qian Hong",
            "Cheng Bian",
            "Xiao Zhou",
            "Xiaoyu Li",
            "Yelei Li",
            "Zijing Zeng"
        ],
        "comments": "The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 26)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Translating non-invasive signals such as photoplethysmography (PPG) and ballistocardiography (BCG) into clinically meaningful signals like arterial blood pressure (ABP) is vital for continuous, low-cost healthcare monitoring. However, temporal misalignment in multimodal signal transformation impairs transformation accuracy, especially in capturing critical features like ABP peaks. Conventional synchronization methods often rely on strong similarity assumptions or manual tuning, while existing Learning with Noisy Labels (LNL) approaches are ineffective under time-shifted supervision, either discarding excessive data or failing to correct label shifts. To address this challenge, we propose ShiftSyncNet, a meta-learning-based bi-level optimization framework that automatically mitigates performance degradation due to time misalignment. It comprises a transformation network (TransNet) and a time-shift correction network (SyncNet), where SyncNet learns time offsets between training pairs and applies Fourier phase shifts to align supervision signals. Experiments on one real-world industrial dataset and two public datasets show that ShiftSyncNet outperforms strong baselines by 9.4%, 6.0%, and 12.8%, respectively. The results highlight its effectiveness in correcting time shifts, improving label quality, and enhancing transformation accuracy across diverse misalignment scenarios, pointing toward a unified direction for addressing temporal inconsistencies in multimodal physiological transformation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21513",
        "abs_url": "https://arxiv.org/abs/2511.21513",
        "pdf_url": "https://arxiv.org/pdf/2511.21513",
        "title": "IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference",
        "authors": [
            "Wanli Zhong",
            "Haibo Feng",
            "Zirui Zhou",
            "Hanyang Peng",
            "Shiqi Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the dominant bottleneck. This stage incurs a costly dequantize-softmax-requantize detour, which can account for up to 65% of total attention latency and disrupts the end-to-end integer dataflow critical for edge hardware efficiency. To address this limitation, we present IntAttention, the first fully integer, plug-and-play attention pipeline without retraining. At the core of our approach lies IndexSoftmax, a hardware-friendly operator that replaces floating-point exponentials entirely within the integer domain. IntAttention integrates sparsity-aware clipping, a 32-entry lookup-table approximation, and direct integer normalization, thereby eliminating all datatype conversion overhead. We evaluate IntAttention and demonstrate consistent and substantial gains. Our method achieves up to 3.7x speedup and 61% energy reduction over FP16 baselines and 2.0x faster than conventional INT8 attention pipelines on Armv8 CPUs. These gains are achieved with high-fidelity accuracy comparable to baselines across diverse language and vision models, enabling practical and efficient Transformer inference on commodity edge devices. Code will be released in later version of this work.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21531",
        "abs_url": "https://arxiv.org/abs/2511.21531",
        "pdf_url": "https://arxiv.org/pdf/2511.21531",
        "title": "Predictive Safety Shield for Dyna-Q Reinforcement Learning",
        "authors": [
            "Jin Pin",
            "Krasowski Hanna",
            "Vanneaux Elena"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Systems and Control (eess.SY)",
        "abstract": "Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions. In this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space. Our safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees. Our experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21537",
        "abs_url": "https://arxiv.org/abs/2511.21537",
        "pdf_url": "https://arxiv.org/pdf/2511.21537",
        "title": "Context-Specific Causal Graph Discovery with Unobserved Contexts: Non-Stationarity, Regimes and Spatio-Temporal Patterns",
        "authors": [
            "Martin Rabel",
            "Jakob Runge"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Real-world data, for example in climate applications, often consists of spatially gridded time series data or data with comparable structure. While the underlying system is often believed to behave similar at different points in space and time, those variations that do exist are twofold relevant: They often encode important information in and of themselves. And they may negatively affect the stability / convergence and reliability\\Slash{}validity of results of algorithms assuming stationarity or space-translation invariance. We study the information encoded in changes of the causal graph, with stability in mind. An analysis of this general task identifies two core challenges. We develop guiding principles to overcome these challenges, and provide a framework realizing these principles by modifying constraint-based causal discovery approaches on the level of independence testing. This leads to an extremely modular, easily extensible and widely applicable framework. It can leverage existing constraint-based causal discovery methods (demonstrated on IID-algorithms PC, PC-stable, FCI and time series algorithms PCMCI, PCMCI+, LPCMCI) with little to no modification. The built-in modularity allows to systematically understand and improve upon an entire array of subproblems. By design, it can be extended by leveraging insights from change-point-detection, clustering, independence-testing and other well-studied related problems. The division into more accessible sub-problems also simplifies the understanding of fundamental limitations, hyperparameters controlling trade-offs and the statistical interpretation of results. An open-source implementation will be available soon.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21560",
        "abs_url": "https://arxiv.org/abs/2511.21560",
        "pdf_url": "https://arxiv.org/pdf/2511.21560",
        "title": "Computing Strategic Responses to Non-Linear Classifiers",
        "authors": [
            "Jack Geary",
            "Boyan Gao",
            "Henry Gouk"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We consider the problem of strategic classification, where the act of deploying a classifier leads to strategic behaviour that induces a distribution shift on subsequent observations. Current approaches to learning classifiers in strategic settings are focused primarily on the linear setting, but in many cases non-linear classifiers are more suitable. A central limitation to progress for non-linear classifiers arises from the inability to compute best responses in these settings. We present a novel method for computing the best response by optimising the Lagrangian dual of the Agents' objective. We demonstrate that our method reproduces best responses in linear settings, identifying key weaknesses in existing approaches. We present further results demonstrating our method can be straight-forwardly applied to non-linear classifier settings, where it is useful for both evaluation and training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21561",
        "abs_url": "https://arxiv.org/abs/2511.21561",
        "pdf_url": "https://arxiv.org/pdf/2511.21561",
        "title": "Machine Learning Approaches to Clinical Risk Prediction: Multi-Scale Temporal Alignment in Electronic Health Records",
        "authors": [
            "Wei-Chen Chang",
            "Lu Dai",
            "Ting Xu"
        ],
        "comments": "5 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This study proposes a risk prediction method based on a Multi-Scale Temporal Alignment Network (MSTAN) to address the challenges of temporal irregularity, sampling interval differences, and multi-scale dynamic dependencies in Electronic Health Records (EHR). The method focuses on temporal feature modeling by introducing a learnable temporal alignment mechanism and a multi-scale convolutional feature extraction structure to jointly model long-term trends and short-term fluctuations in EHR sequences. At the input level, the model maps multi-source clinical features into a unified high-dimensional semantic space and employs temporal embedding and alignment modules to dynamically weight irregularly sampled data, reducing the impact of temporal distribution differences on model performance. The multi-scale feature extraction module then captures key patterns across different temporal granularities through multi-layer convolution and hierarchical fusion, achieving a fine-grained representation of patient states. Finally, an attention-based aggregation mechanism integrates global temporal dependencies to generate individual-level risk representations for disease risk prediction and health status assessment. Experiments conducted on publicly available EHR datasets show that the proposed model outperforms mainstream baselines in accuracy, recall, precision, and F1-Score, demonstrating the effectiveness and robustness of multi-scale temporal alignment in complex medical time-series analysis. This study provides a new solution for intelligent representation of high-dimensional asynchronous medical sequences and offers important technical support for EHR-driven clinical risk prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21566",
        "abs_url": "https://arxiv.org/abs/2511.21566",
        "pdf_url": "https://arxiv.org/pdf/2511.21566",
        "title": "A decoupled alignment kernel for peptide membrane permeability predictions",
        "authors": [
            "Ali Amirahmadi",
            "Gke Geylan",
            "Leonardo De Maria",
            "Farzaneh Etminani",
            "Mattias Ohlsson",
            "Alessandro Tibo"
        ],
        "comments": "submitted to Journal of Cheminformatics",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Cyclic peptides are promising modalities for targeting intracellular sites; however, cell-membrane permeability remains a key bottleneck, exacerbated by limited public data and the need for well-calibrated uncertainty. Instead of relying on data-eager complex deep learning architecture, we propose a monomer-aware decoupled global alignment kernel (MD-GAK), which couples chemically meaningful residue-residue similarity with sequence alignment while decoupling local matches from gap penalties. MD-GAK is a relatively simple kernel. To further demonstrate the robustness of our framework, we also introduce a variant, PMD-GAK, which incorporates a triangular positional prior. As we will show in the experimental section, PMD-GAK can offer additional advantages over MD-GAK, particularly in reducing calibration errors. Since our focus is on uncertainty estimation, we use Gaussian Processes as the predictive model, as both MD-GAK and PMD-GAK can be directly applied within this framework. We demonstrate the effectiveness of our methods through an extensive set of experiments, comparing our fully reproducible approach against state-of-the-art models, and show that it outperforms them across all metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21581",
        "abs_url": "https://arxiv.org/abs/2511.21581",
        "pdf_url": "https://arxiv.org/pdf/2511.21581",
        "title": "Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning",
        "authors": [
            "Alex Ning",
            "Yen-Ling Kuo",
            "Gabe Gomes"
        ],
        "comments": "13 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21590",
        "abs_url": "https://arxiv.org/abs/2511.21590",
        "pdf_url": "https://arxiv.org/pdf/2511.21590",
        "title": "An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids",
        "authors": [
            "Muhammad Siddique",
            "Sohaib Zafar"
        ],
        "comments": "16 pages, 11 figures, IEEEaccess journal",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21594",
        "abs_url": "https://arxiv.org/abs/2511.21594",
        "pdf_url": "https://arxiv.org/pdf/2511.21594",
        "title": "Visualizing LLM Latent Space Geometry Through Dimensionality Reduction",
        "authors": [
            "Alex Ning",
            "Vainateya Rangaraju"
        ],
        "comments": "24 pages, 16 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21626",
        "abs_url": "https://arxiv.org/abs/2511.21626",
        "pdf_url": "https://arxiv.org/pdf/2511.21626",
        "title": "Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks",
        "authors": [
            "Mathew Vanherreweghe",
            "Michael H. Freedman",
            "Keith M. Adams"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits. We extend KAG analysis to MNIST digit classification (784 dimensions) using 2-layer MLPs with systematic spatial analysis at multiple scales. We find that KAG emerges during training and appears consistently across spatial scales, from local 7-pixel neighborhoods to the full 28x28 image. This scale-agnostic property holds across different training procedures: both standard training and training with spatial augmentation produce the same qualitative pattern. These findings reveal that neural networks spontaneously develop organized, scale-invariant geometric structure during learning on realistic high-dimensional data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21638",
        "abs_url": "https://arxiv.org/abs/2511.21638",
        "pdf_url": "https://arxiv.org/pdf/2511.21638",
        "title": "Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO",
        "authors": [
            "Daniel R. Jiang",
            "Jalaj Bhandari",
            "Yukai Yang",
            "Rmi Munos",
            "Tyler Lu"
        ],
        "comments": "12 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21654",
        "abs_url": "https://arxiv.org/abs/2511.21654",
        "pdf_url": "https://arxiv.org/pdf/2511.21654",
        "title": "EvilGenie: A Reward Hacking Benchmark",
        "authors": [
            "Jonathan Gabor",
            "Jayson Lynch",
            "Jonathan Rosenfeld"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce EvilGenie, a benchmark for reward hacking in programming settings. We source problems from LiveCodeBench and create an environment in which agents can easily reward hack, such as by hardcoding test cases or editing the testing files. We measure reward hacking in three ways: held out unit tests, LLM judges, and test file edit detection. We verify these methods against human review and each other. We find the LLM judge to be highly effective at detecting reward hacking in unambiguous cases, and observe only minimal improvement from the use of held out test cases. In addition to testing many models using Inspect's basic_agent scaffold, we also measure reward hacking rates for three popular proprietary coding agents: OpenAI's Codex, Anthropic's Claude Code, and Google's Gemini CLI Using GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro, respectively. We observe explicit reward hacking by both Codex and Claude Code, and misaligned behavior by all three agents. Our codebase can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21668",
        "abs_url": "https://arxiv.org/abs/2511.21668",
        "pdf_url": "https://arxiv.org/pdf/2511.21668",
        "title": "Through the telecom lens: Are all training samples important?",
        "authors": [
            "Shruti Bothe",
            "Illyyne Saffar",
            "Aurelie Boisbunon",
            "Hasan Farooq",
            "Julien Forgeat",
            "Md Moin Uddin Chowdhury"
        ],
        "comments": "8pages, 1 table, 8 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and this http URL paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21669",
        "abs_url": "https://arxiv.org/abs/2511.21669",
        "pdf_url": "https://arxiv.org/pdf/2511.21669",
        "title": "DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving",
        "authors": [
            "Fengze Yu",
            "Leshu Li",
            "Brad McDanel",
            "Saiqian Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20665",
        "abs_url": "https://arxiv.org/abs/2511.20665",
        "pdf_url": "https://arxiv.org/pdf/2511.20665",
        "title": "Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology",
        "authors": [
            "Tcharlies Schmitz"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \\r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20678",
        "abs_url": "https://arxiv.org/abs/2511.20678",
        "pdf_url": "https://arxiv.org/pdf/2511.20678",
        "title": "Cryptocurrency Portfolio Management with Reinforcement Learning: Soft Actor--Critic and Deep Deterministic Policy Gradient Algorithms",
        "authors": [
            "Kamal Paykan"
        ],
        "comments": "",
        "subjects": "Computational Finance (q-fin.CP); Machine Learning (cs.LG)",
        "abstract": "This paper proposes a reinforcement learning--based framework for cryptocurrency portfolio management using the Soft Actor--Critic (SAC) and Deep Deterministic Policy Gradient (DDPG) algorithms. Traditional portfolio optimization methods often struggle to adapt to the highly volatile and nonlinear dynamics of cryptocurrency markets. To address this, we design an agent that learns continuous trading actions directly from historical market data through interaction with a simulated trading environment. The agent optimizes portfolio weights to maximize cumulative returns while minimizing downside risk and transaction costs. Experimental evaluations on multiple cryptocurrencies demonstrate that the SAC and DDPG agents outperform baseline strategies such as equal-weighted and mean--variance portfolios. The SAC algorithm, with its entropy-regularized objective, shows greater stability and robustness in noisy market conditions compared to DDPG. These results highlight the potential of deep reinforcement learning for adaptive and data-driven portfolio management in cryptocurrency markets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20685",
        "abs_url": "https://arxiv.org/abs/2511.20685",
        "pdf_url": "https://arxiv.org/pdf/2511.20685",
        "title": "Dual-Domain Deep Learning Method to Accelerate Local Basis Functions Computation for Reservoir Simulation in High-Contrast Porous Media",
        "authors": [
            "Peiqi Li",
            "Jie Chen"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "In energy science, Darcy flow in heterogeneous porous media is a central problem in reservoir sim-ulation. However, the pronounced multiscale characteristics of such media pose significant challenges to conventional numerical methods in terms of computational demand and efficiency. The Mixed Generalized Multiscale Finite Element Method (MGMsFEM) provides an effective framework for addressing these challenges, yet the construction of multiscale basis functions remains computationally expensive. In this work, we propose a dual-domain deep learning framework to accelerate the computation of multiscale basis functions within MGMsFEM for solving Darcy flow problems. By extracting and decoding permeability field features in both the frequency and spatial domains, the method enables rapid generation of numerical matrices of multiscale basis functions. Numerical experiments demonstrate that the proposed framework achieves significant computational acceleration while maintaining high approximation accuracy, thereby offering the potential for future applications in real-world reservoir engineering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20692",
        "abs_url": "https://arxiv.org/abs/2511.20692",
        "pdf_url": "https://arxiv.org/pdf/2511.20692",
        "title": "The Human Brain as a Combinatorial Complex",
        "authors": [
            "Valentina Snchez",
            "iek Gven",
            "Koen Haak",
            "Theodore Papamarkou",
            "Gonzalo Npoles",
            "Marie af Postma"
        ],
        "comments": "Accepted as an Extended Abstract at the NeurReps Workshop, NeurIPS 2025",
        "subjects": "Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG)",
        "abstract": "We propose a framework for constructing combinatorial complexes (CCs) from fMRI time series data that captures both pairwise and higher-order neural interactions through information-theoretic measures, bridging topological deep learning and network neuroscience. Current graph-based representations of brain networks systematically miss the higher-order dependencies that characterize neural complexity, where information processing often involves synergistic interactions that cannot be decomposed into pairwise relationships. Unlike topological lifting approaches that map relational structures into higher-order domains, our method directly constructs CCs from statistical dependencies in the data. Our CCs generalize graphs by incorporating higher-order cells that represent collective dependencies among brain regions, naturally accommodating the multi-scale, hierarchical nature of neural processing. The framework constructs data-driven combinatorial complexes using O-information and S-information measures computed from fMRI signals, preserving both pairwise connections and higher-order cells (e.g., triplets, quadruplets) based on synergistic dependencies. Using NetSim simulations as a controlled proof-of-concept dataset, we demonstrate our CC construction pipeline and show how both pairwise and higher-order dependencies in neural time series can be quantified and represented within a unified structure. This work provides a framework for brain network representation that preserves fundamental higher-order structure invisible to traditional graph methods, and enables the application of topological deep learning (TDL) architectures to neural data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20701",
        "abs_url": "https://arxiv.org/abs/2511.20701",
        "pdf_url": "https://arxiv.org/pdf/2511.20701",
        "title": "Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework",
        "authors": [
            "Nitya Tiwari",
            "Parv Maheshwari",
            "Vidisha Agarwal"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20711",
        "abs_url": "https://arxiv.org/abs/2511.20711",
        "pdf_url": "https://arxiv.org/pdf/2511.20711",
        "title": "A Set of Rules for Model Validation",
        "authors": [
            "Jos Camacho"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The validation of a data-driven model is the process of assessing the model's ability to generalize to new, unseen data in the population of interest. This paper proposes a set of general rules for model validation. These rules are designed to help practitioners create reliable validation plans and report their results transparently. While no validation scheme is flawless, these rules can help practitioners ensure their strategy is sufficient for practical use, openly discuss any limitations of their validation strategy, and report clear, comparable performance metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20721",
        "abs_url": "https://arxiv.org/abs/2511.20721",
        "pdf_url": "https://arxiv.org/pdf/2511.20721",
        "title": "Foundry: Distilling 3D Foundation Models for the Edge",
        "authors": [
            "Guillaume Letellier",
            "Siddharth Srivastava",
            "Frdric Jurie",
            "Gaurav Sharma"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20799",
        "abs_url": "https://arxiv.org/abs/2511.20799",
        "pdf_url": "https://arxiv.org/pdf/2511.20799",
        "title": "Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models",
        "authors": [
            "Trung Cuong Dang",
            "David Mohaisen"
        ],
        "comments": "11 pages, 2 tables, 8 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20814",
        "abs_url": "https://arxiv.org/abs/2511.20814",
        "pdf_url": "https://arxiv.org/pdf/2511.20814",
        "title": "SPHINX: A Synthetic Environment for Visual Perception and Reasoning",
        "authors": [
            "Md Tanvirul Alam",
            "Saksham Aggarwal",
            "Justin Yang Chae",
            "Nidhi Rastogi"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20823",
        "abs_url": "https://arxiv.org/abs/2511.20823",
        "pdf_url": "https://arxiv.org/pdf/2511.20823",
        "title": "RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs",
        "authors": [
            "Roman Naeem",
            "David Hagerman",
            "Jennifer Alvn",
            "Fredrik Kahl"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20834",
        "abs_url": "https://arxiv.org/abs/2511.20834",
        "pdf_url": "https://arxiv.org/pdf/2511.20834",
        "title": "Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks",
        "authors": [
            "Dionysios Adamopoulos",
            "Anastasia Poulopoulou",
            "Georgios Goumas",
            "Christina Giannoula"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Hardware Architecture (cs.AR); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20836",
        "abs_url": "https://arxiv.org/abs/2511.20836",
        "pdf_url": "https://arxiv.org/pdf/2511.20836",
        "title": "Structured Prompting Enables More Robust, Holistic Evaluation of Language Models",
        "authors": [
            "Asad Aali",
            "Muhammad Ahmed Mohsin",
            "Vasiliki Bikia",
            "Arnav Singhvi",
            "Richard Gaus",
            "Suhana Bedi",
            "Hejie Cui",
            "Miguel Fuentes",
            "Alyssa Unell",
            "Yifan Mai",
            "Jordan Cahoon",
            "Michael Pfeffer",
            "Roxana Daneshjou",
            "Sanmi Koyejo",
            "Emily Alsentzer",
            "Percy Liang",
            "Christopher Potts",
            "Nigam H. Shah",
            "Akshay S. Chaudhari"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller {\\Delta} across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (this https URL) and (ii) Prompt Optimization Pipeline (this https URL).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20849",
        "abs_url": "https://arxiv.org/abs/2511.20849",
        "pdf_url": "https://arxiv.org/pdf/2511.20849",
        "title": "Length-MAX Tokenizer for Language Models",
        "authors": [
            "Dong Dong",
            "Weijie Su"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\\%, 17.2\\%, and 18.5\\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\\%, 12.7\\%, and 13.7\\% lower inference latency, together with a 16\\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\\% and enhancing HellaSwag accuracy by 4.3\\%. Moreover, the Length-MAX tokenizer achieves 99.62\\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\\% at inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20851",
        "abs_url": "https://arxiv.org/abs/2511.20851",
        "pdf_url": "https://arxiv.org/pdf/2511.20851",
        "title": "When Features Beat Noise: A Feature Selection Technique Through Noise-Based Hypothesis Testing",
        "authors": [
            "Mousam Sinha",
            "Tirtha Sarathi Ghosh",
            "Ridam Pal"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Feature selection has remained a daunting challenge in machine learning and artificial intelligence, where increasingly complex, high-dimensional datasets demand principled strategies for isolating the most informative predictors. Despite widespread adoption, many established techniques suffer from notable limitations; some incur substantial computational cost, while others offer no definite statistical driven stopping criteria or assesses the significance of their importance scores. A common heuristic approach introduces multiple random noise features and retains all predictors ranked above the strongest noise feature. Although intuitive, this strategy lacks theoretical justification and depends heavily on heuristics. This paper proposes a novel feature selection method that addresses these limitations. Our approach introduces multiple random noise features and evaluates each feature's importance against the maximum importance value among these noise features incorporating a non-parametric bootstrap-based hypothesis testing framework to establish a solid theoretical foundation. We establish the conceptual soundness of our approach through statistical derivations that articulate the principles guiding the design of our algorithm. To evaluate its reliability, we generated simulated datasets under controlled statistical settings and benchmarked performance against Boruta and Knockoff-based methods, observing consistently stronger recovery of meaningful signal. As a demonstration of practical utility, we applied the technique across diverse real-world datasets, where it surpassed feature selection techniques including Boruta, RFE, and Extra Trees. Hence, the method emerges as a robust algorithm for principled feature selection, enabling the distillation of informative predictors that support reliable inference, enhanced predictive performance, and efficient computation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20871",
        "abs_url": "https://arxiv.org/abs/2511.20871",
        "pdf_url": "https://arxiv.org/pdf/2511.20871",
        "title": "A review on data fusion in multimodal learning analytics and educational data mining",
        "authors": [
            "Wilson Chango",
            "Juan A. Lara",
            "Rebeca Cerezo",
            "Cristbal Romero"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "The new educational models such as smart learning environments use of digital and context-aware devices to facilitate the learning process. In this new educational scenario, a huge quantity of multimodal students' data from a variety of different sources can be captured, fused, and analyze. It offers to researchers and educators a unique opportunity of being able to discover new knowledge to better understand the learning process and to intervene if necessary. However, it is necessary to apply correctly data fusion approaches and techniques in order to combine various sources of multimodal learning analytics (MLA). These sources or modalities in MLA include audio, video, electrodermal activity data, eye-tracking, user logs, and click-stream data, but also learning artifacts and more natural human signals such as gestures, gaze, speech, or writing. This survey introduces data fusion in learning analytics (LA) and educational data mining (EDM) and how these data fusion techniques have been applied in smart learning. It shows the current state of the art by reviewing the main publications, the main type of fused educational data, and the data fusion approaches and techniques used in EDM/LA, as well as the main open problems, trends, and challenges in this specific research area.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20888",
        "abs_url": "https://arxiv.org/abs/2511.20888",
        "pdf_url": "https://arxiv.org/pdf/2511.20888",
        "title": "Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets",
        "authors": [
            "Arthur Jacot"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Computational Complexity (cs.CC); Machine Learning (cs.LG)",
        "abstract": "This paper argues that DNNs implement a computational Occam's razor -- finding the `simplest' algorithm that fits the data -- and that this could explain their incredible and wide-ranging success over more traditional statistical methods. We start with the discovery that the set of real-valued function $f$ that can be $\\epsilon$-approximated with a binary circuit of size at most $c\\epsilon^{-\\gamma}$ becomes convex in the `Harder than Monte Carlo' (HTMC) regime, when $\\gamma>2$, allowing for the definition of a HTMC norm on functions. In parallel one can define a complexity measure on the parameters of a ResNets (a weighted $\\ell_1$ norm of the parameters), which induce a `ResNet norm' on functions. The HTMC and ResNet norms can then be related by an almost matching sandwich bound. Thus minimizing this ResNet norm is equivalent to finding a circuit that fits the data with an almost minimal number of nodes (within a power of 2 of being optimal). ResNets thus appear as an alternative model for computation of real functions, better adapted to the HTMC regime and its convexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20922",
        "abs_url": "https://arxiv.org/abs/2511.20922",
        "pdf_url": "https://arxiv.org/pdf/2511.20922",
        "title": "Readout-Side Bypass for Residual Hybrid Quantum-Classical Models",
        "authors": [
            "Guilin Zhang",
            "Wulan Guo",
            "Ziqi Tan",
            "Hongyang He",
            "Hailong Jiang"
        ],
        "comments": "5 pages, 1 figure, 6 tables",
        "subjects": "Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Quantum machine learning (QML) promises compact and expressive representations, but suffers from the measurement bottleneck - a narrow quantum-to-classical readout that limits performance and amplifies privacy risk. We propose a lightweight residual hybrid architecture that concatenates quantum features with raw inputs before classification, bypassing the bottleneck without increasing quantum complexity. Experiments show our model outperforms pure quantum and prior hybrid models in both centralized and federated settings. It achieves up to +55% accuracy improvement over quantum baselines, while retaining low communication cost and enhanced privacy robustness. Ablation studies confirm the effectiveness of the residual connection at the quantum-classical interface. Our method offers a practical, near-term pathway for integrating quantum models into privacy-sensitive, resource-constrained settings like federated edge learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20934",
        "abs_url": "https://arxiv.org/abs/2511.20934",
        "pdf_url": "https://arxiv.org/pdf/2511.20934",
        "title": "Guaranteed Optimal Compositional Explanations for Neurons",
        "authors": [
            "Biagio La Rosa",
            "Leilani H. Gilpin"
        ],
        "comments": "41 pages, 10 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10-40 percent of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20941",
        "abs_url": "https://arxiv.org/abs/2511.20941",
        "pdf_url": "https://arxiv.org/pdf/2511.20941",
        "title": "Fusion of classical and quantum kernels enables accurate and robust two-sample tests",
        "authors": [
            "Yu Terada",
            "Yugo Ogio",
            "Ken Arai",
            "Hiroyuki Tezuka",
            "Yu Tanaka"
        ],
        "comments": "11 pages, 5 figures",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Two-sample tests have been extensively employed in various scientific fields and machine learning such as evaluation on the effectiveness of drugs and A/B testing on different marketing strategies to discriminate whether two sets of samples come from the same distribution or not. Kernel-based procedures for hypothetical testing have been proposed to efficiently disentangle high-dimensional complex structures in data to obtain accurate results in a model-free way by embedding the data into the reproducing kernel Hilbert space (RKHS). While the choice of kernels plays a crucial role for their performance, little is understood about how to choose kernel especially for small datasets. Here we aim to construct a hypothetical test which is effective even for small datasets, based on the theoretical foundation of kernel-based tests using maximum mean discrepancy, which is called MMD-FUSE. To address this, we enhance the MMD-FUSE framework by incorporating quantum kernels and propose a novel hybrid testing strategy that fuses classical and quantum kernels. This approach creates a powerful and adaptive test by combining the domain-specific inductive biases of classical kernels with the unique expressive power of quantum kernels. We evaluate our method on various synthetic and real-world clinical datasets, and our experiments reveal two key findings: 1) With appropriate hyperparameter tuning, MMD-FUSE with quantum kernels consistently improves test power over classical counterparts, especially for small and high-dimensional data. 2) The proposed hybrid framework demonstrates remarkable robustness, adapting to different data characteristics and achieving high test power across diverse scenarios. These results highlight the potential of quantum-inspired and hybrid kernel strategies to build more effective statistical tests, offering a versatile tool for data analysis where sample sizes are limited.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20960",
        "abs_url": "https://arxiv.org/abs/2511.20960",
        "pdf_url": "https://arxiv.org/pdf/2511.20960",
        "title": "Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification",
        "authors": [
            "Soumojit Das",
            "Nairanjana Dasgupta",
            "Prashanta Dutta"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)",
        "abstract": "Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain. We develop a geometric framework for post-hoc calibration of neural network probability outputs, treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric. Our approach yields Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems (Proposition~1) while extending naturally to multi-class settings -- providing a principled generalization that existing methods lack. Complementing calibration, we define geometric reliability scores based on Fisher--Rao distance and construct neutral zones for principled deferral of uncertain predictions. Theoretical contributions include: (i) consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and (ii) tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework (calibration followed by reliability-based deferral) captures 72.5\\% of errors while deferring 34.5\\% of samples. Notably, this operational gain is achievable with any well-calibrated probability output; the contribution of geometric calibration lies in its theoretical foundations rather than empirical superiority over simpler alternatives. This work bridges information geometry and statistical learning, offering formal guarantees relevant to applications requiring rigorous validation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20963",
        "abs_url": "https://arxiv.org/abs/2511.20963",
        "pdf_url": "https://arxiv.org/pdf/2511.20963",
        "title": "Crowdsourcing the Frontier: Advancing Hybrid Physics-ML Climate Simulation via $50,000 Kaggle Competition",
        "authors": [
            "Jerry Lin",
            "Zeyuan Hu",
            "Tom Beucler",
            "Katherine Frields",
            "Hannah Christensen",
            "Walter Hannah",
            "Helge Heuer",
            "Peter Ukkonnen",
            "Laura A. Mansfield",
            "Tian Zheng",
            "Liran Peng",
            "Ritwik Gupta",
            "Pierre Gentine",
            "Yusef Al-Naher",
            "Mingjiang Duan",
            "Kyo Hattori",
            "Weiliang Ji",
            "Chunhan Li",
            "Kippei Matsuda",
            "Naoki Murakami",
            "Shlomo Ron",
            "Marec Serlin",
            "Hongjian Song",
            "Yuma Tanabe",
            "Daisuke Yamamoto",
            "Jianyao Zhou",
            "Mike Pritchard"
        ],
        "comments": "Main text: 29 pages, 10 figures. SI: 47 pages, 37 figures",
        "subjects": "Atmospheric and Oceanic Physics (physics.ao-ph); Machine Learning (cs.LG)",
        "abstract": "Subgrid machine-learning (ML) parameterizations have the potential to introduce a new generation of climate models that incorporate the effects of higher-resolution physics without incurring the prohibitive computational cost associated with more explicit physics-based simulations. However, important issues, ranging from online instability to inconsistent online performance, have limited their operational use for long-term climate projections. To more rapidly drive progress in solving these issues, domain scientists and machine learning researchers opened up the offline aspect of this problem to the broader machine learning and data science community with the release of ClimSim, a NeurIPS Datasets and Benchmarks publication, and an associated Kaggle competition. This paper reports on the downstream results of the Kaggle competition by coupling emulators inspired by the winning teams' architectures to an interactive climate model (including full cloud microphysics, a regime historically prone to online instability) and systematically evaluating their online performance. Our results demonstrate that online stability in the low-resolution, real-geography setting is reproducible across multiple diverse architectures, which we consider a key milestone. All tested architectures exhibit strikingly similar offline and online biases, though their responses to architecture-agnostic design choices (e.g., expanding the list of input variables) can differ significantly. Multiple Kaggle-inspired architectures achieve state-of-the-art (SOTA) results on certain metrics such as zonal mean bias patterns and global RMSE, indicating that crowdsourcing the essence of the offline problem is one path to improving online performance in hybrid physics-AI climate simulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20974",
        "abs_url": "https://arxiv.org/abs/2511.20974",
        "pdf_url": "https://arxiv.org/pdf/2511.20974",
        "title": "RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data",
        "authors": [
            "Zhisheng Zheng",
            "Xiaohang Sun",
            "Tuan Dinh",
            "Abhishek Yanamandra",
            "Abhinav Jain",
            "Zhu Liu",
            "Sunil Hadap",
            "Vimal Bhat",
            "Manoj Aggarwal",
            "Gerard Medioni",
            "David Harwath"
        ],
        "comments": "Work in progress",
        "subjects": "Audio and Speech Processing (eess.AS); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20977",
        "abs_url": "https://arxiv.org/abs/2511.20977",
        "pdf_url": "https://arxiv.org/pdf/2511.20977",
        "title": "Independent policy gradient-based reinforcement learning for economic and reliable energy management of multi-microgrid systems",
        "authors": [
            "Junkai Hu",
            "Li Xia"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Efficiency and reliability are both crucial for energy management, especially in multi-microgrid systems (MMSs) integrating intermittent and distributed renewable energy sources. This study investigates an economic and reliable energy management problem in MMSs under a distributed scheme, where each microgrid independently updates its energy management policy in a decentralized manner to optimize the long-term system performance collaboratively. We introduce the mean and variance of the exchange power between the MMS and the main grid as indicators for the economic performance and reliability of the system. Accordingly, we formulate the energy management problem as a mean-variance team stochastic game (MV-TSG), where conventional methods based on the maximization of expected cumulative rewards are unsuitable for variance metrics. To solve MV-TSGs, we propose a fully distributed independent policy gradient algorithm, with rigorous convergence analysis, for scenarios with known model parameters. For large-scale scenarios with unknown model parameters, we further develop a deep reinforcement learning algorithm based on independent policy gradients, enabling data-driven policy optimization. Numerical experiments in two scenarios validate the effectiveness of the proposed methods. Our approaches fully leverage the distributed computational capabilities of MMSs and achieve a well-balanced trade-off between economic performance and operational reliability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.20991",
        "abs_url": "https://arxiv.org/abs/2511.20991",
        "pdf_url": "https://arxiv.org/pdf/2511.20991",
        "title": "Wavefront-Constrained Passive Obscured Object Detection",
        "authors": [
            "Zhiwen Zheng",
            "Yiwei Ouyang",
            "Zhao Huang",
            "Tao Zhang",
            "Xiaoshuai Zhang",
            "Huiyu Zhou",
            "Wenwen Tang",
            "Shaowei Jiang",
            "Jin Liu",
            "Xingru Huang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Accurately localizing and segmenting obscured objects from faint light patterns beyond the field of view is highly challenging due to multiple scattering and medium-induced perturbations. Most existing methods, based on real-valued modeling or local convolutional operations, are inadequate for capturing the underlying physics of coherent light propagation. Moreover, under low signal-to-noise conditions, these methods often converge to non-physical solutions, severely compromising the stability and reliability of the observation. To address these challenges, we propose a novel physics-driven Wavefront Propagating Compensation Network (WavePCNet) to simulate wavefront propagation and enhance the perception of obscured objects. This WavePCNet integrates the Tri-Phase Wavefront Complex-Propagation Reprojection (TriWCP) to incorporate complex amplitude transfer operators to precisely constrain coherent propagation behavior, along with a momentum memory mechanism to effectively suppress the accumulation of perturbations. Additionally, a High-frequency Cross-layer Compensation Enhancement is introduced to construct frequency-selective pathways with multi-scale receptive fields and dynamically model structural consistency across layers, further boosting the model's robustness and interpretability under complex environmental conditions. Extensive experiments conducted on four physically collected datasets demonstrate that WavePCNet consistently outperforms state-of-the-art methods across both accuracy and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21038",
        "abs_url": "https://arxiv.org/abs/2511.21038",
        "pdf_url": "https://arxiv.org/pdf/2511.21038",
        "title": "Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels",
        "authors": [
            "Anantha Padmanaban Krishna Kumar"
        ],
        "comments": "13 pages total (7 pages main text, 3 pages references, 3 pages appendix), 2 figures, 14 tables. Code available at this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \\emph{natural} demonstrations (with correct labels) and \\emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21081",
        "abs_url": "https://arxiv.org/abs/2511.21081",
        "pdf_url": "https://arxiv.org/pdf/2511.21081",
        "title": "Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning",
        "authors": [
            "Thura Aung",
            "Eaint Kay Khaing Kyaw",
            "Ye Kyaw Thu",
            "Thazin Myint Oo",
            "Thepchai Supnithi"
        ],
        "comments": "6 pages, 2 figures, 4 tables, Accepted to iSAI-NLP 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21088",
        "abs_url": "https://arxiv.org/abs/2511.21088",
        "pdf_url": "https://arxiv.org/pdf/2511.21088",
        "title": "ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features",
        "authors": [
            "Ye Bhone Lin",
            "Thura Aung",
            "Ye Kyaw Thu",
            "Thazin Myint Oo"
        ],
        "comments": "7 pages, 2 figures, 7 tables, Accepted to iSAI-NLP 2025",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21101",
        "abs_url": "https://arxiv.org/abs/2511.21101",
        "pdf_url": "https://arxiv.org/pdf/2511.21101",
        "title": "MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing",
        "authors": [
            "Manish Jain",
            "Satheesh Kumar Ponnambalam",
            "Salman Faroz",
            "Chandrakanth Lns",
            "Vinay Sharma"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21115",
        "abs_url": "https://arxiv.org/abs/2511.21115",
        "pdf_url": "https://arxiv.org/pdf/2511.21115",
        "title": "Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms",
        "authors": [
            "Lechen Feng",
            "Haoran Li",
            "Lucky Li",
            "Xingqiu Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "This paper investigates the partial linear model by Least Absolute Deviation (LAD) regression. We parameterize the nonparametric term using Deep Neural Networks (DNNs) and formulate a penalized LAD problem for estimation. Specifically, our model exhibits the following challenges. First, the regularization term can be nonconvex and nonsmooth, necessitating the introduction of infinite dimensional variational analysis and nonsmooth analysis into the asymptotic normality discussion. Second, our network must expand (in width, sparsity level and depth) as more samples are observed, thereby introducing additional difficulties for theoretical analysis. Third, the oracle of the proposed estimator is itself defined through a ultra high-dimensional, nonconvex, and discontinuous optimization problem, which already entails substantial computational and theoretical challenges. Under such the challenges, we establish the consistency, convergence rate, and asymptotic normality of the estimator. Furthermore, we analyze the oracle problem itself and its continuous relaxation. We study the convergence of a proximal subgradient method for both formulations, highlighting their structural differences lead to distinct computational subproblems along the iterations. In particular, the relaxed formulation admits significantly cheaper proximal updates, reflecting an inherent trade-off between statistical accuracy and computational tractability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21213",
        "abs_url": "https://arxiv.org/abs/2511.21213",
        "pdf_url": "https://arxiv.org/pdf/2511.21213",
        "title": "Lattice-to-total thermal conductivity ratio: a phonon-glass electron-crystal descriptor for data-driven thermoelectric design",
        "authors": [
            "Yifan Sun",
            "Zhi Li",
            "Tetsuya Imamura",
            "Yuji Ohishi",
            "Chris Wolverton",
            "Ken Kurosaki"
        ],
        "comments": "15 pages, 7 figures",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Thermoelectrics (TEs) are promising candidates for energy harvesting with performance quantified by figure of merit, $ZT$. To accelerate the discovery of high-$ZT$ materials, efforts have focused on identifying compounds with low thermal conductivity $\\kappa$. Using a curated dataset of 71,913 entries, we show that high-$ZT$ materials reside not only in the low-$\\kappa$ regime but also cluster near a lattice-to-total thermal conductivity ratio ($\\kappa_\\mathrm{L}/\\kappa$) of approximately 0.5, consistent with the phonon-glass electron-crystal design concept. Building on this insight, we construct a framework consisting of two machine learning models for the lattice and electronic components of thermal conductivity that jointly provide both $\\kappa$ and $\\kappa_\\mathrm{L}/\\kappa$ for screening and guiding the optimization of TE materials. Among 104,567 compounds screened, our models identify 2,522 ultralow-$\\kappa$ candidates. Follow-up case studies demonstrate that this framework can reliably provide optimization strategies by suggesting new dopants and alloys that shift pristine materials toward the $\\kappa_\\mathrm{L}/\\kappa$ approaching 0.5 regime. Ultimately, by integrating rapid screening with PGEC-guided optimization, our data-driven framework effectively bridges the critical gap between materials discovery and performance enhancement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21223",
        "abs_url": "https://arxiv.org/abs/2511.21223",
        "pdf_url": "https://arxiv.org/pdf/2511.21223",
        "title": "Maxitive Donsker-Varadhan Formulation for Possibilistic Variational Inference",
        "authors": [
            "Jasraj Singh",
            "Shelvia Wongso",
            "Jeremie Houssineau",
            "Badr-Eddine Chrief-Abdellatif"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Variational inference (VI) is a cornerstone of modern Bayesian learning, enabling approximate inference in complex models that would otherwise be intractable. However, its formulation depends on expectations and divergences defined through high-dimensional integrals, often rendering analytical treatment impossible and necessitating heavy reliance on approximate learning and inference techniques. Possibility theory, an imprecise probability framework, allows to directly model epistemic uncertainty instead of leveraging subjective probabilities. While this framework provides robustness and interpretability under sparse or imprecise information, adapting VI to the possibilistic setting requires rethinking core concepts such as entropy and divergence, which presuppose additivity. In this work, we develop a principled formulation of possibilistic variational inference and apply it to a special class of exponential-family functions, highlighting parallels with their probabilistic counterparts and revealing the distinctive mathematical structures of possibility theory.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21232",
        "abs_url": "https://arxiv.org/abs/2511.21232",
        "pdf_url": "https://arxiv.org/pdf/2511.21232",
        "title": "RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI",
        "authors": [
            "Muhammed Yildirim",
            "Ozcan Ozturk"
        ],
        "comments": "13 pages, 7 tables, 14 figures",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21247",
        "abs_url": "https://arxiv.org/abs/2511.21247",
        "pdf_url": "https://arxiv.org/pdf/2511.21247",
        "title": "The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval",
        "authors": [
            "Jaime Garcia-Martinez",
            "David Diaz-Guerra",
            "John Anderson",
            "Ricardo Falcon-Perez",
            "Pablo Cabaas-Molero",
            "Tuomas Virtanen",
            "Julio J. Carabias-Orti",
            "Pedro Vera-Candeas"
        ],
        "comments": "",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the Colibr Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21257",
        "abs_url": "https://arxiv.org/abs/2511.21257",
        "pdf_url": "https://arxiv.org/pdf/2511.21257",
        "title": "Estimation in high-dimensional linear regression: Post-Double-Autometrics as an alternative to Post-Double-Lasso",
        "authors": [
            "Sullivan Hu",
            "Sbastien Laurent",
            "Ulrich Aiounou",
            "Emmanuel Flachaire"
        ],
        "comments": "",
        "subjects": "Econometrics (econ.EM); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Post-Double-Lasso is becoming the most popular method for estimating linear regression models with many covariates when the purpose is to obtain an accurate estimate of a parameter of interest, such as an average treatment effect. However, this method can suffer from substantial omitted variable bias in finite sample. We propose a new method called Post-Double-Autometrics, which is based on Autometrics, and show that this method outperforms Post-Double-Lasso. Its use in a standard application of economic growth sheds new light on the hypothesis of convergence from poor to rich economies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21283",
        "abs_url": "https://arxiv.org/abs/2511.21283",
        "pdf_url": "https://arxiv.org/pdf/2511.21283",
        "title": "On the Periodic Orbits of the Dual Logarithmic Derivative Operator",
        "authors": [
            "Xiaohang Yu",
            "William Knottenbelt"
        ],
        "comments": "",
        "subjects": "Dynamical Systems (math.DS); Machine Learning (cs.LG)",
        "abstract": "We study the periodic behaviour of the dual logarithmic derivative operator $\\mathcal{A}[f]=\\mathrm{d}\\ln f/\\mathrm{d}\\ln x$ in a complex analytic setting. We show that $\\mathcal{A}$ admits genuinely nondegenerate period-$2$ orbits and identify a canonical explicit example. Motivated by this, we obtain a complete classification of all nondegenerate period-$2$ solutions, which are precisely the rational pairs $(c a x^{c}/(1-ax^{c}),\\, c/(1-ax^{c}))$ with $ac\\neq 0$. We further classify all fixed points of $\\mathcal{A}$, showing that every solution of $\\mathcal{A}[f]=f$ has the form $f(x)=1/(a-\\ln x)$. As an illustration, logistic-type functions become pre-periodic under $\\mathcal{A}$ after a logarithmic change of variables, entering the period-$2$ family in one iterate. These results give an explicit description of the low-period structure of $\\mathcal{A}$ and provide a tractable example of operator-induced dynamics on function spaces.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21340",
        "abs_url": "https://arxiv.org/abs/2511.21340",
        "pdf_url": "https://arxiv.org/pdf/2511.21340",
        "title": "Phase-Aware Code-Aided EM Algorithm for Blind Channel Estimation in PSK-Modulated OFDM",
        "authors": [
            "Chin-Hung Chen",
            "Ivana Nikoloska",
            "Wim van Houtum",
            "Yan Wu",
            "Alex Alvarado"
        ],
        "comments": "preprint",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "This paper presents a fully blind phase-aware expectation-maximization (EM) algorithm for OFDM systems with the phase-shift keying (PSK) modulation. We address the well-known local maximum problem of the EM algorithm for blind channel estimation. This is primarily caused by the unknown phase ambiguity in the channel estimates, which conventional blind EM estimators cannot resolve. To overcome this limitation, we propose to exploit the extrinsic information from the decoder as model evidence metrics. A finite set of candidate models is generated based on the inherent symmetries of PSK modulation, and the decoder selects the most likely candidate model. Simulation results demonstrate that, when combined with a simple convolutional code, the phase-aware EM algorithm reliably resolves phase ambiguity during the initialization stage and reduces the local convergence rate from 80% to nearly 0% in frequency-selective channels with a constant phase ambiguity. The algorithm is invoked only once after the EM initialization stage, resulting in negligible additional complexity during subsequent turbo iterations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21350",
        "abs_url": "https://arxiv.org/abs/2511.21350",
        "pdf_url": "https://arxiv.org/pdf/2511.21350",
        "title": "Learning Multi-Order Block Structure in Higher-Order Networks",
        "authors": [
            "Kazuki Nakajima",
            "Yuya Sasaki",
            "Takeaki Uno",
            "Masaki Aida"
        ],
        "comments": "38 pages, 10 figures, and 7 tables",
        "subjects": "Social and Information Networks (cs.SI); Machine Learning (cs.LG)",
        "abstract": "Higher-order networks, naturally described as hypergraphs, are essential for modeling real-world systems involving interactions among three or more entities. Stochastic block models offer a principled framework for characterizing mesoscale organization, yet their extension to hypergraphs involves a trade-off between expressive power and computational complexity. A recent simplification, a single-order model, mitigates this complexity by assuming a single affinity pattern governs interactions of all orders. This universal assumption, however, may overlook order-dependent structural details. Here, we propose a framework that relaxes this assumption by introducing a multi-order block structure, in which different affinity patterns govern distinct subsets of interaction orders. Our framework is based on a multi-order stochastic block model and searches for the optimal partition of the set of interaction orders that maximizes out-of-sample hyperlink prediction performance. Analyzing a diverse range of real-world networks, we find that multi-order block structures are prevalent. Accounting for them not only yields better predictive performance over the single-order model but also uncovers sharper, more interpretable mesoscale organization. Our findings reveal that order-dependent mechanisms are a key feature of the mesoscale organization of real-world higher-order networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21369",
        "abs_url": "https://arxiv.org/abs/2511.21369",
        "pdf_url": "https://arxiv.org/pdf/2511.21369",
        "title": "Differentiable Physics-Neural Models enable Learning of Non-Markovian Closures for Accelerated Coarse-Grained Physics Simulations",
        "authors": [
            "Tingkai Xue",
            "Chin Chun Ooi",
            "Zhengwei Ge",
            "Fong Yew Leong",
            "Hongying Li",
            "Chang Wei Kang"
        ],
        "comments": "",
        "subjects": "Computational Physics (physics.comp-ph); Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Numerical simulations provide key insights into many physical, real-world problems. However, while these simulations are solved on a full 3D domain, most analysis only require a reduced set of metrics (e.g. plane-level concentrations). This work presents a hybrid physics-neural model that predicts scalar transport in a complex domain orders of magnitude faster than the 3D simulation (from hours to less than 1 min). This end-to-end differentiable framework jointly learns the physical model parameterization (i.e. orthotropic diffusivity) and a non-Markovian neural closure model to capture unresolved, 'coarse-grained' effects, thereby enabling stable, long time horizon rollouts. This proposed model is data-efficient (learning with 26 training data), and can be flexibly extended to an out-of-distribution scenario (with a moving source), achieving a Spearman correlation coefficient of 0.96 at the final simulation time. Overall results show that this differentiable physics-neural framework enables fast, accurate, and generalizable coarse-grained surrogates for physical phenomena.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21416",
        "abs_url": "https://arxiv.org/abs/2511.21416",
        "pdf_url": "https://arxiv.org/pdf/2511.21416",
        "title": "Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning",
        "authors": [
            "Kaifeng Hong",
            "Yinglong Zhang",
            "Xiaoying Hong",
            "Xuewen Xia",
            "Xing Xu"
        ],
        "comments": "32 pages, 2 figures",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module this http URL message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and this http URL make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21437",
        "abs_url": "https://arxiv.org/abs/2511.21437",
        "pdf_url": "https://arxiv.org/pdf/2511.21437",
        "title": "A Systematic Study of Model Merging Techniques in Large Language Models",
        "authors": [
            "Ouz Kaan Hitit",
            "Leander Girrbach",
            "Zeynep Akata"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21490",
        "abs_url": "https://arxiv.org/abs/2511.21490",
        "pdf_url": "https://arxiv.org/pdf/2511.21490",
        "title": "Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning",
        "authors": [
            "Taehoon Kim",
            "Donghwan Jang",
            "Bohyung Han"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We present a novel training approach, named Merge-and-Bound (M&B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21526",
        "abs_url": "https://arxiv.org/abs/2511.21526",
        "pdf_url": "https://arxiv.org/pdf/2511.21526",
        "title": "Phase Transition for Stochastic Block Model with more than $\\sqrt{n}$ Communities (II)",
        "authors": [
            "Alexandra Carpentier",
            "Christophe Giraud",
            "Nicolas Verzelen"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST)",
        "abstract": "A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics. When $K \\geq \\sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \\emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \\geq \\sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes. The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\\\ 1- Constructing a family of motifs satisfying specific structural properties; and\\\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \\geq \\sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21550",
        "abs_url": "https://arxiv.org/abs/2511.21550",
        "pdf_url": "https://arxiv.org/pdf/2511.21550",
        "title": "MMA: A Momentum Mamba Architecture for Human Activity Recognition with Inertial Sensors",
        "authors": [
            "Thai-Khanh Nguyen",
            "Uyen Vo",
            "Tan M. Nguyen",
            "Thieu N. Vo",
            "Trung-Hieu Le",
            "Cuong Pham"
        ],
        "comments": "14 pages, 5 pages",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Human activity recognition (HAR) from inertial sensors is essential for ubiquitous computing, mobile health, and ambient intelligence. Conventional deep models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformers have advanced HAR but remain limited by vanishing or exloding gradients, high computational cost, and difficulty in capturing long-range dependencies. Structured state-space models (SSMs) like Mamba address these challenges with linear complexity and effective temporal modeling, yet they are restricted to first-order dynamics without stable longterm memory mechanisms. We introduce Momentum Mamba, a momentum-augmented SSM that incorporates second-order dynamics to improve stability of information flow across time steps, robustness, and long-sequence modeling. Two extensions further expand its capacity: Complex Momentum Mamba for frequency-selective memory scaling. Experiments on multiple HAR benchmarks demonstrate consistent gains over vanilla Mamba and Transformer baselines in accuracy, robustness, and convergence speed. With only moderate increases in training cost, momentum-augmented SSMs offer a favorable accuracy-efficiency balance, establishing them as a scalable paradigm for HAR and a promising principal framework for broader sequence modeling applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21600",
        "abs_url": "https://arxiv.org/abs/2511.21600",
        "pdf_url": "https://arxiv.org/pdf/2511.21600",
        "title": "TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data",
        "authors": [
            "Yizhou Zhao",
            "Xiang Li",
            "Peter Song",
            "Qi Long",
            "Weijie Su"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "The rise of generative AI has enabled the production of high-fidelity synthetic tabular data across fields such as healthcare, finance, and public policy, raising growing concerns about data provenance and misuse. Watermarking offers a promising solution to address these concerns by ensuring the traceability of synthetic data, but existing methods face many limitations: they are computationally expensive due to reliance on large diffusion models, struggle with mixed discrete-continuous data, or lack robustness to post-modifications. To address them, we propose TAB-DRW, an efficient and robust post-editing watermarking scheme for generative tabular data. TAB-DRW embeds watermark signals in the frequency domain: it normalizes heterogeneous features via the Yeo-Johnson transformation and standardization, applies the discrete Fourier transform (DFT), and adjusts the imaginary parts of adaptively selected entries according to precomputed pseudorandom bits. To further enhance robustness and efficiency, we introduce a novel rank-based pseudorandom bit generation method that enables row-wise retrieval without incurring storage overhead. Experiments on five benchmark tabular datasets show that TAB-DRW achieves strong detectability and robustness against common post-processing attacks, while preserving high data fidelity and fully supporting mixed-type features.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21607",
        "abs_url": "https://arxiv.org/abs/2511.21607",
        "pdf_url": "https://arxiv.org/pdf/2511.21607",
        "title": "Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation",
        "authors": [
            "Zarin Tahia Hossain",
            "Mostafa Milani"
        ],
        "comments": "To appear in conference proceedings",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21652",
        "abs_url": "https://arxiv.org/abs/2511.21652",
        "pdf_url": "https://arxiv.org/pdf/2511.21652",
        "title": "Continual Error Correction on Low-Resource Devices",
        "authors": [
            "Kirill Paramonov",
            "Mete Ozay",
            "Aristeidis Mystakidis",
            "Nikolaos Tsalikidis",
            "Dimitrios Sotos",
            "Anastasios Drosou",
            "Dimitrios Tzovaras",
            "Hyunjun Kim",
            "Kiseok Chang",
            "Sangdok Mo",
            "Namwoong Kim",
            "Woojong Yoo",
            "Jijoong Moon",
            "Umberto Michieli"
        ],
        "comments": "ACM MMSys 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21675",
        "abs_url": "https://arxiv.org/abs/2511.21675",
        "pdf_url": "https://arxiv.org/pdf/2511.21675",
        "title": "On Evolution-Based Models for Experimentation Under Interference",
        "authors": [
            "Sadegh Shirani",
            "Mohsen Bayati"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Social and Information Networks (cs.SI); Econometrics (econ.EM)",
        "abstract": "Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21686",
        "abs_url": "https://arxiv.org/abs/2511.21686",
        "pdf_url": "https://arxiv.org/pdf/2511.21686",
        "title": "Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework",
        "authors": [
            "Dong Wang",
            "Yang Li",
            "Ansong Ni",
            "Ching-Feng Yeh",
            "Youssef Emad",
            "Xinjie Lei",
            "Liam Robbins",
            "Karthik Padthe",
            "Hu Xu",
            "Xian Li",
            "Asli Celikyilmaz",
            "Ramya Raghavendra",
            "Lifei Huang",
            "Carole-Jean Wu",
            "Shang-Wen Li"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \\textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\\times$ higher data generation throughput under identical hardware resources, without compromising output quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-27",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-27?abs=True",
        "arxiv_id": "2511.21689",
        "abs_url": "https://arxiv.org/abs/2511.21689",
        "pdf_url": "https://arxiv.org/pdf/2511.21689",
        "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
        "authors": [
            "Hongjin Su",
            "Shizhe Diao",
            "Ximing Lu",
            "Mingjie Liu",
            "Jiacheng Xu",
            "Xin Dong",
            "Yonggan Fu",
            "Peter Belcak",
            "Hanrong Ye",
            "Hongxu Yin",
            "Yi Dong",
            "Evelina Bakhturina",
            "Tao Yu",
            "Yejin Choi",
            "Jan Kautz",
            "Pavlo Molchanov"
        ],
        "comments": "21 pages, 6 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]